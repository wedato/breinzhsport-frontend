{"ast":null,"code":"import { __assign } from \"tslib\";\nexport var AccessDeniedException;\n(function (AccessDeniedException) {\n  AccessDeniedException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(AccessDeniedException || (AccessDeniedException = {}));\nexport var AgeRange;\n(function (AgeRange) {\n  AgeRange.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(AgeRange || (AgeRange = {}));\nexport var S3Object;\n(function (S3Object) {\n  S3Object.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(S3Object || (S3Object = {}));\nexport var GroundTruthManifest;\n(function (GroundTruthManifest) {\n  GroundTruthManifest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GroundTruthManifest || (GroundTruthManifest = {}));\nexport var Asset;\n(function (Asset) {\n  Asset.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Asset || (Asset = {}));\nexport var Attribute;\n(function (Attribute) {\n  Attribute[\"ALL\"] = \"ALL\";\n  Attribute[\"DEFAULT\"] = \"DEFAULT\";\n})(Attribute || (Attribute = {}));\nexport var AudioMetadata;\n(function (AudioMetadata) {\n  AudioMetadata.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(AudioMetadata || (AudioMetadata = {}));\nexport var Beard;\n(function (Beard) {\n  Beard.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Beard || (Beard = {}));\nexport var BodyPart;\n(function (BodyPart) {\n  BodyPart[\"FACE\"] = \"FACE\";\n  BodyPart[\"HEAD\"] = \"HEAD\";\n  BodyPart[\"LEFT_HAND\"] = \"LEFT_HAND\";\n  BodyPart[\"RIGHT_HAND\"] = \"RIGHT_HAND\";\n})(BodyPart || (BodyPart = {}));\nexport var BoundingBox;\n(function (BoundingBox) {\n  BoundingBox.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(BoundingBox || (BoundingBox = {}));\nexport var CoversBodyPart;\n(function (CoversBodyPart) {\n  CoversBodyPart.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CoversBodyPart || (CoversBodyPart = {}));\nexport var ProtectiveEquipmentType;\n(function (ProtectiveEquipmentType) {\n  ProtectiveEquipmentType[\"FACE_COVER\"] = \"FACE_COVER\";\n  ProtectiveEquipmentType[\"HAND_COVER\"] = \"HAND_COVER\";\n  ProtectiveEquipmentType[\"HEAD_COVER\"] = \"HEAD_COVER\";\n})(ProtectiveEquipmentType || (ProtectiveEquipmentType = {}));\nexport var EquipmentDetection;\n(function (EquipmentDetection) {\n  EquipmentDetection.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(EquipmentDetection || (EquipmentDetection = {}));\nexport var ProtectiveEquipmentBodyPart;\n(function (ProtectiveEquipmentBodyPart) {\n  ProtectiveEquipmentBodyPart.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ProtectiveEquipmentBodyPart || (ProtectiveEquipmentBodyPart = {}));\nexport var LandmarkType;\n(function (LandmarkType) {\n  LandmarkType[\"chinBottom\"] = \"chinBottom\";\n  LandmarkType[\"eyeLeft\"] = \"eyeLeft\";\n  LandmarkType[\"eyeRight\"] = \"eyeRight\";\n  LandmarkType[\"leftEyeBrowLeft\"] = \"leftEyeBrowLeft\";\n  LandmarkType[\"leftEyeBrowRight\"] = \"leftEyeBrowRight\";\n  LandmarkType[\"leftEyeBrowUp\"] = \"leftEyeBrowUp\";\n  LandmarkType[\"leftEyeDown\"] = \"leftEyeDown\";\n  LandmarkType[\"leftEyeLeft\"] = \"leftEyeLeft\";\n  LandmarkType[\"leftEyeRight\"] = \"leftEyeRight\";\n  LandmarkType[\"leftEyeUp\"] = \"leftEyeUp\";\n  LandmarkType[\"leftPupil\"] = \"leftPupil\";\n  LandmarkType[\"midJawlineLeft\"] = \"midJawlineLeft\";\n  LandmarkType[\"midJawlineRight\"] = \"midJawlineRight\";\n  LandmarkType[\"mouthDown\"] = \"mouthDown\";\n  LandmarkType[\"mouthLeft\"] = \"mouthLeft\";\n  LandmarkType[\"mouthRight\"] = \"mouthRight\";\n  LandmarkType[\"mouthUp\"] = \"mouthUp\";\n  LandmarkType[\"nose\"] = \"nose\";\n  LandmarkType[\"noseLeft\"] = \"noseLeft\";\n  LandmarkType[\"noseRight\"] = \"noseRight\";\n  LandmarkType[\"rightEyeBrowLeft\"] = \"rightEyeBrowLeft\";\n  LandmarkType[\"rightEyeBrowRight\"] = \"rightEyeBrowRight\";\n  LandmarkType[\"rightEyeBrowUp\"] = \"rightEyeBrowUp\";\n  LandmarkType[\"rightEyeDown\"] = \"rightEyeDown\";\n  LandmarkType[\"rightEyeLeft\"] = \"rightEyeLeft\";\n  LandmarkType[\"rightEyeRight\"] = \"rightEyeRight\";\n  LandmarkType[\"rightEyeUp\"] = \"rightEyeUp\";\n  LandmarkType[\"rightPupil\"] = \"rightPupil\";\n  LandmarkType[\"upperJawlineLeft\"] = \"upperJawlineLeft\";\n  LandmarkType[\"upperJawlineRight\"] = \"upperJawlineRight\";\n})(LandmarkType || (LandmarkType = {}));\nexport var Landmark;\n(function (Landmark) {\n  Landmark.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Landmark || (Landmark = {}));\nexport var Pose;\n(function (Pose) {\n  Pose.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Pose || (Pose = {}));\nexport var ImageQuality;\n(function (ImageQuality) {\n  ImageQuality.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ImageQuality || (ImageQuality = {}));\nexport var ComparedFace;\n(function (ComparedFace) {\n  ComparedFace.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ComparedFace || (ComparedFace = {}));\nexport var Celebrity;\n(function (Celebrity) {\n  Celebrity.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Celebrity || (Celebrity = {}));\nexport var Emotion;\n(function (Emotion) {\n  Emotion.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Emotion || (Emotion = {}));\nexport var Eyeglasses;\n(function (Eyeglasses) {\n  Eyeglasses.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Eyeglasses || (Eyeglasses = {}));\nexport var EyeOpen;\n(function (EyeOpen) {\n  EyeOpen.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(EyeOpen || (EyeOpen = {}));\nexport var GenderType;\n(function (GenderType) {\n  GenderType[\"Female\"] = \"Female\";\n  GenderType[\"Male\"] = \"Male\";\n})(GenderType || (GenderType = {}));\nexport var Gender;\n(function (Gender) {\n  Gender.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Gender || (Gender = {}));\nexport var MouthOpen;\n(function (MouthOpen) {\n  MouthOpen.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(MouthOpen || (MouthOpen = {}));\nexport var Mustache;\n(function (Mustache) {\n  Mustache.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Mustache || (Mustache = {}));\nexport var Smile;\n(function (Smile) {\n  Smile.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Smile || (Smile = {}));\nexport var Sunglasses;\n(function (Sunglasses) {\n  Sunglasses.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Sunglasses || (Sunglasses = {}));\nexport var FaceDetail;\n(function (FaceDetail) {\n  FaceDetail.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(FaceDetail || (FaceDetail = {}));\nexport var CelebrityDetail;\n(function (CelebrityDetail) {\n  CelebrityDetail.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CelebrityDetail || (CelebrityDetail = {}));\nexport var CelebrityRecognition;\n(function (CelebrityRecognition) {\n  CelebrityRecognition.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CelebrityRecognition || (CelebrityRecognition = {}));\nexport var CelebrityRecognitionSortBy;\n(function (CelebrityRecognitionSortBy) {\n  CelebrityRecognitionSortBy[\"ID\"] = \"ID\";\n  CelebrityRecognitionSortBy[\"TIMESTAMP\"] = \"TIMESTAMP\";\n})(CelebrityRecognitionSortBy || (CelebrityRecognitionSortBy = {}));\nexport var ComparedSourceImageFace;\n(function (ComparedSourceImageFace) {\n  ComparedSourceImageFace.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ComparedSourceImageFace || (ComparedSourceImageFace = {}));\nexport var QualityFilter;\n(function (QualityFilter) {\n  QualityFilter[\"AUTO\"] = \"AUTO\";\n  QualityFilter[\"HIGH\"] = \"HIGH\";\n  QualityFilter[\"LOW\"] = \"LOW\";\n  QualityFilter[\"MEDIUM\"] = \"MEDIUM\";\n  QualityFilter[\"NONE\"] = \"NONE\";\n})(QualityFilter || (QualityFilter = {}));\nexport var Image;\n(function (Image) {\n  Image.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Image || (Image = {}));\nexport var CompareFacesRequest;\n(function (CompareFacesRequest) {\n  CompareFacesRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CompareFacesRequest || (CompareFacesRequest = {}));\nexport var CompareFacesMatch;\n(function (CompareFacesMatch) {\n  CompareFacesMatch.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CompareFacesMatch || (CompareFacesMatch = {}));\nexport var OrientationCorrection;\n(function (OrientationCorrection) {\n  OrientationCorrection[\"ROTATE_0\"] = \"ROTATE_0\";\n  OrientationCorrection[\"ROTATE_180\"] = \"ROTATE_180\";\n  OrientationCorrection[\"ROTATE_270\"] = \"ROTATE_270\";\n  OrientationCorrection[\"ROTATE_90\"] = \"ROTATE_90\";\n})(OrientationCorrection || (OrientationCorrection = {}));\nexport var CompareFacesResponse;\n(function (CompareFacesResponse) {\n  CompareFacesResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CompareFacesResponse || (CompareFacesResponse = {}));\nexport var ImageTooLargeException;\n(function (ImageTooLargeException) {\n  ImageTooLargeException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ImageTooLargeException || (ImageTooLargeException = {}));\nexport var InternalServerError;\n(function (InternalServerError) {\n  InternalServerError.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(InternalServerError || (InternalServerError = {}));\nexport var InvalidImageFormatException;\n(function (InvalidImageFormatException) {\n  InvalidImageFormatException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(InvalidImageFormatException || (InvalidImageFormatException = {}));\nexport var InvalidParameterException;\n(function (InvalidParameterException) {\n  InvalidParameterException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(InvalidParameterException || (InvalidParameterException = {}));\nexport var InvalidS3ObjectException;\n(function (InvalidS3ObjectException) {\n  InvalidS3ObjectException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(InvalidS3ObjectException || (InvalidS3ObjectException = {}));\nexport var ProvisionedThroughputExceededException;\n(function (ProvisionedThroughputExceededException) {\n  ProvisionedThroughputExceededException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ProvisionedThroughputExceededException || (ProvisionedThroughputExceededException = {}));\nexport var ThrottlingException;\n(function (ThrottlingException) {\n  ThrottlingException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ThrottlingException || (ThrottlingException = {}));\nexport var ContentClassifier;\n(function (ContentClassifier) {\n  ContentClassifier[\"FREE_OF_ADULT_CONTENT\"] = \"FreeOfAdultContent\";\n  ContentClassifier[\"FREE_OF_PERSONALLY_IDENTIFIABLE_INFORMATION\"] = \"FreeOfPersonallyIdentifiableInformation\";\n})(ContentClassifier || (ContentClassifier = {}));\nexport var ModerationLabel;\n(function (ModerationLabel) {\n  ModerationLabel.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ModerationLabel || (ModerationLabel = {}));\nexport var ContentModerationDetection;\n(function (ContentModerationDetection) {\n  ContentModerationDetection.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ContentModerationDetection || (ContentModerationDetection = {}));\nexport var ContentModerationSortBy;\n(function (ContentModerationSortBy) {\n  ContentModerationSortBy[\"NAME\"] = \"NAME\";\n  ContentModerationSortBy[\"TIMESTAMP\"] = \"TIMESTAMP\";\n})(ContentModerationSortBy || (ContentModerationSortBy = {}));\nexport var CreateCollectionRequest;\n(function (CreateCollectionRequest) {\n  CreateCollectionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CreateCollectionRequest || (CreateCollectionRequest = {}));\nexport var CreateCollectionResponse;\n(function (CreateCollectionResponse) {\n  CreateCollectionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CreateCollectionResponse || (CreateCollectionResponse = {}));\nexport var ResourceAlreadyExistsException;\n(function (ResourceAlreadyExistsException) {\n  ResourceAlreadyExistsException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ResourceAlreadyExistsException || (ResourceAlreadyExistsException = {}));\nexport var CreateProjectRequest;\n(function (CreateProjectRequest) {\n  CreateProjectRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CreateProjectRequest || (CreateProjectRequest = {}));\nexport var CreateProjectResponse;\n(function (CreateProjectResponse) {\n  CreateProjectResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CreateProjectResponse || (CreateProjectResponse = {}));\nexport var LimitExceededException;\n(function (LimitExceededException) {\n  LimitExceededException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(LimitExceededException || (LimitExceededException = {}));\nexport var ResourceInUseException;\n(function (ResourceInUseException) {\n  ResourceInUseException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ResourceInUseException || (ResourceInUseException = {}));\nexport var OutputConfig;\n(function (OutputConfig) {\n  OutputConfig.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(OutputConfig || (OutputConfig = {}));\nexport var TestingData;\n(function (TestingData) {\n  TestingData.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(TestingData || (TestingData = {}));\nexport var TrainingData;\n(function (TrainingData) {\n  TrainingData.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(TrainingData || (TrainingData = {}));\nexport var CreateProjectVersionRequest;\n(function (CreateProjectVersionRequest) {\n  CreateProjectVersionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CreateProjectVersionRequest || (CreateProjectVersionRequest = {}));\nexport var CreateProjectVersionResponse;\n(function (CreateProjectVersionResponse) {\n  CreateProjectVersionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CreateProjectVersionResponse || (CreateProjectVersionResponse = {}));\nexport var ResourceNotFoundException;\n(function (ResourceNotFoundException) {\n  ResourceNotFoundException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ResourceNotFoundException || (ResourceNotFoundException = {}));\nexport var KinesisVideoStream;\n(function (KinesisVideoStream) {\n  KinesisVideoStream.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(KinesisVideoStream || (KinesisVideoStream = {}));\nexport var StreamProcessorInput;\n(function (StreamProcessorInput) {\n  StreamProcessorInput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StreamProcessorInput || (StreamProcessorInput = {}));\nexport var KinesisDataStream;\n(function (KinesisDataStream) {\n  KinesisDataStream.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(KinesisDataStream || (KinesisDataStream = {}));\nexport var StreamProcessorOutput;\n(function (StreamProcessorOutput) {\n  StreamProcessorOutput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StreamProcessorOutput || (StreamProcessorOutput = {}));\nexport var FaceSearchSettings;\n(function (FaceSearchSettings) {\n  FaceSearchSettings.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(FaceSearchSettings || (FaceSearchSettings = {}));\nexport var StreamProcessorSettings;\n(function (StreamProcessorSettings) {\n  StreamProcessorSettings.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StreamProcessorSettings || (StreamProcessorSettings = {}));\nexport var CreateStreamProcessorRequest;\n(function (CreateStreamProcessorRequest) {\n  CreateStreamProcessorRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CreateStreamProcessorRequest || (CreateStreamProcessorRequest = {}));\nexport var CreateStreamProcessorResponse;\n(function (CreateStreamProcessorResponse) {\n  CreateStreamProcessorResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CreateStreamProcessorResponse || (CreateStreamProcessorResponse = {}));\nexport var Point;\n(function (Point) {\n  Point.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Point || (Point = {}));\nexport var Geometry;\n(function (Geometry) {\n  Geometry.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Geometry || (Geometry = {}));\nexport var CustomLabel;\n(function (CustomLabel) {\n  CustomLabel.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(CustomLabel || (CustomLabel = {}));\nexport var DeleteCollectionRequest;\n(function (DeleteCollectionRequest) {\n  DeleteCollectionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DeleteCollectionRequest || (DeleteCollectionRequest = {}));\nexport var DeleteCollectionResponse;\n(function (DeleteCollectionResponse) {\n  DeleteCollectionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DeleteCollectionResponse || (DeleteCollectionResponse = {}));\nexport var DeleteFacesRequest;\n(function (DeleteFacesRequest) {\n  DeleteFacesRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DeleteFacesRequest || (DeleteFacesRequest = {}));\nexport var DeleteFacesResponse;\n(function (DeleteFacesResponse) {\n  DeleteFacesResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DeleteFacesResponse || (DeleteFacesResponse = {}));\nexport var DeleteProjectRequest;\n(function (DeleteProjectRequest) {\n  DeleteProjectRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DeleteProjectRequest || (DeleteProjectRequest = {}));\nexport var ProjectStatus;\n(function (ProjectStatus) {\n  ProjectStatus[\"CREATED\"] = \"CREATED\";\n  ProjectStatus[\"CREATING\"] = \"CREATING\";\n  ProjectStatus[\"DELETING\"] = \"DELETING\";\n})(ProjectStatus || (ProjectStatus = {}));\nexport var DeleteProjectResponse;\n(function (DeleteProjectResponse) {\n  DeleteProjectResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DeleteProjectResponse || (DeleteProjectResponse = {}));\nexport var DeleteProjectVersionRequest;\n(function (DeleteProjectVersionRequest) {\n  DeleteProjectVersionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DeleteProjectVersionRequest || (DeleteProjectVersionRequest = {}));\nexport var ProjectVersionStatus;\n(function (ProjectVersionStatus) {\n  ProjectVersionStatus[\"DELETING\"] = \"DELETING\";\n  ProjectVersionStatus[\"FAILED\"] = \"FAILED\";\n  ProjectVersionStatus[\"RUNNING\"] = \"RUNNING\";\n  ProjectVersionStatus[\"STARTING\"] = \"STARTING\";\n  ProjectVersionStatus[\"STOPPED\"] = \"STOPPED\";\n  ProjectVersionStatus[\"STOPPING\"] = \"STOPPING\";\n  ProjectVersionStatus[\"TRAINING_COMPLETED\"] = \"TRAINING_COMPLETED\";\n  ProjectVersionStatus[\"TRAINING_FAILED\"] = \"TRAINING_FAILED\";\n  ProjectVersionStatus[\"TRAINING_IN_PROGRESS\"] = \"TRAINING_IN_PROGRESS\";\n})(ProjectVersionStatus || (ProjectVersionStatus = {}));\nexport var DeleteProjectVersionResponse;\n(function (DeleteProjectVersionResponse) {\n  DeleteProjectVersionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DeleteProjectVersionResponse || (DeleteProjectVersionResponse = {}));\nexport var DeleteStreamProcessorRequest;\n(function (DeleteStreamProcessorRequest) {\n  DeleteStreamProcessorRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DeleteStreamProcessorRequest || (DeleteStreamProcessorRequest = {}));\nexport var DeleteStreamProcessorResponse;\n(function (DeleteStreamProcessorResponse) {\n  DeleteStreamProcessorResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DeleteStreamProcessorResponse || (DeleteStreamProcessorResponse = {}));\nexport var DescribeCollectionRequest;\n(function (DescribeCollectionRequest) {\n  DescribeCollectionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DescribeCollectionRequest || (DescribeCollectionRequest = {}));\nexport var DescribeCollectionResponse;\n(function (DescribeCollectionResponse) {\n  DescribeCollectionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DescribeCollectionResponse || (DescribeCollectionResponse = {}));\nexport var DescribeProjectsRequest;\n(function (DescribeProjectsRequest) {\n  DescribeProjectsRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DescribeProjectsRequest || (DescribeProjectsRequest = {}));\nexport var ProjectDescription;\n(function (ProjectDescription) {\n  ProjectDescription.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ProjectDescription || (ProjectDescription = {}));\nexport var DescribeProjectsResponse;\n(function (DescribeProjectsResponse) {\n  DescribeProjectsResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DescribeProjectsResponse || (DescribeProjectsResponse = {}));\nexport var InvalidPaginationTokenException;\n(function (InvalidPaginationTokenException) {\n  InvalidPaginationTokenException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(InvalidPaginationTokenException || (InvalidPaginationTokenException = {}));\nexport var DescribeProjectVersionsRequest;\n(function (DescribeProjectVersionsRequest) {\n  DescribeProjectVersionsRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DescribeProjectVersionsRequest || (DescribeProjectVersionsRequest = {}));\nexport var Summary;\n(function (Summary) {\n  Summary.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Summary || (Summary = {}));\nexport var EvaluationResult;\n(function (EvaluationResult) {\n  EvaluationResult.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(EvaluationResult || (EvaluationResult = {}));\nexport var ValidationData;\n(function (ValidationData) {\n  ValidationData.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ValidationData || (ValidationData = {}));\nexport var TestingDataResult;\n(function (TestingDataResult) {\n  TestingDataResult.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(TestingDataResult || (TestingDataResult = {}));\nexport var TrainingDataResult;\n(function (TrainingDataResult) {\n  TrainingDataResult.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(TrainingDataResult || (TrainingDataResult = {}));\nexport var ProjectVersionDescription;\n(function (ProjectVersionDescription) {\n  ProjectVersionDescription.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ProjectVersionDescription || (ProjectVersionDescription = {}));\nexport var DescribeProjectVersionsResponse;\n(function (DescribeProjectVersionsResponse) {\n  DescribeProjectVersionsResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DescribeProjectVersionsResponse || (DescribeProjectVersionsResponse = {}));\nexport var DescribeStreamProcessorRequest;\n(function (DescribeStreamProcessorRequest) {\n  DescribeStreamProcessorRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DescribeStreamProcessorRequest || (DescribeStreamProcessorRequest = {}));\nexport var StreamProcessorStatus;\n(function (StreamProcessorStatus) {\n  StreamProcessorStatus[\"FAILED\"] = \"FAILED\";\n  StreamProcessorStatus[\"RUNNING\"] = \"RUNNING\";\n  StreamProcessorStatus[\"STARTING\"] = \"STARTING\";\n  StreamProcessorStatus[\"STOPPED\"] = \"STOPPED\";\n  StreamProcessorStatus[\"STOPPING\"] = \"STOPPING\";\n})(StreamProcessorStatus || (StreamProcessorStatus = {}));\nexport var DescribeStreamProcessorResponse;\n(function (DescribeStreamProcessorResponse) {\n  DescribeStreamProcessorResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DescribeStreamProcessorResponse || (DescribeStreamProcessorResponse = {}));\nexport var DetectCustomLabelsRequest;\n(function (DetectCustomLabelsRequest) {\n  DetectCustomLabelsRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectCustomLabelsRequest || (DetectCustomLabelsRequest = {}));\nexport var DetectCustomLabelsResponse;\n(function (DetectCustomLabelsResponse) {\n  DetectCustomLabelsResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectCustomLabelsResponse || (DetectCustomLabelsResponse = {}));\nexport var ResourceNotReadyException;\n(function (ResourceNotReadyException) {\n  ResourceNotReadyException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ResourceNotReadyException || (ResourceNotReadyException = {}));\nexport var DetectFacesRequest;\n(function (DetectFacesRequest) {\n  DetectFacesRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectFacesRequest || (DetectFacesRequest = {}));\nexport var DetectFacesResponse;\n(function (DetectFacesResponse) {\n  DetectFacesResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectFacesResponse || (DetectFacesResponse = {}));\nexport var DetectionFilter;\n(function (DetectionFilter) {\n  DetectionFilter.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectionFilter || (DetectionFilter = {}));\nexport var DetectLabelsRequest;\n(function (DetectLabelsRequest) {\n  DetectLabelsRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectLabelsRequest || (DetectLabelsRequest = {}));\nexport var Instance;\n(function (Instance) {\n  Instance.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Instance || (Instance = {}));\nexport var Parent;\n(function (Parent) {\n  Parent.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Parent || (Parent = {}));\nexport var Label;\n(function (Label) {\n  Label.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Label || (Label = {}));\nexport var DetectLabelsResponse;\n(function (DetectLabelsResponse) {\n  DetectLabelsResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectLabelsResponse || (DetectLabelsResponse = {}));\nexport var HumanLoopDataAttributes;\n(function (HumanLoopDataAttributes) {\n  HumanLoopDataAttributes.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(HumanLoopDataAttributes || (HumanLoopDataAttributes = {}));\nexport var HumanLoopConfig;\n(function (HumanLoopConfig) {\n  HumanLoopConfig.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(HumanLoopConfig || (HumanLoopConfig = {}));\nexport var DetectModerationLabelsRequest;\n(function (DetectModerationLabelsRequest) {\n  DetectModerationLabelsRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectModerationLabelsRequest || (DetectModerationLabelsRequest = {}));\nexport var HumanLoopActivationOutput;\n(function (HumanLoopActivationOutput) {\n  HumanLoopActivationOutput.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(HumanLoopActivationOutput || (HumanLoopActivationOutput = {}));\nexport var DetectModerationLabelsResponse;\n(function (DetectModerationLabelsResponse) {\n  DetectModerationLabelsResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectModerationLabelsResponse || (DetectModerationLabelsResponse = {}));\nexport var HumanLoopQuotaExceededException;\n(function (HumanLoopQuotaExceededException) {\n  HumanLoopQuotaExceededException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(HumanLoopQuotaExceededException || (HumanLoopQuotaExceededException = {}));\nexport var ProtectiveEquipmentSummarizationAttributes;\n(function (ProtectiveEquipmentSummarizationAttributes) {\n  ProtectiveEquipmentSummarizationAttributes.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ProtectiveEquipmentSummarizationAttributes || (ProtectiveEquipmentSummarizationAttributes = {}));\nexport var DetectProtectiveEquipmentRequest;\n(function (DetectProtectiveEquipmentRequest) {\n  DetectProtectiveEquipmentRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectProtectiveEquipmentRequest || (DetectProtectiveEquipmentRequest = {}));\nexport var ProtectiveEquipmentPerson;\n(function (ProtectiveEquipmentPerson) {\n  ProtectiveEquipmentPerson.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ProtectiveEquipmentPerson || (ProtectiveEquipmentPerson = {}));\nexport var ProtectiveEquipmentSummary;\n(function (ProtectiveEquipmentSummary) {\n  ProtectiveEquipmentSummary.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ProtectiveEquipmentSummary || (ProtectiveEquipmentSummary = {}));\nexport var DetectProtectiveEquipmentResponse;\n(function (DetectProtectiveEquipmentResponse) {\n  DetectProtectiveEquipmentResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectProtectiveEquipmentResponse || (DetectProtectiveEquipmentResponse = {}));\nexport var RegionOfInterest;\n(function (RegionOfInterest) {\n  RegionOfInterest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(RegionOfInterest || (RegionOfInterest = {}));\nexport var DetectTextFilters;\n(function (DetectTextFilters) {\n  DetectTextFilters.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectTextFilters || (DetectTextFilters = {}));\nexport var DetectTextRequest;\n(function (DetectTextRequest) {\n  DetectTextRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectTextRequest || (DetectTextRequest = {}));\nexport var TextTypes;\n(function (TextTypes) {\n  TextTypes[\"LINE\"] = \"LINE\";\n  TextTypes[\"WORD\"] = \"WORD\";\n})(TextTypes || (TextTypes = {}));\nexport var TextDetection;\n(function (TextDetection) {\n  TextDetection.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(TextDetection || (TextDetection = {}));\nexport var DetectTextResponse;\n(function (DetectTextResponse) {\n  DetectTextResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(DetectTextResponse || (DetectTextResponse = {}));\nexport var Face;\n(function (Face) {\n  Face.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Face || (Face = {}));\nexport var FaceAttributes;\n(function (FaceAttributes) {\n  FaceAttributes[\"ALL\"] = \"ALL\";\n  FaceAttributes[\"DEFAULT\"] = \"DEFAULT\";\n})(FaceAttributes || (FaceAttributes = {}));\nexport var FaceDetection;\n(function (FaceDetection) {\n  FaceDetection.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(FaceDetection || (FaceDetection = {}));\nexport var FaceMatch;\n(function (FaceMatch) {\n  FaceMatch.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(FaceMatch || (FaceMatch = {}));\nexport var FaceRecord;\n(function (FaceRecord) {\n  FaceRecord.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(FaceRecord || (FaceRecord = {}));\nexport var FaceSearchSortBy;\n(function (FaceSearchSortBy) {\n  FaceSearchSortBy[\"INDEX\"] = \"INDEX\";\n  FaceSearchSortBy[\"TIMESTAMP\"] = \"TIMESTAMP\";\n})(FaceSearchSortBy || (FaceSearchSortBy = {}));\nexport var GetCelebrityInfoRequest;\n(function (GetCelebrityInfoRequest) {\n  GetCelebrityInfoRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetCelebrityInfoRequest || (GetCelebrityInfoRequest = {}));\nexport var GetCelebrityInfoResponse;\n(function (GetCelebrityInfoResponse) {\n  GetCelebrityInfoResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetCelebrityInfoResponse || (GetCelebrityInfoResponse = {}));\nexport var GetCelebrityRecognitionRequest;\n(function (GetCelebrityRecognitionRequest) {\n  GetCelebrityRecognitionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetCelebrityRecognitionRequest || (GetCelebrityRecognitionRequest = {}));\nexport var VideoJobStatus;\n(function (VideoJobStatus) {\n  VideoJobStatus[\"FAILED\"] = \"FAILED\";\n  VideoJobStatus[\"IN_PROGRESS\"] = \"IN_PROGRESS\";\n  VideoJobStatus[\"SUCCEEDED\"] = \"SUCCEEDED\";\n})(VideoJobStatus || (VideoJobStatus = {}));\nexport var VideoMetadata;\n(function (VideoMetadata) {\n  VideoMetadata.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(VideoMetadata || (VideoMetadata = {}));\nexport var GetCelebrityRecognitionResponse;\n(function (GetCelebrityRecognitionResponse) {\n  GetCelebrityRecognitionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetCelebrityRecognitionResponse || (GetCelebrityRecognitionResponse = {}));\nexport var GetContentModerationRequest;\n(function (GetContentModerationRequest) {\n  GetContentModerationRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetContentModerationRequest || (GetContentModerationRequest = {}));\nexport var GetContentModerationResponse;\n(function (GetContentModerationResponse) {\n  GetContentModerationResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetContentModerationResponse || (GetContentModerationResponse = {}));\nexport var GetFaceDetectionRequest;\n(function (GetFaceDetectionRequest) {\n  GetFaceDetectionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetFaceDetectionRequest || (GetFaceDetectionRequest = {}));\nexport var GetFaceDetectionResponse;\n(function (GetFaceDetectionResponse) {\n  GetFaceDetectionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetFaceDetectionResponse || (GetFaceDetectionResponse = {}));\nexport var GetFaceSearchRequest;\n(function (GetFaceSearchRequest) {\n  GetFaceSearchRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetFaceSearchRequest || (GetFaceSearchRequest = {}));\nexport var PersonDetail;\n(function (PersonDetail) {\n  PersonDetail.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(PersonDetail || (PersonDetail = {}));\nexport var PersonMatch;\n(function (PersonMatch) {\n  PersonMatch.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(PersonMatch || (PersonMatch = {}));\nexport var GetFaceSearchResponse;\n(function (GetFaceSearchResponse) {\n  GetFaceSearchResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetFaceSearchResponse || (GetFaceSearchResponse = {}));\nexport var LabelDetectionSortBy;\n(function (LabelDetectionSortBy) {\n  LabelDetectionSortBy[\"NAME\"] = \"NAME\";\n  LabelDetectionSortBy[\"TIMESTAMP\"] = \"TIMESTAMP\";\n})(LabelDetectionSortBy || (LabelDetectionSortBy = {}));\nexport var GetLabelDetectionRequest;\n(function (GetLabelDetectionRequest) {\n  GetLabelDetectionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetLabelDetectionRequest || (GetLabelDetectionRequest = {}));\nexport var LabelDetection;\n(function (LabelDetection) {\n  LabelDetection.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(LabelDetection || (LabelDetection = {}));\nexport var GetLabelDetectionResponse;\n(function (GetLabelDetectionResponse) {\n  GetLabelDetectionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetLabelDetectionResponse || (GetLabelDetectionResponse = {}));\nexport var PersonTrackingSortBy;\n(function (PersonTrackingSortBy) {\n  PersonTrackingSortBy[\"INDEX\"] = \"INDEX\";\n  PersonTrackingSortBy[\"TIMESTAMP\"] = \"TIMESTAMP\";\n})(PersonTrackingSortBy || (PersonTrackingSortBy = {}));\nexport var GetPersonTrackingRequest;\n(function (GetPersonTrackingRequest) {\n  GetPersonTrackingRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetPersonTrackingRequest || (GetPersonTrackingRequest = {}));\nexport var PersonDetection;\n(function (PersonDetection) {\n  PersonDetection.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(PersonDetection || (PersonDetection = {}));\nexport var GetPersonTrackingResponse;\n(function (GetPersonTrackingResponse) {\n  GetPersonTrackingResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetPersonTrackingResponse || (GetPersonTrackingResponse = {}));\nexport var GetSegmentDetectionRequest;\n(function (GetSegmentDetectionRequest) {\n  GetSegmentDetectionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetSegmentDetectionRequest || (GetSegmentDetectionRequest = {}));\nexport var ShotSegment;\n(function (ShotSegment) {\n  ShotSegment.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ShotSegment || (ShotSegment = {}));\nexport var TechnicalCueType;\n(function (TechnicalCueType) {\n  TechnicalCueType[\"BLACK_FRAMES\"] = \"BlackFrames\";\n  TechnicalCueType[\"COLOR_BARS\"] = \"ColorBars\";\n  TechnicalCueType[\"END_CREDITS\"] = \"EndCredits\";\n})(TechnicalCueType || (TechnicalCueType = {}));\nexport var TechnicalCueSegment;\n(function (TechnicalCueSegment) {\n  TechnicalCueSegment.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(TechnicalCueSegment || (TechnicalCueSegment = {}));\nexport var SegmentType;\n(function (SegmentType) {\n  SegmentType[\"SHOT\"] = \"SHOT\";\n  SegmentType[\"TECHNICAL_CUE\"] = \"TECHNICAL_CUE\";\n})(SegmentType || (SegmentType = {}));\nexport var SegmentDetection;\n(function (SegmentDetection) {\n  SegmentDetection.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(SegmentDetection || (SegmentDetection = {}));\nexport var SegmentTypeInfo;\n(function (SegmentTypeInfo) {\n  SegmentTypeInfo.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(SegmentTypeInfo || (SegmentTypeInfo = {}));\nexport var GetSegmentDetectionResponse;\n(function (GetSegmentDetectionResponse) {\n  GetSegmentDetectionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetSegmentDetectionResponse || (GetSegmentDetectionResponse = {}));\nexport var GetTextDetectionRequest;\n(function (GetTextDetectionRequest) {\n  GetTextDetectionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetTextDetectionRequest || (GetTextDetectionRequest = {}));\nexport var TextDetectionResult;\n(function (TextDetectionResult) {\n  TextDetectionResult.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(TextDetectionResult || (TextDetectionResult = {}));\nexport var GetTextDetectionResponse;\n(function (GetTextDetectionResponse) {\n  GetTextDetectionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(GetTextDetectionResponse || (GetTextDetectionResponse = {}));\nexport var IdempotentParameterMismatchException;\n(function (IdempotentParameterMismatchException) {\n  IdempotentParameterMismatchException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(IdempotentParameterMismatchException || (IdempotentParameterMismatchException = {}));\nexport var IndexFacesRequest;\n(function (IndexFacesRequest) {\n  IndexFacesRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(IndexFacesRequest || (IndexFacesRequest = {}));\nexport var Reason;\n(function (Reason) {\n  Reason[\"EXCEEDS_MAX_FACES\"] = \"EXCEEDS_MAX_FACES\";\n  Reason[\"EXTREME_POSE\"] = \"EXTREME_POSE\";\n  Reason[\"LOW_BRIGHTNESS\"] = \"LOW_BRIGHTNESS\";\n  Reason[\"LOW_CONFIDENCE\"] = \"LOW_CONFIDENCE\";\n  Reason[\"LOW_FACE_QUALITY\"] = \"LOW_FACE_QUALITY\";\n  Reason[\"LOW_SHARPNESS\"] = \"LOW_SHARPNESS\";\n  Reason[\"SMALL_BOUNDING_BOX\"] = \"SMALL_BOUNDING_BOX\";\n})(Reason || (Reason = {}));\nexport var UnindexedFace;\n(function (UnindexedFace) {\n  UnindexedFace.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(UnindexedFace || (UnindexedFace = {}));\nexport var IndexFacesResponse;\n(function (IndexFacesResponse) {\n  IndexFacesResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(IndexFacesResponse || (IndexFacesResponse = {}));\nexport var ServiceQuotaExceededException;\n(function (ServiceQuotaExceededException) {\n  ServiceQuotaExceededException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ServiceQuotaExceededException || (ServiceQuotaExceededException = {}));\nexport var ListCollectionsRequest;\n(function (ListCollectionsRequest) {\n  ListCollectionsRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ListCollectionsRequest || (ListCollectionsRequest = {}));\nexport var ListCollectionsResponse;\n(function (ListCollectionsResponse) {\n  ListCollectionsResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ListCollectionsResponse || (ListCollectionsResponse = {}));\nexport var ListFacesRequest;\n(function (ListFacesRequest) {\n  ListFacesRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ListFacesRequest || (ListFacesRequest = {}));\nexport var ListFacesResponse;\n(function (ListFacesResponse) {\n  ListFacesResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ListFacesResponse || (ListFacesResponse = {}));\nexport var ListStreamProcessorsRequest;\n(function (ListStreamProcessorsRequest) {\n  ListStreamProcessorsRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ListStreamProcessorsRequest || (ListStreamProcessorsRequest = {}));\nexport var StreamProcessor;\n(function (StreamProcessor) {\n  StreamProcessor.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StreamProcessor || (StreamProcessor = {}));\nexport var ListStreamProcessorsResponse;\n(function (ListStreamProcessorsResponse) {\n  ListStreamProcessorsResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(ListStreamProcessorsResponse || (ListStreamProcessorsResponse = {}));\nexport var NotificationChannel;\n(function (NotificationChannel) {\n  NotificationChannel.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(NotificationChannel || (NotificationChannel = {}));\nexport var RecognizeCelebritiesRequest;\n(function (RecognizeCelebritiesRequest) {\n  RecognizeCelebritiesRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(RecognizeCelebritiesRequest || (RecognizeCelebritiesRequest = {}));\nexport var RecognizeCelebritiesResponse;\n(function (RecognizeCelebritiesResponse) {\n  RecognizeCelebritiesResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(RecognizeCelebritiesResponse || (RecognizeCelebritiesResponse = {}));\nexport var SearchFacesRequest;\n(function (SearchFacesRequest) {\n  SearchFacesRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(SearchFacesRequest || (SearchFacesRequest = {}));\nexport var SearchFacesResponse;\n(function (SearchFacesResponse) {\n  SearchFacesResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(SearchFacesResponse || (SearchFacesResponse = {}));\nexport var SearchFacesByImageRequest;\n(function (SearchFacesByImageRequest) {\n  SearchFacesByImageRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(SearchFacesByImageRequest || (SearchFacesByImageRequest = {}));\nexport var SearchFacesByImageResponse;\n(function (SearchFacesByImageResponse) {\n  SearchFacesByImageResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(SearchFacesByImageResponse || (SearchFacesByImageResponse = {}));\nexport var Video;\n(function (Video) {\n  Video.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(Video || (Video = {}));\nexport var StartCelebrityRecognitionRequest;\n(function (StartCelebrityRecognitionRequest) {\n  StartCelebrityRecognitionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartCelebrityRecognitionRequest || (StartCelebrityRecognitionRequest = {}));\nexport var StartCelebrityRecognitionResponse;\n(function (StartCelebrityRecognitionResponse) {\n  StartCelebrityRecognitionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartCelebrityRecognitionResponse || (StartCelebrityRecognitionResponse = {}));\nexport var VideoTooLargeException;\n(function (VideoTooLargeException) {\n  VideoTooLargeException.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(VideoTooLargeException || (VideoTooLargeException = {}));\nexport var StartContentModerationRequest;\n(function (StartContentModerationRequest) {\n  StartContentModerationRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartContentModerationRequest || (StartContentModerationRequest = {}));\nexport var StartContentModerationResponse;\n(function (StartContentModerationResponse) {\n  StartContentModerationResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartContentModerationResponse || (StartContentModerationResponse = {}));\nexport var StartFaceDetectionRequest;\n(function (StartFaceDetectionRequest) {\n  StartFaceDetectionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartFaceDetectionRequest || (StartFaceDetectionRequest = {}));\nexport var StartFaceDetectionResponse;\n(function (StartFaceDetectionResponse) {\n  StartFaceDetectionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartFaceDetectionResponse || (StartFaceDetectionResponse = {}));\nexport var StartFaceSearchRequest;\n(function (StartFaceSearchRequest) {\n  StartFaceSearchRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartFaceSearchRequest || (StartFaceSearchRequest = {}));\nexport var StartFaceSearchResponse;\n(function (StartFaceSearchResponse) {\n  StartFaceSearchResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartFaceSearchResponse || (StartFaceSearchResponse = {}));\nexport var StartLabelDetectionRequest;\n(function (StartLabelDetectionRequest) {\n  StartLabelDetectionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartLabelDetectionRequest || (StartLabelDetectionRequest = {}));\nexport var StartLabelDetectionResponse;\n(function (StartLabelDetectionResponse) {\n  StartLabelDetectionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartLabelDetectionResponse || (StartLabelDetectionResponse = {}));\nexport var StartPersonTrackingRequest;\n(function (StartPersonTrackingRequest) {\n  StartPersonTrackingRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartPersonTrackingRequest || (StartPersonTrackingRequest = {}));\nexport var StartPersonTrackingResponse;\n(function (StartPersonTrackingResponse) {\n  StartPersonTrackingResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartPersonTrackingResponse || (StartPersonTrackingResponse = {}));\nexport var StartProjectVersionRequest;\n(function (StartProjectVersionRequest) {\n  StartProjectVersionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartProjectVersionRequest || (StartProjectVersionRequest = {}));\nexport var StartProjectVersionResponse;\n(function (StartProjectVersionResponse) {\n  StartProjectVersionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartProjectVersionResponse || (StartProjectVersionResponse = {}));\nexport var StartShotDetectionFilter;\n(function (StartShotDetectionFilter) {\n  StartShotDetectionFilter.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartShotDetectionFilter || (StartShotDetectionFilter = {}));\nexport var StartTechnicalCueDetectionFilter;\n(function (StartTechnicalCueDetectionFilter) {\n  StartTechnicalCueDetectionFilter.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartTechnicalCueDetectionFilter || (StartTechnicalCueDetectionFilter = {}));\nexport var StartSegmentDetectionFilters;\n(function (StartSegmentDetectionFilters) {\n  StartSegmentDetectionFilters.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartSegmentDetectionFilters || (StartSegmentDetectionFilters = {}));\nexport var StartSegmentDetectionRequest;\n(function (StartSegmentDetectionRequest) {\n  StartSegmentDetectionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartSegmentDetectionRequest || (StartSegmentDetectionRequest = {}));\nexport var StartSegmentDetectionResponse;\n(function (StartSegmentDetectionResponse) {\n  StartSegmentDetectionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartSegmentDetectionResponse || (StartSegmentDetectionResponse = {}));\nexport var StartStreamProcessorRequest;\n(function (StartStreamProcessorRequest) {\n  StartStreamProcessorRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartStreamProcessorRequest || (StartStreamProcessorRequest = {}));\nexport var StartStreamProcessorResponse;\n(function (StartStreamProcessorResponse) {\n  StartStreamProcessorResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartStreamProcessorResponse || (StartStreamProcessorResponse = {}));\nexport var StartTextDetectionFilters;\n(function (StartTextDetectionFilters) {\n  StartTextDetectionFilters.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartTextDetectionFilters || (StartTextDetectionFilters = {}));\nexport var StartTextDetectionRequest;\n(function (StartTextDetectionRequest) {\n  StartTextDetectionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartTextDetectionRequest || (StartTextDetectionRequest = {}));\nexport var StartTextDetectionResponse;\n(function (StartTextDetectionResponse) {\n  StartTextDetectionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StartTextDetectionResponse || (StartTextDetectionResponse = {}));\nexport var StopProjectVersionRequest;\n(function (StopProjectVersionRequest) {\n  StopProjectVersionRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StopProjectVersionRequest || (StopProjectVersionRequest = {}));\nexport var StopProjectVersionResponse;\n(function (StopProjectVersionResponse) {\n  StopProjectVersionResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StopProjectVersionResponse || (StopProjectVersionResponse = {}));\nexport var StopStreamProcessorRequest;\n(function (StopStreamProcessorRequest) {\n  StopStreamProcessorRequest.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StopStreamProcessorRequest || (StopStreamProcessorRequest = {}));\nexport var StopStreamProcessorResponse;\n(function (StopStreamProcessorResponse) {\n  StopStreamProcessorResponse.filterSensitiveLog = function (obj) {\n    return __assign({}, obj);\n  };\n})(StopStreamProcessorResponse || (StopStreamProcessorResponse = {}));","map":{"version":3,"names":["AccessDeniedException","filterSensitiveLog","obj","__assign","AgeRange","S3Object","GroundTruthManifest","Asset","Attribute","AudioMetadata","Beard","BodyPart","BoundingBox","CoversBodyPart","ProtectiveEquipmentType","EquipmentDetection","ProtectiveEquipmentBodyPart","LandmarkType","Landmark","Pose","ImageQuality","ComparedFace","Celebrity","Emotion","Eyeglasses","EyeOpen","GenderType","Gender","MouthOpen","Mustache","Smile","Sunglasses","FaceDetail","CelebrityDetail","CelebrityRecognition","CelebrityRecognitionSortBy","ComparedSourceImageFace","QualityFilter","Image","CompareFacesRequest","CompareFacesMatch","OrientationCorrection","CompareFacesResponse","ImageTooLargeException","InternalServerError","InvalidImageFormatException","InvalidParameterException","InvalidS3ObjectException","ProvisionedThroughputExceededException","ThrottlingException","ContentClassifier","ModerationLabel","ContentModerationDetection","ContentModerationSortBy","CreateCollectionRequest","CreateCollectionResponse","ResourceAlreadyExistsException","CreateProjectRequest","CreateProjectResponse","LimitExceededException","ResourceInUseException","OutputConfig","TestingData","TrainingData","CreateProjectVersionRequest","CreateProjectVersionResponse","ResourceNotFoundException","KinesisVideoStream","StreamProcessorInput","KinesisDataStream","StreamProcessorOutput","FaceSearchSettings","StreamProcessorSettings","CreateStreamProcessorRequest","CreateStreamProcessorResponse","Point","Geometry","CustomLabel","DeleteCollectionRequest","DeleteCollectionResponse","DeleteFacesRequest","DeleteFacesResponse","DeleteProjectRequest","ProjectStatus","DeleteProjectResponse","DeleteProjectVersionRequest","ProjectVersionStatus","DeleteProjectVersionResponse","DeleteStreamProcessorRequest","DeleteStreamProcessorResponse","DescribeCollectionRequest","DescribeCollectionResponse","DescribeProjectsRequest","ProjectDescription","DescribeProjectsResponse","InvalidPaginationTokenException","DescribeProjectVersionsRequest","Summary","EvaluationResult","ValidationData","TestingDataResult","TrainingDataResult","ProjectVersionDescription","DescribeProjectVersionsResponse","DescribeStreamProcessorRequest","StreamProcessorStatus","DescribeStreamProcessorResponse","DetectCustomLabelsRequest","DetectCustomLabelsResponse","ResourceNotReadyException","DetectFacesRequest","DetectFacesResponse","DetectionFilter","DetectLabelsRequest","Instance","Parent","Label","DetectLabelsResponse","HumanLoopDataAttributes","HumanLoopConfig","DetectModerationLabelsRequest","HumanLoopActivationOutput","DetectModerationLabelsResponse","HumanLoopQuotaExceededException","ProtectiveEquipmentSummarizationAttributes","DetectProtectiveEquipmentRequest","ProtectiveEquipmentPerson","ProtectiveEquipmentSummary","DetectProtectiveEquipmentResponse","RegionOfInterest","DetectTextFilters","DetectTextRequest","TextTypes","TextDetection","DetectTextResponse","Face","FaceAttributes","FaceDetection","FaceMatch","FaceRecord","FaceSearchSortBy","GetCelebrityInfoRequest","GetCelebrityInfoResponse","GetCelebrityRecognitionRequest","VideoJobStatus","VideoMetadata","GetCelebrityRecognitionResponse","GetContentModerationRequest","GetContentModerationResponse","GetFaceDetectionRequest","GetFaceDetectionResponse","GetFaceSearchRequest","PersonDetail","PersonMatch","GetFaceSearchResponse","LabelDetectionSortBy","GetLabelDetectionRequest","LabelDetection","GetLabelDetectionResponse","PersonTrackingSortBy","GetPersonTrackingRequest","PersonDetection","GetPersonTrackingResponse","GetSegmentDetectionRequest","ShotSegment","TechnicalCueType","TechnicalCueSegment","SegmentType","SegmentDetection","SegmentTypeInfo","GetSegmentDetectionResponse","GetTextDetectionRequest","TextDetectionResult","GetTextDetectionResponse","IdempotentParameterMismatchException","IndexFacesRequest","Reason","UnindexedFace","IndexFacesResponse","ServiceQuotaExceededException","ListCollectionsRequest","ListCollectionsResponse","ListFacesRequest","ListFacesResponse","ListStreamProcessorsRequest","StreamProcessor","ListStreamProcessorsResponse","NotificationChannel","RecognizeCelebritiesRequest","RecognizeCelebritiesResponse","SearchFacesRequest","SearchFacesResponse","SearchFacesByImageRequest","SearchFacesByImageResponse","Video","StartCelebrityRecognitionRequest","StartCelebrityRecognitionResponse","VideoTooLargeException","StartContentModerationRequest","StartContentModerationResponse","StartFaceDetectionRequest","StartFaceDetectionResponse","StartFaceSearchRequest","StartFaceSearchResponse","StartLabelDetectionRequest","StartLabelDetectionResponse","StartPersonTrackingRequest","StartPersonTrackingResponse","StartProjectVersionRequest","StartProjectVersionResponse","StartShotDetectionFilter","StartTechnicalCueDetectionFilter","StartSegmentDetectionFilters","StartSegmentDetectionRequest","StartSegmentDetectionResponse","StartStreamProcessorRequest","StartStreamProcessorResponse","StartTextDetectionFilters","StartTextDetectionRequest","StartTextDetectionResponse","StopProjectVersionRequest","StopProjectVersionResponse","StopStreamProcessorRequest","StopStreamProcessorResponse"],"sources":["C:\\Users\\jonat\\aws_poc\\frontend\\node_modules\\@aws-sdk\\client-rekognition\\models\\models_0.ts"],"sourcesContent":["import { LazyJsonString as __LazyJsonString, SmithyException as __SmithyException } from \"@aws-sdk/smithy-client\";\nimport { MetadataBearer as $MetadataBearer } from \"@aws-sdk/types\";\n\n/**\n * <p>You are not authorized to perform the action.</p>\n */\nexport interface AccessDeniedException extends __SmithyException, $MetadataBearer {\n  name: \"AccessDeniedException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace AccessDeniedException {\n  export const filterSensitiveLog = (obj: AccessDeniedException): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Structure containing the estimated age range, in years, for a face.</p>\n *          <p>Amazon Rekognition estimates an age range for faces detected in the input image. Estimated age\n *       ranges can overlap. A face of a 5-year-old might have an estimated range of 4-6, while the\n *       face of a 6-year-old might have an estimated range of 4-8.</p>\n */\nexport interface AgeRange {\n  /**\n   * <p>The lowest estimated age.</p>\n   */\n  Low?: number;\n\n  /**\n   * <p>The highest estimated age.</p>\n   */\n  High?: number;\n}\n\nexport namespace AgeRange {\n  export const filterSensitiveLog = (obj: AgeRange): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Provides the S3 bucket name and object name.</p>\n *          <p>The region for the S3 bucket containing the S3 object must match the region you use for\n *       Amazon Rekognition operations.</p>\n *\n *          <p>For Amazon Rekognition to process an S3 object, the user must have permission to\n *       access the S3 object. For more information, see Resource-Based Policies in the Amazon Rekognition\n *       Developer Guide. </p>\n */\nexport interface S3Object {\n  /**\n   * <p>Name of the S3 bucket.</p>\n   */\n  Bucket?: string;\n\n  /**\n   * <p>S3 object key name.</p>\n   */\n  Name?: string;\n\n  /**\n   * <p>If the bucket is versioning enabled, you can specify the object version. </p>\n   */\n  Version?: string;\n}\n\nexport namespace S3Object {\n  export const filterSensitiveLog = (obj: S3Object): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The S3 bucket that contains an Amazon Sagemaker Ground Truth format manifest file.\n * </p>\n */\nexport interface GroundTruthManifest {\n  /**\n   * <p>Provides the S3 bucket name and object name.</p>\n   *          <p>The region for the S3 bucket containing the S3 object must match the region you use for\n   *       Amazon Rekognition operations.</p>\n   *\n   *          <p>For Amazon Rekognition to process an S3 object, the user must have permission to\n   *       access the S3 object. For more information, see Resource-Based Policies in the Amazon Rekognition\n   *       Developer Guide. </p>\n   */\n  S3Object?: S3Object;\n}\n\nexport namespace GroundTruthManifest {\n  export const filterSensitiveLog = (obj: GroundTruthManifest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Assets are the images that you use to train and evaluate a model version.\n *          Assets can also contain validation information that you use to debug a failed model training.\n *          </p>\n */\nexport interface Asset {\n  /**\n   * <p>The S3 bucket that contains an Amazon Sagemaker Ground Truth format manifest file.\n   * </p>\n   */\n  GroundTruthManifest?: GroundTruthManifest;\n}\n\nexport namespace Asset {\n  export const filterSensitiveLog = (obj: Asset): any => ({\n    ...obj,\n  });\n}\n\nexport enum Attribute {\n  ALL = \"ALL\",\n  DEFAULT = \"DEFAULT\",\n}\n\n/**\n * <p>Metadata information about an audio stream. An array of <code>AudioMetadata</code> objects\n *       for the audio streams found in a stored video is returned by <a>GetSegmentDetection</a>. </p>\n */\nexport interface AudioMetadata {\n  /**\n   * <p>The audio codec used to encode or decode the audio stream. </p>\n   */\n  Codec?: string;\n\n  /**\n   * <p>The duration of the audio stream in milliseconds.</p>\n   */\n  DurationMillis?: number;\n\n  /**\n   * <p>The sample rate for the audio stream.</p>\n   */\n  SampleRate?: number;\n\n  /**\n   * <p>The number of audio channels in the segment.</p>\n   */\n  NumberOfChannels?: number;\n}\n\nexport namespace AudioMetadata {\n  export const filterSensitiveLog = (obj: AudioMetadata): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Indicates whether or not the face has a beard, and the confidence level in the\n *       determination.</p>\n */\nexport interface Beard {\n  /**\n   * <p>Boolean value that indicates whether the face has beard or not.</p>\n   */\n  Value?: boolean;\n\n  /**\n   * <p>Level of confidence in the determination.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace Beard {\n  export const filterSensitiveLog = (obj: Beard): any => ({\n    ...obj,\n  });\n}\n\nexport enum BodyPart {\n  FACE = \"FACE\",\n  HEAD = \"HEAD\",\n  LEFT_HAND = \"LEFT_HAND\",\n  RIGHT_HAND = \"RIGHT_HAND\",\n}\n\n/**\n * <p>Identifies the bounding box around the label, face, text or personal protective equipment.\n *       The <code>left</code> (x-coordinate) and <code>top</code> (y-coordinate) are coordinates representing the top and\n *       left sides of the bounding box. Note that the upper-left corner of the image is the origin\n *       (0,0). </p>\n *          <p>The <code>top</code> and <code>left</code> values returned are ratios of the overall\n *       image size. For example, if the input image is 700x200 pixels, and the top-left coordinate of\n *       the bounding box is 350x50 pixels, the API returns a <code>left</code> value of 0.5 (350/700)\n *       and a <code>top</code> value of 0.25 (50/200).</p>\n *          <p>The <code>width</code> and <code>height</code> values represent the dimensions of the\n *       bounding box as a ratio of the overall image dimension. For example, if the input image is\n *       700x200 pixels, and the bounding box width is 70 pixels, the width returned is 0.1. </p>\n *          <note>\n *             <p> The bounding box coordinates can have negative values. For example, if Amazon Rekognition is\n *         able to detect a face that is at the image edge and is only partially visible, the service\n *         can return coordinates that are outside the image bounds and, depending on the image edge,\n *         you might get negative values or values greater than 1 for the <code>left</code> or\n *           <code>top</code> values. </p>\n *          </note>\n */\nexport interface BoundingBox {\n  /**\n   * <p>Width of the bounding box as a ratio of the overall image width.</p>\n   */\n  Width?: number;\n\n  /**\n   * <p>Height of the bounding box as a ratio of the overall image height.</p>\n   */\n  Height?: number;\n\n  /**\n   * <p>Left coordinate of the bounding box as a ratio of overall image width.</p>\n   */\n  Left?: number;\n\n  /**\n   * <p>Top coordinate of the bounding box as a ratio of overall image height.</p>\n   */\n  Top?: number;\n}\n\nexport namespace BoundingBox {\n  export const filterSensitiveLog = (obj: BoundingBox): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Information about an item of Personal Protective Equipment covering a corresponding body part. For more\n *          information, see <a>DetectProtectiveEquipment</a>.</p>\n */\nexport interface CoversBodyPart {\n  /**\n   * <p>The confidence that Amazon Rekognition has in the value of <code>Value</code>.</p>\n   */\n  Confidence?: number;\n\n  /**\n   * <p>True if the PPE covers the corresponding body part, otherwise false.</p>\n   */\n  Value?: boolean;\n}\n\nexport namespace CoversBodyPart {\n  export const filterSensitiveLog = (obj: CoversBodyPart): any => ({\n    ...obj,\n  });\n}\n\nexport enum ProtectiveEquipmentType {\n  FACE_COVER = \"FACE_COVER\",\n  HAND_COVER = \"HAND_COVER\",\n  HEAD_COVER = \"HEAD_COVER\",\n}\n\n/**\n * <p>Information about an item of Personal Protective Equipment (PPE) detected by\n *          <a>DetectProtectiveEquipment</a>. For more\n *          information, see <a>DetectProtectiveEquipment</a>.</p>\n */\nexport interface EquipmentDetection {\n  /**\n   * <p>A bounding box surrounding the item of detected PPE.</p>\n   */\n  BoundingBox?: BoundingBox;\n\n  /**\n   * <p>The confidence that Amazon Rekognition has that the bounding box (<code>BoundingBox</code>) contains an item of PPE.</p>\n   */\n  Confidence?: number;\n\n  /**\n   * <p>The type of detected PPE.</p>\n   */\n  Type?: ProtectiveEquipmentType | string;\n\n  /**\n   * <p>Information about the body part covered by the detected PPE.</p>\n   */\n  CoversBodyPart?: CoversBodyPart;\n}\n\nexport namespace EquipmentDetection {\n  export const filterSensitiveLog = (obj: EquipmentDetection): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Information about a body part detected by <a>DetectProtectiveEquipment</a> that contains PPE.\n *          An array of <code>ProtectiveEquipmentBodyPart</code> objects is returned for each person detected by\n *          <code>DetectProtectiveEquipment</code>. </p>\n */\nexport interface ProtectiveEquipmentBodyPart {\n  /**\n   * <p>The detected body part.</p>\n   */\n  Name?: BodyPart | string;\n\n  /**\n   * <p>The confidence that Amazon Rekognition has in the detection accuracy of the detected body part.\n   *       </p>\n   */\n  Confidence?: number;\n\n  /**\n   * <p>An array of Personal Protective Equipment items detected around a body part.</p>\n   */\n  EquipmentDetections?: EquipmentDetection[];\n}\n\nexport namespace ProtectiveEquipmentBodyPart {\n  export const filterSensitiveLog = (obj: ProtectiveEquipmentBodyPart): any => ({\n    ...obj,\n  });\n}\n\nexport enum LandmarkType {\n  chinBottom = \"chinBottom\",\n  eyeLeft = \"eyeLeft\",\n  eyeRight = \"eyeRight\",\n  leftEyeBrowLeft = \"leftEyeBrowLeft\",\n  leftEyeBrowRight = \"leftEyeBrowRight\",\n  leftEyeBrowUp = \"leftEyeBrowUp\",\n  leftEyeDown = \"leftEyeDown\",\n  leftEyeLeft = \"leftEyeLeft\",\n  leftEyeRight = \"leftEyeRight\",\n  leftEyeUp = \"leftEyeUp\",\n  leftPupil = \"leftPupil\",\n  midJawlineLeft = \"midJawlineLeft\",\n  midJawlineRight = \"midJawlineRight\",\n  mouthDown = \"mouthDown\",\n  mouthLeft = \"mouthLeft\",\n  mouthRight = \"mouthRight\",\n  mouthUp = \"mouthUp\",\n  nose = \"nose\",\n  noseLeft = \"noseLeft\",\n  noseRight = \"noseRight\",\n  rightEyeBrowLeft = \"rightEyeBrowLeft\",\n  rightEyeBrowRight = \"rightEyeBrowRight\",\n  rightEyeBrowUp = \"rightEyeBrowUp\",\n  rightEyeDown = \"rightEyeDown\",\n  rightEyeLeft = \"rightEyeLeft\",\n  rightEyeRight = \"rightEyeRight\",\n  rightEyeUp = \"rightEyeUp\",\n  rightPupil = \"rightPupil\",\n  upperJawlineLeft = \"upperJawlineLeft\",\n  upperJawlineRight = \"upperJawlineRight\",\n}\n\n/**\n * <p>Indicates the location of the landmark on the face.</p>\n */\nexport interface Landmark {\n  /**\n   * <p>Type of landmark.</p>\n   */\n  Type?: LandmarkType | string;\n\n  /**\n   * <p>The x-coordinate of the landmark expressed as a ratio of the width of the image.\n   *       The x-coordinate is measured from the left-side of the image.\n   *       For example, if the image is 700 pixels wide and the x-coordinate of the landmark is at 350 pixels, this value is 0.5. </p>\n   */\n  X?: number;\n\n  /**\n   * <p>The y-coordinate of the landmark expressed as a ratio of the height of the image.\n   *       The y-coordinate is measured from the top of the image.\n   *       For example, if the image height is 200 pixels and the y-coordinate of the landmark is at 50 pixels, this value is 0.25.</p>\n   */\n  Y?: number;\n}\n\nexport namespace Landmark {\n  export const filterSensitiveLog = (obj: Landmark): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Indicates the pose of the face as determined by its pitch, roll, and yaw.</p>\n */\nexport interface Pose {\n  /**\n   * <p>Value representing the face rotation on the roll axis.</p>\n   */\n  Roll?: number;\n\n  /**\n   * <p>Value representing the face rotation on the yaw axis.</p>\n   */\n  Yaw?: number;\n\n  /**\n   * <p>Value representing the face rotation on the pitch axis.</p>\n   */\n  Pitch?: number;\n}\n\nexport namespace Pose {\n  export const filterSensitiveLog = (obj: Pose): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Identifies face image brightness and sharpness. </p>\n */\nexport interface ImageQuality {\n  /**\n   * <p>Value representing brightness of the face. The service returns a value between 0 and\n   *       100 (inclusive). A higher value indicates a brighter face image.</p>\n   */\n  Brightness?: number;\n\n  /**\n   * <p>Value representing sharpness of the face. The service returns a value between 0 and 100\n   *       (inclusive). A higher value indicates a sharper face image.</p>\n   */\n  Sharpness?: number;\n}\n\nexport namespace ImageQuality {\n  export const filterSensitiveLog = (obj: ImageQuality): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Provides face metadata for target image faces that are analyzed by\n *         <code>CompareFaces</code> and <code>RecognizeCelebrities</code>.</p>\n */\nexport interface ComparedFace {\n  /**\n   * <p>Bounding box of the face.</p>\n   */\n  BoundingBox?: BoundingBox;\n\n  /**\n   * <p>Level of confidence that what the bounding box contains is a face.</p>\n   */\n  Confidence?: number;\n\n  /**\n   * <p>An array of facial landmarks.</p>\n   */\n  Landmarks?: Landmark[];\n\n  /**\n   * <p>Indicates the pose of the face as determined by its pitch, roll, and yaw.</p>\n   */\n  Pose?: Pose;\n\n  /**\n   * <p>Identifies face image brightness and sharpness. </p>\n   */\n  Quality?: ImageQuality;\n}\n\nexport namespace ComparedFace {\n  export const filterSensitiveLog = (obj: ComparedFace): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Provides information about a celebrity recognized by the <a>RecognizeCelebrities</a> operation.</p>\n */\nexport interface Celebrity {\n  /**\n   * <p>An array of URLs pointing to additional information about the celebrity. If there is no\n   *       additional information about the celebrity, this list is empty.</p>\n   */\n  Urls?: string[];\n\n  /**\n   * <p>The name of the celebrity.</p>\n   */\n  Name?: string;\n\n  /**\n   * <p>A unique identifier for the celebrity. </p>\n   */\n  Id?: string;\n\n  /**\n   * <p>Provides information about the celebrity's face, such as its location on the\n   *       image.</p>\n   */\n  Face?: ComparedFace;\n\n  /**\n   * <p>The confidence, in percentage, that Amazon Rekognition has that the recognized face is the\n   *       celebrity.</p>\n   */\n  MatchConfidence?: number;\n}\n\nexport namespace Celebrity {\n  export const filterSensitiveLog = (obj: Celebrity): any => ({\n    ...obj,\n  });\n}\n\nexport type EmotionName =\n  | \"ANGRY\"\n  | \"CALM\"\n  | \"CONFUSED\"\n  | \"DISGUSTED\"\n  | \"FEAR\"\n  | \"HAPPY\"\n  | \"SAD\"\n  | \"SURPRISED\"\n  | \"UNKNOWN\";\n\n/**\n * <p>The emotions that appear to be expressed on the face, and the confidence level in the determination.\n *       The API is only making a determination of the physical appearance of a person's face. It is not a determination\n *       of the person’s internal emotional state and should not be used in such a way. For example, a person pretending to have\n *       a sad face might not be sad emotionally.</p>\n */\nexport interface Emotion {\n  /**\n   * <p>Type of emotion detected.</p>\n   */\n  Type?: EmotionName | string;\n\n  /**\n   * <p>Level of confidence in the determination.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace Emotion {\n  export const filterSensitiveLog = (obj: Emotion): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Indicates whether or not the face is wearing eye glasses, and the confidence level in\n *       the determination.</p>\n */\nexport interface Eyeglasses {\n  /**\n   * <p>Boolean value that indicates whether the face is wearing eye glasses or not.</p>\n   */\n  Value?: boolean;\n\n  /**\n   * <p>Level of confidence in the determination.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace Eyeglasses {\n  export const filterSensitiveLog = (obj: Eyeglasses): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Indicates whether or not the eyes on the face are open, and the confidence level in the\n *       determination.</p>\n */\nexport interface EyeOpen {\n  /**\n   * <p>Boolean value that indicates whether the eyes on the face are open.</p>\n   */\n  Value?: boolean;\n\n  /**\n   * <p>Level of confidence in the determination.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace EyeOpen {\n  export const filterSensitiveLog = (obj: EyeOpen): any => ({\n    ...obj,\n  });\n}\n\nexport enum GenderType {\n  Female = \"Female\",\n  Male = \"Male\",\n}\n\n/**\n * <p>The predicted gender of a detected face.\n *           </p>\n *\n *\n *          <p>Amazon Rekognition makes gender binary (male/female) predictions based on the physical appearance\n *       of a face in a particular image. This kind of prediction is not designed to categorize a person’s gender\n *       identity, and you shouldn't use Amazon Rekognition to make such a determination. For example, a male actor\n *       wearing a long-haired wig and earrings for a role might be predicted as female.</p>\n *\n *          <p>Using Amazon Rekognition to make gender binary predictions is best suited for use cases where aggregate gender distribution statistics need to be\n *       analyzed without identifying specific users. For example, the percentage of female users compared to male users on a social media platform. </p>\n *\n *          <p>We don't recommend using gender binary predictions to make decisions that impact  an individual's rights, privacy, or access to services.</p>\n */\nexport interface Gender {\n  /**\n   * <p>The predicted gender of the face.</p>\n   */\n  Value?: GenderType | string;\n\n  /**\n   * <p>Level of confidence in the prediction.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace Gender {\n  export const filterSensitiveLog = (obj: Gender): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Indicates whether or not the mouth on the face is open, and the confidence level in the\n *       determination.</p>\n */\nexport interface MouthOpen {\n  /**\n   * <p>Boolean value that indicates whether the mouth on the face is open or not.</p>\n   */\n  Value?: boolean;\n\n  /**\n   * <p>Level of confidence in the determination.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace MouthOpen {\n  export const filterSensitiveLog = (obj: MouthOpen): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Indicates whether or not the face has a mustache, and the confidence level in the\n *       determination.</p>\n */\nexport interface Mustache {\n  /**\n   * <p>Boolean value that indicates whether the face has mustache or not.</p>\n   */\n  Value?: boolean;\n\n  /**\n   * <p>Level of confidence in the determination.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace Mustache {\n  export const filterSensitiveLog = (obj: Mustache): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Indicates whether or not the face is smiling, and the confidence level in the\n *       determination.</p>\n */\nexport interface Smile {\n  /**\n   * <p>Boolean value that indicates whether the face is smiling or not.</p>\n   */\n  Value?: boolean;\n\n  /**\n   * <p>Level of confidence in the determination.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace Smile {\n  export const filterSensitiveLog = (obj: Smile): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Indicates whether or not the face is wearing sunglasses, and the confidence level in\n *       the determination.</p>\n */\nexport interface Sunglasses {\n  /**\n   * <p>Boolean value that indicates whether the face is wearing sunglasses or not.</p>\n   */\n  Value?: boolean;\n\n  /**\n   * <p>Level of confidence in the determination.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace Sunglasses {\n  export const filterSensitiveLog = (obj: Sunglasses): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Structure containing attributes of the face that the algorithm detected.</p>\n *          <p>A <code>FaceDetail</code> object contains either the default facial attributes or all facial attributes.\n *       The default attributes are <code>BoundingBox</code>, <code>Confidence</code>, <code>Landmarks</code>, <code>Pose</code>, and <code>Quality</code>.</p>\n *          <p>\n *             <a>GetFaceDetection</a> is the only Amazon Rekognition Video stored video operation that can return a <code>FaceDetail</code> object with all attributes.\n *       To specify which attributes to return, use the <code>FaceAttributes</code> input parameter for <a>StartFaceDetection</a>.\n *       The following Amazon Rekognition Video operations return only the default attributes. The corresponding Start operations\n *         don't have a <code>FaceAttributes</code> input parameter.</p>\n *          <ul>\n *             <li>\n *                <p>GetCelebrityRecognition</p>\n *             </li>\n *             <li>\n *                <p>GetPersonTracking</p>\n *             </li>\n *             <li>\n *                <p>GetFaceSearch</p>\n *             </li>\n *          </ul>\n *          <p>The Amazon Rekognition Image <a>DetectFaces</a> and <a>IndexFaces</a> operations\n *       can return all facial attributes. To specify which attributes to return, use the\n *       <code>Attributes</code> input parameter for <code>DetectFaces</code>. For <code>IndexFaces</code>, use the\n *       <code>DetectAttributes</code> input parameter.</p>\n */\nexport interface FaceDetail {\n  /**\n   * <p>Bounding box of the face. Default attribute.</p>\n   */\n  BoundingBox?: BoundingBox;\n\n  /**\n   * <p>The estimated age range, in years, for the face. Low represents the lowest estimated\n   *       age and High represents the highest estimated age.</p>\n   */\n  AgeRange?: AgeRange;\n\n  /**\n   * <p>Indicates whether or not the face is smiling, and the confidence level in the\n   *       determination.</p>\n   */\n  Smile?: Smile;\n\n  /**\n   * <p>Indicates whether or not the face is wearing eye glasses, and the confidence level in\n   *       the determination.</p>\n   */\n  Eyeglasses?: Eyeglasses;\n\n  /**\n   * <p>Indicates whether or not the face is wearing sunglasses, and the confidence level in\n   *       the determination.</p>\n   */\n  Sunglasses?: Sunglasses;\n\n  /**\n   * <p>The predicted gender of a detected face.\n   *     </p>\n   */\n  Gender?: Gender;\n\n  /**\n   * <p>Indicates whether or not the face has a beard, and the confidence level in the\n   *       determination.</p>\n   */\n  Beard?: Beard;\n\n  /**\n   * <p>Indicates whether or not the face has a mustache, and the confidence level in the\n   *       determination.</p>\n   */\n  Mustache?: Mustache;\n\n  /**\n   * <p>Indicates whether or not the eyes on the face are open, and the confidence level in the\n   *       determination.</p>\n   */\n  EyesOpen?: EyeOpen;\n\n  /**\n   * <p>Indicates whether or not the mouth on the face is open, and the confidence level in the\n   *       determination.</p>\n   */\n  MouthOpen?: MouthOpen;\n\n  /**\n   * <p>The emotions that appear to be expressed on the face, and the confidence level in the determination.\n   *       The API is only making a determination of the physical appearance of a person's face. It is not a determination\n   *       of the person’s internal emotional state and should not be used in such a way. For example, a person pretending to have\n   *       a sad face might not be sad emotionally.</p>\n   */\n  Emotions?: Emotion[];\n\n  /**\n   * <p>Indicates the location of landmarks on the face. Default attribute.</p>\n   */\n  Landmarks?: Landmark[];\n\n  /**\n   * <p>Indicates the pose of the face as determined by its pitch, roll, and yaw. Default attribute.</p>\n   */\n  Pose?: Pose;\n\n  /**\n   * <p>Identifies image brightness and sharpness. Default attribute.</p>\n   */\n  Quality?: ImageQuality;\n\n  /**\n   * <p>Confidence level that the bounding box contains a face (and not a different object such\n   *       as a tree). Default attribute.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace FaceDetail {\n  export const filterSensitiveLog = (obj: FaceDetail): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Information about a recognized celebrity.</p>\n */\nexport interface CelebrityDetail {\n  /**\n   * <p>An array of URLs pointing to additional celebrity information. </p>\n   */\n  Urls?: string[];\n\n  /**\n   * <p>The name of the celebrity.</p>\n   */\n  Name?: string;\n\n  /**\n   * <p>The unique identifier for the celebrity. </p>\n   */\n  Id?: string;\n\n  /**\n   * <p>The confidence, in percentage, that Amazon Rekognition has that the recognized face is the celebrity. </p>\n   */\n  Confidence?: number;\n\n  /**\n   * <p>Bounding box around the body of a celebrity.</p>\n   */\n  BoundingBox?: BoundingBox;\n\n  /**\n   * <p>Face details for the recognized celebrity.</p>\n   */\n  Face?: FaceDetail;\n}\n\nexport namespace CelebrityDetail {\n  export const filterSensitiveLog = (obj: CelebrityDetail): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Information about a detected celebrity and the time the celebrity was detected in a stored video.\n *         For more information, see GetCelebrityRecognition in the Amazon Rekognition Developer Guide.</p>\n */\nexport interface CelebrityRecognition {\n  /**\n   * <p>The time, in milliseconds from the start of the video, that the celebrity was recognized.</p>\n   */\n  Timestamp?: number;\n\n  /**\n   * <p>Information about a recognized celebrity.</p>\n   */\n  Celebrity?: CelebrityDetail;\n}\n\nexport namespace CelebrityRecognition {\n  export const filterSensitiveLog = (obj: CelebrityRecognition): any => ({\n    ...obj,\n  });\n}\n\nexport enum CelebrityRecognitionSortBy {\n  ID = \"ID\",\n  TIMESTAMP = \"TIMESTAMP\",\n}\n\n/**\n * <p>Type that describes the face Amazon Rekognition chose to compare with the faces in the target.\n *       This contains a bounding box for the selected face and confidence level that the bounding box\n *       contains a face. Note that Amazon Rekognition selects the largest face in the source image for this\n *       comparison. </p>\n */\nexport interface ComparedSourceImageFace {\n  /**\n   * <p>Bounding box of the face.</p>\n   */\n  BoundingBox?: BoundingBox;\n\n  /**\n   * <p>Confidence level that the selected bounding box contains a face.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace ComparedSourceImageFace {\n  export const filterSensitiveLog = (obj: ComparedSourceImageFace): any => ({\n    ...obj,\n  });\n}\n\nexport enum QualityFilter {\n  AUTO = \"AUTO\",\n  HIGH = \"HIGH\",\n  LOW = \"LOW\",\n  MEDIUM = \"MEDIUM\",\n  NONE = \"NONE\",\n}\n\n/**\n * <p>Provides the input image either as bytes or an S3 object.</p>\n *          <p>You pass image bytes to an Amazon Rekognition API operation by using the <code>Bytes</code>\n *       property. For example, you would use the <code>Bytes</code> property to pass an image loaded\n *       from a local file system. Image bytes passed by using the <code>Bytes</code> property must be\n *       base64-encoded. Your code may not need to encode image bytes if you are using an AWS SDK to\n *       call Amazon Rekognition API operations. </p>\n *\n *          <p>For more information, see Analyzing an Image Loaded from a Local File System\n *       in the Amazon Rekognition Developer Guide.</p>\n *          <p> You pass images stored in an S3 bucket to an Amazon Rekognition API operation by using the\n *         <code>S3Object</code> property. Images stored in an S3 bucket do not need to be\n *       base64-encoded.</p>\n *          <p>The region for the S3 bucket containing the S3 object must match the region you use for\n *       Amazon Rekognition operations.</p>\n *          <p>If you use the\n *       AWS\n *       CLI to call Amazon Rekognition operations, passing image bytes using the Bytes\n *       property is not supported. You must first upload the image to an Amazon S3 bucket and then\n *       call the operation using the S3Object property.</p>\n *\n *          <p>For Amazon Rekognition to process an S3 object, the user must have permission to access the S3\n *       object. For more information, see Resource Based Policies in the Amazon Rekognition Developer Guide.\n *     </p>\n */\nexport interface Image {\n  /**\n   * <p>Blob of image bytes up to 5 MBs.</p>\n   */\n  Bytes?: Uint8Array;\n\n  /**\n   * <p>Identifies an S3 object as the image source.</p>\n   */\n  S3Object?: S3Object;\n}\n\nexport namespace Image {\n  export const filterSensitiveLog = (obj: Image): any => ({\n    ...obj,\n  });\n}\n\nexport interface CompareFacesRequest {\n  /**\n   * <p>The input image as base64-encoded bytes or an S3 object.\n   *       If you use the AWS CLI to call Amazon Rekognition operations,\n   *       passing base64-encoded image bytes is not supported. </p>\n   *          <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes\n   *       passed using the <code>Bytes</code> field.\n   *       For more information, see Images in the Amazon Rekognition developer guide.</p>\n   */\n  SourceImage: Image | undefined;\n\n  /**\n   * <p>The target image as base64-encoded bytes or an S3 object. If you use the AWS CLI to\n   *       call Amazon Rekognition operations, passing base64-encoded image bytes is not supported.\n   *     </p>\n   *          <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes\n   *       passed using the <code>Bytes</code> field.\n   *       For more information, see Images in the Amazon Rekognition developer guide.</p>\n   */\n  TargetImage: Image | undefined;\n\n  /**\n   * <p>The minimum level of confidence in the face matches that a match must meet to be\n   *       included in the <code>FaceMatches</code> array.</p>\n   */\n  SimilarityThreshold?: number;\n\n  /**\n   * <p>A filter that specifies a quality bar for how much filtering is done to identify faces.\n   *       Filtered faces aren't compared. If you specify <code>AUTO</code>, Amazon Rekognition chooses the quality bar.\n   *       If you specify <code>LOW</code>,\n   *       <code>MEDIUM</code>, or <code>HIGH</code>, filtering removes all faces that\n   *       don’t meet the chosen quality bar.\n   *\n   *       The quality bar is based on a variety of common use cases. Low-quality\n   *       detections can occur for a number of reasons. Some examples are an object that's misidentified\n   *       as a face, a face that's too blurry, or a face with a\n   *       pose that's too extreme to use. If you specify <code>NONE</code>, no\n   *       filtering is performed. The default value is <code>NONE</code>.\n   *     </p>\n   *          <p>To use quality filtering, the collection you are using must be associated with version 3 of the face model or higher.</p>\n   */\n  QualityFilter?: QualityFilter | string;\n}\n\nexport namespace CompareFacesRequest {\n  export const filterSensitiveLog = (obj: CompareFacesRequest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Provides information about a face in a target image that matches the source image face\n *       analyzed by <code>CompareFaces</code>. The <code>Face</code> property contains the bounding\n *       box of the face in the target image. The <code>Similarity</code> property is the confidence\n *       that the source image face matches the face in the bounding box.</p>\n */\nexport interface CompareFacesMatch {\n  /**\n   * <p>Level of confidence that the faces match.</p>\n   */\n  Similarity?: number;\n\n  /**\n   * <p>Provides face metadata (bounding box and confidence that the bounding box actually\n   *       contains a face).</p>\n   */\n  Face?: ComparedFace;\n}\n\nexport namespace CompareFacesMatch {\n  export const filterSensitiveLog = (obj: CompareFacesMatch): any => ({\n    ...obj,\n  });\n}\n\nexport enum OrientationCorrection {\n  ROTATE_0 = \"ROTATE_0\",\n  ROTATE_180 = \"ROTATE_180\",\n  ROTATE_270 = \"ROTATE_270\",\n  ROTATE_90 = \"ROTATE_90\",\n}\n\nexport interface CompareFacesResponse {\n  /**\n   * <p>The face in the source image that was used for comparison.</p>\n   */\n  SourceImageFace?: ComparedSourceImageFace;\n\n  /**\n   * <p>An array of faces in the target image that match the source image face. Each\n   *         <code>CompareFacesMatch</code> object provides the bounding box, the confidence level that\n   *       the bounding box contains a face, and the similarity score for the face in the bounding box\n   *       and the face in the source image.</p>\n   */\n  FaceMatches?: CompareFacesMatch[];\n\n  /**\n   * <p>An array of faces in the target image that did not match the source image\n   *       face.</p>\n   */\n  UnmatchedFaces?: ComparedFace[];\n\n  /**\n   * <p>The value of <code>SourceImageOrientationCorrection</code> is always null.</p>\n   *          <p>If the input image is in .jpeg format, it might contain exchangeable image file format (Exif) metadata\n   *       that includes the image's orientation. Amazon Rekognition uses this orientation information to perform\n   *       image correction. The bounding box coordinates are translated to represent object locations\n   *       after the orientation information in the Exif metadata is used to correct the image orientation.\n   *       Images in .png format don't contain Exif metadata.</p>\n   *          <p>Amazon Rekognition doesn’t perform image correction for images in .png format and\n   *       .jpeg images without orientation information in the image Exif metadata. The bounding box\n   *       coordinates aren't translated and represent the object locations before the image is rotated.\n   *     </p>\n   */\n  SourceImageOrientationCorrection?: OrientationCorrection | string;\n\n  /**\n   * <p>The value of <code>TargetImageOrientationCorrection</code> is always null.</p>\n   *          <p>If the input image is in .jpeg format, it might contain exchangeable image file format (Exif) metadata\n   *       that includes the image's orientation. Amazon Rekognition uses this orientation information to perform\n   *       image correction. The bounding box coordinates are translated to represent object locations\n   *       after the orientation information in the Exif metadata is used to correct the image orientation.\n   *       Images in .png format don't contain Exif metadata.</p>\n   *          <p>Amazon Rekognition doesn’t perform image correction for images in .png format and\n   *       .jpeg images without orientation information in the image Exif metadata. The bounding box\n   *       coordinates aren't translated and represent the object locations before the image is rotated.\n   *     </p>\n   */\n  TargetImageOrientationCorrection?: OrientationCorrection | string;\n}\n\nexport namespace CompareFacesResponse {\n  export const filterSensitiveLog = (obj: CompareFacesResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The input image size exceeds the allowed limit. For more information, see\n *       Limits in Amazon Rekognition in the Amazon Rekognition Developer Guide. </p>\n */\nexport interface ImageTooLargeException extends __SmithyException, $MetadataBearer {\n  name: \"ImageTooLargeException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace ImageTooLargeException {\n  export const filterSensitiveLog = (obj: ImageTooLargeException): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Amazon Rekognition experienced a service issue. Try your call again.</p>\n */\nexport interface InternalServerError extends __SmithyException, $MetadataBearer {\n  name: \"InternalServerError\";\n  $fault: \"server\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace InternalServerError {\n  export const filterSensitiveLog = (obj: InternalServerError): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The provided image format is not supported. </p>\n */\nexport interface InvalidImageFormatException extends __SmithyException, $MetadataBearer {\n  name: \"InvalidImageFormatException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace InvalidImageFormatException {\n  export const filterSensitiveLog = (obj: InvalidImageFormatException): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Input parameter violated a constraint. Validate your parameter before calling the API\n *       operation again.</p>\n */\nexport interface InvalidParameterException extends __SmithyException, $MetadataBearer {\n  name: \"InvalidParameterException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace InvalidParameterException {\n  export const filterSensitiveLog = (obj: InvalidParameterException): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Amazon Rekognition is unable to access the S3 object specified in the request.</p>\n */\nexport interface InvalidS3ObjectException extends __SmithyException, $MetadataBearer {\n  name: \"InvalidS3ObjectException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace InvalidS3ObjectException {\n  export const filterSensitiveLog = (obj: InvalidS3ObjectException): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The number of requests exceeded your throughput limit. If you want to increase this\n *       limit, contact Amazon Rekognition.</p>\n */\nexport interface ProvisionedThroughputExceededException extends __SmithyException, $MetadataBearer {\n  name: \"ProvisionedThroughputExceededException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace ProvisionedThroughputExceededException {\n  export const filterSensitiveLog = (obj: ProvisionedThroughputExceededException): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Amazon Rekognition is temporarily unable to process the request. Try your call again.</p>\n */\nexport interface ThrottlingException extends __SmithyException, $MetadataBearer {\n  name: \"ThrottlingException\";\n  $fault: \"server\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace ThrottlingException {\n  export const filterSensitiveLog = (obj: ThrottlingException): any => ({\n    ...obj,\n  });\n}\n\nexport enum ContentClassifier {\n  FREE_OF_ADULT_CONTENT = \"FreeOfAdultContent\",\n  FREE_OF_PERSONALLY_IDENTIFIABLE_INFORMATION = \"FreeOfPersonallyIdentifiableInformation\",\n}\n\n/**\n * <p>Provides information about a single type of unsafe content found in an image or video. Each type of\n *       moderated content has a label within a hierarchical taxonomy. For more information, see\n *       Detecting Unsafe Content in the Amazon Rekognition Developer Guide.</p>\n */\nexport interface ModerationLabel {\n  /**\n   * <p>Specifies the confidence that Amazon Rekognition has that the label has been correctly\n   *       identified.</p>\n   *          <p>If you don't specify the <code>MinConfidence</code> parameter in the call to\n   *         <code>DetectModerationLabels</code>, the operation returns labels with a confidence value\n   *       greater than or equal to 50 percent.</p>\n   */\n  Confidence?: number;\n\n  /**\n   * <p>The label name for the type of unsafe content detected in the image.</p>\n   */\n  Name?: string;\n\n  /**\n   * <p>The name for the parent label. Labels at the top level of the hierarchy have the parent\n   *       label <code>\"\"</code>.</p>\n   */\n  ParentName?: string;\n}\n\nexport namespace ModerationLabel {\n  export const filterSensitiveLog = (obj: ModerationLabel): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Information about an unsafe content label detection in a stored video.</p>\n */\nexport interface ContentModerationDetection {\n  /**\n   * <p>Time, in milliseconds from the beginning of the video, that the unsafe content label was detected.</p>\n   */\n  Timestamp?: number;\n\n  /**\n   * <p>The unsafe content label detected by in the stored video.</p>\n   */\n  ModerationLabel?: ModerationLabel;\n}\n\nexport namespace ContentModerationDetection {\n  export const filterSensitiveLog = (obj: ContentModerationDetection): any => ({\n    ...obj,\n  });\n}\n\nexport enum ContentModerationSortBy {\n  NAME = \"NAME\",\n  TIMESTAMP = \"TIMESTAMP\",\n}\n\nexport interface CreateCollectionRequest {\n  /**\n   * <p>ID for the collection that you are creating.</p>\n   */\n  CollectionId: string | undefined;\n}\n\nexport namespace CreateCollectionRequest {\n  export const filterSensitiveLog = (obj: CreateCollectionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface CreateCollectionResponse {\n  /**\n   * <p>HTTP status code indicating the result of the operation.</p>\n   */\n  StatusCode?: number;\n\n  /**\n   * <p>Amazon Resource Name (ARN) of the collection. You can use this to manage permissions on\n   *       your resources. </p>\n   */\n  CollectionArn?: string;\n\n  /**\n   * <p>Version number of the face detection model associated with the collection you are creating.</p>\n   */\n  FaceModelVersion?: string;\n}\n\nexport namespace CreateCollectionResponse {\n  export const filterSensitiveLog = (obj: CreateCollectionResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>A collection with the specified ID already exists.</p>\n */\nexport interface ResourceAlreadyExistsException extends __SmithyException, $MetadataBearer {\n  name: \"ResourceAlreadyExistsException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace ResourceAlreadyExistsException {\n  export const filterSensitiveLog = (obj: ResourceAlreadyExistsException): any => ({\n    ...obj,\n  });\n}\n\nexport interface CreateProjectRequest {\n  /**\n   * <p>The name of the project to create.</p>\n   */\n  ProjectName: string | undefined;\n}\n\nexport namespace CreateProjectRequest {\n  export const filterSensitiveLog = (obj: CreateProjectRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface CreateProjectResponse {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the new project. You can use the ARN to\n   *       configure IAM access to the project. </p>\n   */\n  ProjectArn?: string;\n}\n\nexport namespace CreateProjectResponse {\n  export const filterSensitiveLog = (obj: CreateProjectResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>An Amazon Rekognition service limit was exceeded. For example, if you start too many Amazon Rekognition Video jobs concurrently, calls to start operations\n *             (<code>StartLabelDetection</code>, for example) will raise a <code>LimitExceededException</code> exception (HTTP status code: 400) until\n *             the number of concurrently running jobs is below the Amazon Rekognition service limit.  </p>\n */\nexport interface LimitExceededException extends __SmithyException, $MetadataBearer {\n  name: \"LimitExceededException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace LimitExceededException {\n  export const filterSensitiveLog = (obj: LimitExceededException): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The specified resource is already being used.</p>\n */\nexport interface ResourceInUseException extends __SmithyException, $MetadataBearer {\n  name: \"ResourceInUseException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace ResourceInUseException {\n  export const filterSensitiveLog = (obj: ResourceInUseException): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The S3 bucket and folder location where training output is placed.</p>\n */\nexport interface OutputConfig {\n  /**\n   * <p>The S3 bucket where training output is placed.</p>\n   */\n  S3Bucket?: string;\n\n  /**\n   * <p>The prefix applied to the training output files. </p>\n   */\n  S3KeyPrefix?: string;\n}\n\nexport namespace OutputConfig {\n  export const filterSensitiveLog = (obj: OutputConfig): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The dataset used for testing. Optionally, if <code>AutoCreate</code> is set,  Amazon Rekognition Custom Labels creates a\n *          testing dataset using an 80/20 split of the training dataset.</p>\n */\nexport interface TestingData {\n  /**\n   * <p>The assets used for testing.</p>\n   */\n  Assets?: Asset[];\n\n  /**\n   * <p>If specified, Amazon Rekognition Custom Labels creates a testing dataset with an 80/20 split of the training dataset.</p>\n   */\n  AutoCreate?: boolean;\n}\n\nexport namespace TestingData {\n  export const filterSensitiveLog = (obj: TestingData): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The dataset used for training.</p>\n */\nexport interface TrainingData {\n  /**\n   * <p>A Sagemaker GroundTruth manifest file that contains the training images (assets).</p>\n   */\n  Assets?: Asset[];\n}\n\nexport namespace TrainingData {\n  export const filterSensitiveLog = (obj: TrainingData): any => ({\n    ...obj,\n  });\n}\n\nexport interface CreateProjectVersionRequest {\n  /**\n   * <p>The ARN of the Amazon Rekognition Custom Labels project that\n   *          manages the model that you want to train.</p>\n   */\n  ProjectArn: string | undefined;\n\n  /**\n   * <p>A name for the version of the model. This value must be unique.</p>\n   */\n  VersionName: string | undefined;\n\n  /**\n   * <p>The Amazon S3 location to store the results of training.</p>\n   */\n  OutputConfig: OutputConfig | undefined;\n\n  /**\n   * <p>The dataset to use for training. </p>\n   */\n  TrainingData: TrainingData | undefined;\n\n  /**\n   * <p>The dataset to use for testing.</p>\n   */\n  TestingData: TestingData | undefined;\n}\n\nexport namespace CreateProjectVersionRequest {\n  export const filterSensitiveLog = (obj: CreateProjectVersionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface CreateProjectVersionResponse {\n  /**\n   * <p>The ARN of the model version that was created. Use <code>DescribeProjectVersion</code>\n   *          to get the current status of the training operation.</p>\n   */\n  ProjectVersionArn?: string;\n}\n\nexport namespace CreateProjectVersionResponse {\n  export const filterSensitiveLog = (obj: CreateProjectVersionResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The collection specified in the request cannot be found.</p>\n */\nexport interface ResourceNotFoundException extends __SmithyException, $MetadataBearer {\n  name: \"ResourceNotFoundException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace ResourceNotFoundException {\n  export const filterSensitiveLog = (obj: ResourceNotFoundException): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Kinesis video stream stream that provides the source streaming video for a Amazon Rekognition Video stream processor. For more information, see\n *             CreateStreamProcessor in the Amazon Rekognition Developer Guide.</p>\n */\nexport interface KinesisVideoStream {\n  /**\n   * <p>ARN of the Kinesis video stream stream that streams the source video.</p>\n   */\n  Arn?: string;\n}\n\nexport namespace KinesisVideoStream {\n  export const filterSensitiveLog = (obj: KinesisVideoStream): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Information about the source streaming video. </p>\n */\nexport interface StreamProcessorInput {\n  /**\n   * <p>The Kinesis video stream input stream for the source streaming video.</p>\n   */\n  KinesisVideoStream?: KinesisVideoStream;\n}\n\nexport namespace StreamProcessorInput {\n  export const filterSensitiveLog = (obj: StreamProcessorInput): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The Kinesis data stream Amazon Rekognition to which the analysis results of a Amazon Rekognition stream processor are streamed. For more information, see\n *             CreateStreamProcessor in the Amazon Rekognition Developer Guide.</p>\n */\nexport interface KinesisDataStream {\n  /**\n   * <p>ARN of the output Amazon Kinesis Data Streams stream.</p>\n   */\n  Arn?: string;\n}\n\nexport namespace KinesisDataStream {\n  export const filterSensitiveLog = (obj: KinesisDataStream): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Information about the Amazon Kinesis Data Streams stream to which a Amazon Rekognition Video stream processor streams the results of a video analysis. For more\n *            information, see CreateStreamProcessor in the Amazon Rekognition Developer Guide.</p>\n */\nexport interface StreamProcessorOutput {\n  /**\n   * <p>The Amazon Kinesis Data Streams stream to which the Amazon Rekognition stream processor streams the analysis results.</p>\n   */\n  KinesisDataStream?: KinesisDataStream;\n}\n\nexport namespace StreamProcessorOutput {\n  export const filterSensitiveLog = (obj: StreamProcessorOutput): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Input face recognition parameters for an Amazon Rekognition stream processor. <code>FaceRecognitionSettings</code> is a request\n *         parameter for <a>CreateStreamProcessor</a>.</p>\n */\nexport interface FaceSearchSettings {\n  /**\n   * <p>The ID of a collection that contains faces that you want to search for.</p>\n   */\n  CollectionId?: string;\n\n  /**\n   * <p>Minimum face match confidence score that must be met to return a result for a recognized face. Default is 80.\n   *         0 is the lowest confidence. 100 is the highest confidence.</p>\n   */\n  FaceMatchThreshold?: number;\n}\n\nexport namespace FaceSearchSettings {\n  export const filterSensitiveLog = (obj: FaceSearchSettings): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Input parameters used to recognize faces in a streaming video analyzed by a Amazon Rekognition stream processor.</p>\n */\nexport interface StreamProcessorSettings {\n  /**\n   * <p>Face search settings to use on a streaming video. </p>\n   */\n  FaceSearch?: FaceSearchSettings;\n}\n\nexport namespace StreamProcessorSettings {\n  export const filterSensitiveLog = (obj: StreamProcessorSettings): any => ({\n    ...obj,\n  });\n}\n\nexport interface CreateStreamProcessorRequest {\n  /**\n   * <p>Kinesis video stream stream that provides the source streaming video. If you are using the AWS CLI, the parameter name is <code>StreamProcessorInput</code>.</p>\n   */\n  Input: StreamProcessorInput | undefined;\n\n  /**\n   * <p>Kinesis data stream stream to which Amazon Rekognition Video puts the analysis results. If you are using the AWS CLI, the parameter name is <code>StreamProcessorOutput</code>.</p>\n   */\n  Output: StreamProcessorOutput | undefined;\n\n  /**\n   * <p>An identifier you assign to the stream processor. You can use <code>Name</code> to\n   *             manage the stream processor. For example, you can get the current status of the stream processor by calling <a>DescribeStreamProcessor</a>.\n   *              <code>Name</code> is idempotent.\n   *        </p>\n   */\n  Name: string | undefined;\n\n  /**\n   * <p>Face recognition input parameters to be used by the stream processor. Includes the collection to use for face recognition and the face\n   *         attributes to detect.</p>\n   */\n  Settings: StreamProcessorSettings | undefined;\n\n  /**\n   * <p>ARN of the IAM role that allows access to the stream processor.</p>\n   */\n  RoleArn: string | undefined;\n}\n\nexport namespace CreateStreamProcessorRequest {\n  export const filterSensitiveLog = (obj: CreateStreamProcessorRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface CreateStreamProcessorResponse {\n  /**\n   * <p>ARN for the newly create stream processor.</p>\n   */\n  StreamProcessorArn?: string;\n}\n\nexport namespace CreateStreamProcessorResponse {\n  export const filterSensitiveLog = (obj: CreateStreamProcessorResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The X and Y coordinates of a point on an image. The X and Y values returned are ratios\n *       of the overall image size. For example, if the input image is 700x200 and the\n *       operation returns X=0.5 and Y=0.25, then the point is at the (350,50) pixel coordinate on the image.</p>\n *\n *          <p>An array of <code>Point</code> objects,\n *       <code>Polygon</code>, is returned by <a>DetectText</a> and by <a>DetectCustomLabels</a>. <code>Polygon</code>\n *       represents a fine-grained polygon around a detected item. For more information, see Geometry in the\n *       Amazon Rekognition Developer Guide. </p>\n */\nexport interface Point {\n  /**\n   * <p>The value of the X coordinate for a point on a <code>Polygon</code>.</p>\n   */\n  X?: number;\n\n  /**\n   * <p>The value of the Y coordinate for a point on a <code>Polygon</code>.</p>\n   */\n  Y?: number;\n}\n\nexport namespace Point {\n  export const filterSensitiveLog = (obj: Point): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Information about where an object (<a>DetectCustomLabels</a>) or text (<a>DetectText</a>) is located on\n *       an image.</p>\n */\nexport interface Geometry {\n  /**\n   * <p>An axis-aligned coarse representation of the detected item's location on the\n   *       image.</p>\n   */\n  BoundingBox?: BoundingBox;\n\n  /**\n   * <p>Within the bounding box, a fine-grained polygon around the detected item.</p>\n   */\n  Polygon?: Point[];\n}\n\nexport namespace Geometry {\n  export const filterSensitiveLog = (obj: Geometry): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>A custom label detected in an image by a call to <a>DetectCustomLabels</a>.</p>\n */\nexport interface CustomLabel {\n  /**\n   * <p>The name of the custom label.</p>\n   */\n  Name?: string;\n\n  /**\n   * <p>The confidence that the model has in the detection of the custom label. The\n   *       range is 0-100. A higher value indicates a higher confidence.</p>\n   */\n  Confidence?: number;\n\n  /**\n   * <p>The location of the detected object on the image that corresponds to the custom label.\n   *          Includes an axis aligned coarse bounding box surrounding the object and a finer grain polygon\n   *          for more accurate spatial information.</p>\n   */\n  Geometry?: Geometry;\n}\n\nexport namespace CustomLabel {\n  export const filterSensitiveLog = (obj: CustomLabel): any => ({\n    ...obj,\n  });\n}\n\nexport interface DeleteCollectionRequest {\n  /**\n   * <p>ID of the collection to delete.</p>\n   */\n  CollectionId: string | undefined;\n}\n\nexport namespace DeleteCollectionRequest {\n  export const filterSensitiveLog = (obj: DeleteCollectionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface DeleteCollectionResponse {\n  /**\n   * <p>HTTP status code that indicates the result of the operation.</p>\n   */\n  StatusCode?: number;\n}\n\nexport namespace DeleteCollectionResponse {\n  export const filterSensitiveLog = (obj: DeleteCollectionResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface DeleteFacesRequest {\n  /**\n   * <p>Collection from which to remove the specific faces.</p>\n   */\n  CollectionId: string | undefined;\n\n  /**\n   * <p>An array of face IDs to delete.</p>\n   */\n  FaceIds: string[] | undefined;\n}\n\nexport namespace DeleteFacesRequest {\n  export const filterSensitiveLog = (obj: DeleteFacesRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface DeleteFacesResponse {\n  /**\n   * <p>An array of strings (face IDs) of the faces that were deleted.</p>\n   */\n  DeletedFaces?: string[];\n}\n\nexport namespace DeleteFacesResponse {\n  export const filterSensitiveLog = (obj: DeleteFacesResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface DeleteProjectRequest {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the project that you want to delete.</p>\n   */\n  ProjectArn: string | undefined;\n}\n\nexport namespace DeleteProjectRequest {\n  export const filterSensitiveLog = (obj: DeleteProjectRequest): any => ({\n    ...obj,\n  });\n}\n\nexport enum ProjectStatus {\n  CREATED = \"CREATED\",\n  CREATING = \"CREATING\",\n  DELETING = \"DELETING\",\n}\n\nexport interface DeleteProjectResponse {\n  /**\n   * <p>The current status of the delete project operation.</p>\n   */\n  Status?: ProjectStatus | string;\n}\n\nexport namespace DeleteProjectResponse {\n  export const filterSensitiveLog = (obj: DeleteProjectResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface DeleteProjectVersionRequest {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the model version that you want to delete.</p>\n   */\n  ProjectVersionArn: string | undefined;\n}\n\nexport namespace DeleteProjectVersionRequest {\n  export const filterSensitiveLog = (obj: DeleteProjectVersionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport enum ProjectVersionStatus {\n  DELETING = \"DELETING\",\n  FAILED = \"FAILED\",\n  RUNNING = \"RUNNING\",\n  STARTING = \"STARTING\",\n  STOPPED = \"STOPPED\",\n  STOPPING = \"STOPPING\",\n  TRAINING_COMPLETED = \"TRAINING_COMPLETED\",\n  TRAINING_FAILED = \"TRAINING_FAILED\",\n  TRAINING_IN_PROGRESS = \"TRAINING_IN_PROGRESS\",\n}\n\nexport interface DeleteProjectVersionResponse {\n  /**\n   * <p>The status of the deletion operation.</p>\n   */\n  Status?: ProjectVersionStatus | string;\n}\n\nexport namespace DeleteProjectVersionResponse {\n  export const filterSensitiveLog = (obj: DeleteProjectVersionResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface DeleteStreamProcessorRequest {\n  /**\n   * <p>The name of the stream processor you want to delete.</p>\n   */\n  Name: string | undefined;\n}\n\nexport namespace DeleteStreamProcessorRequest {\n  export const filterSensitiveLog = (obj: DeleteStreamProcessorRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface DeleteStreamProcessorResponse {}\n\nexport namespace DeleteStreamProcessorResponse {\n  export const filterSensitiveLog = (obj: DeleteStreamProcessorResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface DescribeCollectionRequest {\n  /**\n   * <p>The ID of the collection to describe.</p>\n   */\n  CollectionId: string | undefined;\n}\n\nexport namespace DescribeCollectionRequest {\n  export const filterSensitiveLog = (obj: DescribeCollectionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface DescribeCollectionResponse {\n  /**\n   * <p>The number of faces that are indexed into the collection. To index faces into a\n   *          collection, use <a>IndexFaces</a>.</p>\n   */\n  FaceCount?: number;\n\n  /**\n   * <p>The version of the face model that's used by the collection for face detection.</p>\n   *\n   *          <p>For more information, see Model Versioning in the\n   *      Amazon Rekognition Developer Guide.</p>\n   */\n  FaceModelVersion?: string;\n\n  /**\n   * <p>The Amazon Resource Name (ARN) of the collection.</p>\n   */\n  CollectionARN?: string;\n\n  /**\n   * <p>The number of milliseconds since the Unix epoch time until the creation of the collection.\n   *          The Unix epoch time is 00:00:00 Coordinated Universal Time (UTC), Thursday, 1 January 1970.</p>\n   */\n  CreationTimestamp?: Date;\n}\n\nexport namespace DescribeCollectionResponse {\n  export const filterSensitiveLog = (obj: DescribeCollectionResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface DescribeProjectsRequest {\n  /**\n   * <p>If the previous response was incomplete (because there is more\n   *          results to retrieve), Amazon Rekognition Custom Labels returns a pagination token in the response. You can use this pagination\n   *          token to retrieve the next set of results. </p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>The maximum number of results to return per paginated call. The largest value you can specify is 100.\n   *          If you specify a value greater than 100, a ValidationException\n   *          error occurs. The default value is 100. </p>\n   */\n  MaxResults?: number;\n}\n\nexport namespace DescribeProjectsRequest {\n  export const filterSensitiveLog = (obj: DescribeProjectsRequest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>A description of a Amazon Rekognition Custom Labels project.</p>\n */\nexport interface ProjectDescription {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the project.</p>\n   */\n  ProjectArn?: string;\n\n  /**\n   * <p>The Unix timestamp for the date and time that the project was created.</p>\n   */\n  CreationTimestamp?: Date;\n\n  /**\n   * <p>The current status of the project.</p>\n   */\n  Status?: ProjectStatus | string;\n}\n\nexport namespace ProjectDescription {\n  export const filterSensitiveLog = (obj: ProjectDescription): any => ({\n    ...obj,\n  });\n}\n\nexport interface DescribeProjectsResponse {\n  /**\n   * <p>A list of project descriptions. The list is sorted by the date and time the projects are created.</p>\n   */\n  ProjectDescriptions?: ProjectDescription[];\n\n  /**\n   * <p>If the previous response was incomplete (because there is more\n   *          results to retrieve), Amazon Rekognition Custom Labels returns a pagination token in the response.\n   *          You can use this pagination token to retrieve the next set of results. </p>\n   */\n  NextToken?: string;\n}\n\nexport namespace DescribeProjectsResponse {\n  export const filterSensitiveLog = (obj: DescribeProjectsResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Pagination token in the request is not valid.</p>\n */\nexport interface InvalidPaginationTokenException extends __SmithyException, $MetadataBearer {\n  name: \"InvalidPaginationTokenException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace InvalidPaginationTokenException {\n  export const filterSensitiveLog = (obj: InvalidPaginationTokenException): any => ({\n    ...obj,\n  });\n}\n\nexport interface DescribeProjectVersionsRequest {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the project that contains the models you want to describe.</p>\n   */\n  ProjectArn: string | undefined;\n\n  /**\n   * <p>A list of model version names that you want to describe. You can add up to 10 model version names\n   *          to the list. If you don't specify a value, all model descriptions are returned.  A version name is part of a\n   *          model (ProjectVersion) ARN. For example, <code>my-model.2020-01-21T09.10.15</code> is the version name in the following ARN.\n   *                <code>arn:aws:rekognition:us-east-1:123456789012:project/getting-started/version/<i>my-model.2020-01-21T09.10.15</i>/1234567890123</code>.</p>\n   */\n  VersionNames?: string[];\n\n  /**\n   * <p>If the previous response was incomplete (because there is more\n   *            results to retrieve), Amazon Rekognition Custom Labels returns a pagination token in the response.\n   *            You can use this pagination token to retrieve the next set of results. </p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>The maximum number of results to return per paginated call.\n   *           The largest value you can specify is 100. If you specify a value greater than 100, a ValidationException\n   *           error occurs. The default value is 100. </p>\n   */\n  MaxResults?: number;\n}\n\nexport namespace DescribeProjectVersionsRequest {\n  export const filterSensitiveLog = (obj: DescribeProjectVersionsRequest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The S3 bucket that contains the training summary. The training summary includes\n *          aggregated evaluation metrics for the entire testing dataset and metrics for each\n *          individual label.  </p>\n *          <p>You get the training summary S3 bucket location by calling <a>DescribeProjectVersions</a>.\n *          </p>\n */\nexport interface Summary {\n  /**\n   * <p>Provides the S3 bucket name and object name.</p>\n   *          <p>The region for the S3 bucket containing the S3 object must match the region you use for\n   *       Amazon Rekognition operations.</p>\n   *\n   *          <p>For Amazon Rekognition to process an S3 object, the user must have permission to\n   *       access the S3 object. For more information, see Resource-Based Policies in the Amazon Rekognition\n   *       Developer Guide. </p>\n   */\n  S3Object?: S3Object;\n}\n\nexport namespace Summary {\n  export const filterSensitiveLog = (obj: Summary): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The evaluation results for the training of a model.</p>\n */\nexport interface EvaluationResult {\n  /**\n   * <p>The F1 score for the evaluation of all labels. The F1 score metric evaluates the overall precision\n   *          and recall performance of the model as a single value. A higher value indicates better precision\n   *          and recall performance. A lower score indicates that precision, recall, or both are performing poorly.\n   *\n   *   </p>\n   */\n  F1Score?: number;\n\n  /**\n   * <p>The S3 bucket that contains the training summary.</p>\n   */\n  Summary?: Summary;\n}\n\nexport namespace EvaluationResult {\n  export const filterSensitiveLog = (obj: EvaluationResult): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Contains the Amazon S3 bucket location of the validation data for a model training job. </p>\n *\n *          <p>The validation data includes error information for individual\n *          JSON lines in the dataset.\n *             For more information, see Debugging a Failed Model Training in the\n *             Amazon Rekognition Custom Labels Developer Guide. </p>\n *          <p>You get the <code>ValidationData</code> object for the training dataset (<a>TrainingDataResult</a>)\n *          and the test dataset (<a>TestingDataResult</a>) by calling <a>DescribeProjectVersions</a>. </p>\n *          <p>The assets array contains a single <a>Asset</a> object.\n *          The <a>GroundTruthManifest</a> field of the Asset object contains the S3 bucket location of\n *          the validation data.\n * </p>\n */\nexport interface ValidationData {\n  /**\n   * <p>The assets that comprise the validation data. </p>\n   */\n  Assets?: Asset[];\n}\n\nexport namespace ValidationData {\n  export const filterSensitiveLog = (obj: ValidationData): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Sagemaker Groundtruth format manifest files for the input, output and validation datasets that are used and created during testing.</p>\n */\nexport interface TestingDataResult {\n  /**\n   * <p>The testing dataset that was supplied for training.</p>\n   */\n  Input?: TestingData;\n\n  /**\n   * <p>The subset of the dataset that was actually tested. Some images (assets) might not be tested due to\n   *          file formatting and other issues. </p>\n   */\n  Output?: TestingData;\n\n  /**\n   * <p>The location of the data validation manifest. The data validation manifest is created for the test dataset during model training.</p>\n   */\n  Validation?: ValidationData;\n}\n\nexport namespace TestingDataResult {\n  export const filterSensitiveLog = (obj: TestingDataResult): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Sagemaker Groundtruth format manifest files for the input, output and validation datasets that are used and created during testing.</p>\n */\nexport interface TrainingDataResult {\n  /**\n   * <p>The training assets that you supplied for training.</p>\n   */\n  Input?: TrainingData;\n\n  /**\n   * <p>The images (assets) that were actually trained by Amazon Rekognition Custom Labels. </p>\n   */\n  Output?: TrainingData;\n\n  /**\n   * <p>The location of the data validation manifest. The data validation manifest is created for the training dataset during model training.</p>\n   */\n  Validation?: ValidationData;\n}\n\nexport namespace TrainingDataResult {\n  export const filterSensitiveLog = (obj: TrainingDataResult): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The description of a version of a model.</p>\n */\nexport interface ProjectVersionDescription {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the model version. </p>\n   */\n  ProjectVersionArn?: string;\n\n  /**\n   * <p>The Unix datetime for the date and time that training started.</p>\n   */\n  CreationTimestamp?: Date;\n\n  /**\n   * <p>The minimum number of inference units used by the model. For more information,\n   *       see <a>StartProjectVersion</a>.</p>\n   */\n  MinInferenceUnits?: number;\n\n  /**\n   * <p>The current status of the model version.</p>\n   */\n  Status?: ProjectVersionStatus | string;\n\n  /**\n   * <p>A descriptive message for an error or warning that occurred.</p>\n   */\n  StatusMessage?: string;\n\n  /**\n   * <p>The duration, in seconds, that the model version has been billed for training.\n   *       This value is only returned if the model version has been successfully trained.</p>\n   */\n  BillableTrainingTimeInSeconds?: number;\n\n  /**\n   * <p>The Unix date and time that training of the model ended.</p>\n   */\n  TrainingEndTimestamp?: Date;\n\n  /**\n   * <p>The location where training results are saved.</p>\n   */\n  OutputConfig?: OutputConfig;\n\n  /**\n   * <p>Contains information about the training results.</p>\n   */\n  TrainingDataResult?: TrainingDataResult;\n\n  /**\n   * <p>Contains information about the testing results.</p>\n   */\n  TestingDataResult?: TestingDataResult;\n\n  /**\n   * <p>The training results. <code>EvaluationResult</code> is only returned if training is successful.</p>\n   */\n  EvaluationResult?: EvaluationResult;\n\n  /**\n   * <p>The location of the summary manifest. The summary manifest provides aggregate data validation results for the training\n   *          and test datasets.</p>\n   */\n  ManifestSummary?: GroundTruthManifest;\n}\n\nexport namespace ProjectVersionDescription {\n  export const filterSensitiveLog = (obj: ProjectVersionDescription): any => ({\n    ...obj,\n  });\n}\n\nexport interface DescribeProjectVersionsResponse {\n  /**\n   * <p>A list of model descriptions. The list is sorted by the creation date and time of\n   *          the model versions, latest to earliest.</p>\n   */\n  ProjectVersionDescriptions?: ProjectVersionDescription[];\n\n  /**\n   * <p>If the previous response was incomplete (because there is more\n   *          results to retrieve), Amazon Rekognition Custom Labels returns a pagination token in the response.\n   *          You can use this pagination token to retrieve the next set of results. </p>\n   */\n  NextToken?: string;\n}\n\nexport namespace DescribeProjectVersionsResponse {\n  export const filterSensitiveLog = (obj: DescribeProjectVersionsResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface DescribeStreamProcessorRequest {\n  /**\n   * <p>Name of the stream processor for which you want information.</p>\n   */\n  Name: string | undefined;\n}\n\nexport namespace DescribeStreamProcessorRequest {\n  export const filterSensitiveLog = (obj: DescribeStreamProcessorRequest): any => ({\n    ...obj,\n  });\n}\n\nexport enum StreamProcessorStatus {\n  FAILED = \"FAILED\",\n  RUNNING = \"RUNNING\",\n  STARTING = \"STARTING\",\n  STOPPED = \"STOPPED\",\n  STOPPING = \"STOPPING\",\n}\n\nexport interface DescribeStreamProcessorResponse {\n  /**\n   * <p>Name of the stream processor. </p>\n   */\n  Name?: string;\n\n  /**\n   * <p>ARN of the stream processor.</p>\n   */\n  StreamProcessorArn?: string;\n\n  /**\n   * <p>Current status of the stream processor.</p>\n   */\n  Status?: StreamProcessorStatus | string;\n\n  /**\n   * <p>Detailed status message about the stream processor.</p>\n   */\n  StatusMessage?: string;\n\n  /**\n   * <p>Date and time the stream processor was created</p>\n   */\n  CreationTimestamp?: Date;\n\n  /**\n   * <p>The time, in Unix format, the stream processor was last updated. For example, when the stream\n   *         processor moves from a running state to a failed state, or when the user starts or stops the stream processor.</p>\n   */\n  LastUpdateTimestamp?: Date;\n\n  /**\n   * <p>Kinesis video stream that provides the source streaming video.</p>\n   */\n  Input?: StreamProcessorInput;\n\n  /**\n   * <p>Kinesis data stream to which Amazon Rekognition Video puts the analysis results.</p>\n   */\n  Output?: StreamProcessorOutput;\n\n  /**\n   * <p>ARN of the IAM role that allows access to the stream processor.</p>\n   */\n  RoleArn?: string;\n\n  /**\n   * <p>Face recognition input parameters that are being used by the stream processor.\n   *             Includes the collection to use for face recognition and the face\n   *             attributes to detect.</p>\n   */\n  Settings?: StreamProcessorSettings;\n}\n\nexport namespace DescribeStreamProcessorResponse {\n  export const filterSensitiveLog = (obj: DescribeStreamProcessorResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface DetectCustomLabelsRequest {\n  /**\n   * <p>The ARN of the model version that you want to use.</p>\n   */\n  ProjectVersionArn: string | undefined;\n\n  /**\n   * <p>Provides the input image either as bytes or an S3 object.</p>\n   *          <p>You pass image bytes to an Amazon Rekognition API operation by using the <code>Bytes</code>\n   *       property. For example, you would use the <code>Bytes</code> property to pass an image loaded\n   *       from a local file system. Image bytes passed by using the <code>Bytes</code> property must be\n   *       base64-encoded. Your code may not need to encode image bytes if you are using an AWS SDK to\n   *       call Amazon Rekognition API operations. </p>\n   *\n   *          <p>For more information, see Analyzing an Image Loaded from a Local File System\n   *       in the Amazon Rekognition Developer Guide.</p>\n   *          <p> You pass images stored in an S3 bucket to an Amazon Rekognition API operation by using the\n   *         <code>S3Object</code> property. Images stored in an S3 bucket do not need to be\n   *       base64-encoded.</p>\n   *          <p>The region for the S3 bucket containing the S3 object must match the region you use for\n   *       Amazon Rekognition operations.</p>\n   *          <p>If you use the\n   *       AWS\n   *       CLI to call Amazon Rekognition operations, passing image bytes using the Bytes\n   *       property is not supported. You must first upload the image to an Amazon S3 bucket and then\n   *       call the operation using the S3Object property.</p>\n   *\n   *          <p>For Amazon Rekognition to process an S3 object, the user must have permission to access the S3\n   *       object. For more information, see Resource Based Policies in the Amazon Rekognition Developer Guide.\n   *     </p>\n   */\n  Image: Image | undefined;\n\n  /**\n   * <p>Maximum number of results you want the service to return in the response.\n   *          The service returns the specified number of highest confidence labels ranked from highest confidence\n   *       to lowest.</p>\n   */\n  MaxResults?: number;\n\n  /**\n   * <p>Specifies the minimum confidence level for the labels to return.\n   *          Amazon Rekognition doesn't return any labels with a confidence lower than this specified value. If you specify a\n   *       value of 0, all labels are return, regardless of the default thresholds that the model version applies.</p>\n   */\n  MinConfidence?: number;\n}\n\nexport namespace DetectCustomLabelsRequest {\n  export const filterSensitiveLog = (obj: DetectCustomLabelsRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface DetectCustomLabelsResponse {\n  /**\n   * <p>An array of custom labels detected in the input image.</p>\n   */\n  CustomLabels?: CustomLabel[];\n}\n\nexport namespace DetectCustomLabelsResponse {\n  export const filterSensitiveLog = (obj: DetectCustomLabelsResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The requested resource isn't ready. For example,\n *          this exception occurs when you call <code>DetectCustomLabels</code> with a\n *          model version that isn't deployed. </p>\n */\nexport interface ResourceNotReadyException extends __SmithyException, $MetadataBearer {\n  name: \"ResourceNotReadyException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace ResourceNotReadyException {\n  export const filterSensitiveLog = (obj: ResourceNotReadyException): any => ({\n    ...obj,\n  });\n}\n\nexport interface DetectFacesRequest {\n  /**\n   * <p>The input image as base64-encoded bytes or an S3 object. If you use the AWS CLI to call\n   *       Amazon Rekognition operations, passing base64-encoded image bytes is not supported. </p>\n   *          <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes\n   *       passed using the <code>Bytes</code> field.\n   *       For more information, see Images in the Amazon Rekognition developer guide.</p>\n   */\n  Image: Image | undefined;\n\n  /**\n   * <p>An array of facial attributes you want to be returned. This can be the default list of\n   *       attributes or all attributes. If you don't specify a value for <code>Attributes</code> or if\n   *       you specify <code>[\"DEFAULT\"]</code>, the API returns the following subset of facial\n   *       attributes: <code>BoundingBox</code>, <code>Confidence</code>, <code>Pose</code>,\n   *         <code>Quality</code>, and <code>Landmarks</code>. If you provide <code>[\"ALL\"]</code>, all\n   *       facial attributes are returned, but the operation takes longer to complete.</p>\n   *          <p>If you provide both, <code>[\"ALL\", \"DEFAULT\"]</code>, the service uses a logical AND\n   *       operator to determine which attributes to return (in this case, all attributes). </p>\n   */\n  Attributes?: (Attribute | string)[];\n}\n\nexport namespace DetectFacesRequest {\n  export const filterSensitiveLog = (obj: DetectFacesRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface DetectFacesResponse {\n  /**\n   * <p>Details of each face found in the image. </p>\n   */\n  FaceDetails?: FaceDetail[];\n\n  /**\n   * <p>The value of <code>OrientationCorrection</code> is always null.</p>\n   *          <p>If the input image is in .jpeg format, it might contain exchangeable image file format (Exif) metadata\n   *       that includes the image's orientation. Amazon Rekognition uses this orientation information to perform\n   *       image correction. The bounding box coordinates are translated to represent object locations\n   *       after the orientation information in the Exif metadata is used to correct the image orientation.\n   *       Images in .png format don't contain Exif metadata.</p>\n   *          <p>Amazon Rekognition doesn’t perform image correction for images in .png format and\n   *       .jpeg images without orientation information in the image Exif metadata. The bounding box\n   *       coordinates aren't translated and represent the object locations before the image is rotated.\n   *     </p>\n   */\n  OrientationCorrection?: OrientationCorrection | string;\n}\n\nexport namespace DetectFacesResponse {\n  export const filterSensitiveLog = (obj: DetectFacesResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>A set of parameters that allow you to filter out certain results from your returned results.</p>\n */\nexport interface DetectionFilter {\n  /**\n   * <p>Sets confidence of word detection. Words with detection confidence below this will be excluded\n   *       from the result. Values should be between 0.5 and 1 as Text in Video will not return any result below\n   *       0.5.</p>\n   */\n  MinConfidence?: number;\n\n  /**\n   * <p>Sets the minimum height of the word bounding box. Words with bounding box heights lesser than\n   *       this value will be excluded from the result. Value is relative to the video frame height.</p>\n   */\n  MinBoundingBoxHeight?: number;\n\n  /**\n   * <p>Sets the minimum width of the word bounding box. Words with bounding boxes widths lesser than\n   *       this value will be excluded from the result. Value is relative to the video frame width.</p>\n   */\n  MinBoundingBoxWidth?: number;\n}\n\nexport namespace DetectionFilter {\n  export const filterSensitiveLog = (obj: DetectionFilter): any => ({\n    ...obj,\n  });\n}\n\nexport interface DetectLabelsRequest {\n  /**\n   * <p>The input image as base64-encoded bytes or an S3 object. If you use the AWS CLI to call\n   *       Amazon Rekognition operations, passing image bytes is not supported. Images stored in an S3 Bucket do\n   *     not need to be base64-encoded.</p>\n   *          <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes\n   *       passed using the <code>Bytes</code> field.\n   *       For more information, see Images in the Amazon Rekognition developer guide.</p>\n   */\n  Image: Image | undefined;\n\n  /**\n   * <p>Maximum number of labels you want the service to return in the response. The service\n   *       returns the specified number of highest confidence labels. </p>\n   */\n  MaxLabels?: number;\n\n  /**\n   * <p>Specifies the minimum confidence level for the labels to return. Amazon Rekognition doesn't\n   *       return any labels with confidence lower than this specified value.</p>\n   *          <p>If <code>MinConfidence</code> is not specified, the operation returns labels with a\n   *       confidence values greater than or equal to 55 percent.</p>\n   */\n  MinConfidence?: number;\n}\n\nexport namespace DetectLabelsRequest {\n  export const filterSensitiveLog = (obj: DetectLabelsRequest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>An instance of a label returned by Amazon Rekognition Image (<a>DetectLabels</a>)\n *       or by Amazon Rekognition Video (<a>GetLabelDetection</a>).</p>\n */\nexport interface Instance {\n  /**\n   * <p>The position of the label instance on the image.</p>\n   */\n  BoundingBox?: BoundingBox;\n\n  /**\n   * <p>The confidence that Amazon Rekognition has in the accuracy of the bounding box.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace Instance {\n  export const filterSensitiveLog = (obj: Instance): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>A parent label for a label. A label can have 0, 1, or more parents. </p>\n */\nexport interface Parent {\n  /**\n   * <p>The name of the parent label.</p>\n   */\n  Name?: string;\n}\n\nexport namespace Parent {\n  export const filterSensitiveLog = (obj: Parent): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Structure containing details about the detected label, including the name, detected instances, parent labels, and level of\n *       confidence.</p>\n *          <p>\n *     </p>\n */\nexport interface Label {\n  /**\n   * <p>The name (label) of the object or scene.</p>\n   */\n  Name?: string;\n\n  /**\n   * <p>Level of confidence.</p>\n   */\n  Confidence?: number;\n\n  /**\n   * <p>If <code>Label</code> represents an object, <code>Instances</code> contains the bounding boxes for each instance of the detected object.\n   *       Bounding boxes are returned for common object labels such as people, cars, furniture, apparel or pets.</p>\n   */\n  Instances?: Instance[];\n\n  /**\n   * <p>The parent labels for a label. The response includes all ancestor labels.</p>\n   */\n  Parents?: Parent[];\n}\n\nexport namespace Label {\n  export const filterSensitiveLog = (obj: Label): any => ({\n    ...obj,\n  });\n}\n\nexport interface DetectLabelsResponse {\n  /**\n   * <p>An array of labels for the real-world objects detected. </p>\n   */\n  Labels?: Label[];\n\n  /**\n   * <p>The value of <code>OrientationCorrection</code> is always null.</p>\n   *          <p>If the input image is in .jpeg format, it might contain exchangeable image file format (Exif) metadata\n   *       that includes the image's orientation. Amazon Rekognition uses this orientation information to perform\n   *       image correction. The bounding box coordinates are translated to represent object locations\n   *       after the orientation information in the Exif metadata is used to correct the image orientation.\n   *       Images in .png format don't contain Exif metadata.</p>\n   *          <p>Amazon Rekognition doesn’t perform image correction for images in .png format and\n   *          .jpeg images without orientation information in the image Exif metadata. The bounding box\n   *          coordinates aren't translated and represent the object locations before the image is rotated.\n   *       </p>\n   */\n  OrientationCorrection?: OrientationCorrection | string;\n\n  /**\n   * <p>Version number of the label detection model that was used to detect labels.</p>\n   */\n  LabelModelVersion?: string;\n}\n\nexport namespace DetectLabelsResponse {\n  export const filterSensitiveLog = (obj: DetectLabelsResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Allows you to set attributes of the image. Currently, you can declare an image as free of\n *       personally identifiable information.</p>\n */\nexport interface HumanLoopDataAttributes {\n  /**\n   * <p>Sets whether the input image is free of personally identifiable information.</p>\n   */\n  ContentClassifiers?: (ContentClassifier | string)[];\n}\n\nexport namespace HumanLoopDataAttributes {\n  export const filterSensitiveLog = (obj: HumanLoopDataAttributes): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Sets up the flow definition the image will be sent to if one of the conditions is met.\n *       You can also set certain attributes of the image before review.</p>\n */\nexport interface HumanLoopConfig {\n  /**\n   * <p>The name of the human review used for this image. This should be kept unique within a region.</p>\n   */\n  HumanLoopName: string | undefined;\n\n  /**\n   * <p>The Amazon Resource Name (ARN) of the flow definition. You can create a flow definition by using the Amazon Sagemaker\n   *       <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateFlowDefinition.html\">CreateFlowDefinition</a>\n   *      Operation. </p>\n   */\n  FlowDefinitionArn: string | undefined;\n\n  /**\n   * <p>Sets attributes of the input data.</p>\n   */\n  DataAttributes?: HumanLoopDataAttributes;\n}\n\nexport namespace HumanLoopConfig {\n  export const filterSensitiveLog = (obj: HumanLoopConfig): any => ({\n    ...obj,\n  });\n}\n\nexport interface DetectModerationLabelsRequest {\n  /**\n   * <p>The input image as base64-encoded bytes or an S3 object.\n   *       If you use the AWS CLI to call Amazon Rekognition operations,\n   *       passing base64-encoded image bytes is not supported. </p>\n   *          <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes\n   *       passed using the <code>Bytes</code> field.\n   *       For more information, see Images in the Amazon Rekognition developer guide.</p>\n   */\n  Image: Image | undefined;\n\n  /**\n   * <p>Specifies the minimum confidence level for the labels to return. Amazon Rekognition doesn't\n   *       return any labels with a confidence level lower than this specified value.</p>\n   *          <p>If you don't specify <code>MinConfidence</code>, the operation returns labels with\n   *       confidence values greater than or equal to 50 percent.</p>\n   */\n  MinConfidence?: number;\n\n  /**\n   * <p>Sets up the configuration for human evaluation, including the FlowDefinition\n   *       the image will be sent to.</p>\n   */\n  HumanLoopConfig?: HumanLoopConfig;\n}\n\nexport namespace DetectModerationLabelsRequest {\n  export const filterSensitiveLog = (obj: DetectModerationLabelsRequest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Shows the results of the human in the loop evaluation. If there is no HumanLoopArn, the input did\n *        not trigger human review.</p>\n */\nexport interface HumanLoopActivationOutput {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the HumanLoop created.</p>\n   */\n  HumanLoopArn?: string;\n\n  /**\n   * <p>Shows if and why human review was needed.</p>\n   */\n  HumanLoopActivationReasons?: string[];\n\n  /**\n   * <p>Shows the result of condition evaluations, including those conditions which activated a\n   *       human review.</p>\n   */\n  HumanLoopActivationConditionsEvaluationResults?: __LazyJsonString | string;\n}\n\nexport namespace HumanLoopActivationOutput {\n  export const filterSensitiveLog = (obj: HumanLoopActivationOutput): any => ({\n    ...obj,\n  });\n}\n\nexport interface DetectModerationLabelsResponse {\n  /**\n   * <p>Array of detected Moderation labels and the time, in milliseconds from the\n   *       start of the video, they were detected.</p>\n   */\n  ModerationLabels?: ModerationLabel[];\n\n  /**\n   * <p>Version number of the moderation detection model that was used to detect unsafe content.</p>\n   */\n  ModerationModelVersion?: string;\n\n  /**\n   * <p>Shows the results of the human in the loop evaluation.</p>\n   */\n  HumanLoopActivationOutput?: HumanLoopActivationOutput;\n}\n\nexport namespace DetectModerationLabelsResponse {\n  export const filterSensitiveLog = (obj: DetectModerationLabelsResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The number of in-progress human reviews you have has exceeded the number allowed.</p>\n */\nexport interface HumanLoopQuotaExceededException extends __SmithyException, $MetadataBearer {\n  name: \"HumanLoopQuotaExceededException\";\n  $fault: \"client\";\n  /**\n   * <p>The resource type.</p>\n   */\n  ResourceType?: string;\n\n  /**\n   * <p>The quota code.</p>\n   */\n  QuotaCode?: string;\n\n  /**\n   * <p>The service code.</p>\n   */\n  ServiceCode?: string;\n\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace HumanLoopQuotaExceededException {\n  export const filterSensitiveLog = (obj: HumanLoopQuotaExceededException): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Specifies summary attributes to return from a call to <a>DetectProtectiveEquipment</a>.\n *          You can specify which types of PPE to summarize. You can also specify a minimum confidence value for detections.\n *          Summary information is returned in the <code>Summary</code> (<a>ProtectiveEquipmentSummary</a>) field of the response from\n *          <code>DetectProtectiveEquipment</code>.\n *          The summary includes which persons in an image were detected wearing the requested types of person protective equipment (PPE), which persons\n *          were detected as not wearing PPE, and the persons in which a determination could not be made. For more information,\n *          see <a>ProtectiveEquipmentSummary</a>.</p>\n */\nexport interface ProtectiveEquipmentSummarizationAttributes {\n  /**\n   * <p>The minimum confidence level for which you want summary information.\n   *          The confidence level applies to person detection, body part detection, equipment detection, and body part coverage.\n   *          Amazon Rekognition doesn't return summary information with a confidence than this specified value. There isn't a\n   *          default value.</p>\n   *          <p>Specify a <code>MinConfidence</code> value that is between 50-100% as <code>DetectProtectiveEquipment</code>\n   *          returns predictions only where the detection confidence is between 50% - 100%.\n   *          If you specify a value that is less than 50%, the results are the same specifying a value of 50%.</p>\n   *          <p>\n   *       </p>\n   */\n  MinConfidence: number | undefined;\n\n  /**\n   * <p>An array of personal protective equipment types for which you want summary information.\n   *          If a person is detected wearing a required requipment type, the person's ID is added to the\n   *          <code>PersonsWithRequiredEquipment</code> array field returned in <a>ProtectiveEquipmentSummary</a>\n   *          by <code>DetectProtectiveEquipment</code>.  </p>\n   */\n  RequiredEquipmentTypes: (ProtectiveEquipmentType | string)[] | undefined;\n}\n\nexport namespace ProtectiveEquipmentSummarizationAttributes {\n  export const filterSensitiveLog = (obj: ProtectiveEquipmentSummarizationAttributes): any => ({\n    ...obj,\n  });\n}\n\nexport interface DetectProtectiveEquipmentRequest {\n  /**\n   * <p>The image in which you want to detect PPE on detected persons. The image can be passed as image bytes or you can\n   *          reference an image stored in an Amazon S3 bucket. </p>\n   */\n  Image: Image | undefined;\n\n  /**\n   * <p>An array of PPE types that you want to summarize.</p>\n   */\n  SummarizationAttributes?: ProtectiveEquipmentSummarizationAttributes;\n}\n\nexport namespace DetectProtectiveEquipmentRequest {\n  export const filterSensitiveLog = (obj: DetectProtectiveEquipmentRequest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>A person detected by a call to <a>DetectProtectiveEquipment</a>. The API returns\n *          all persons detected in the input\n *          image in an array of <code>ProtectiveEquipmentPerson</code> objects.</p>\n */\nexport interface ProtectiveEquipmentPerson {\n  /**\n   * <p>An array of body parts detected on a person's body (including body parts without PPE). </p>\n   */\n  BodyParts?: ProtectiveEquipmentBodyPart[];\n\n  /**\n   * <p>A bounding box around the detected person.</p>\n   */\n  BoundingBox?: BoundingBox;\n\n  /**\n   * <p>The confidence that Amazon Rekognition has that the bounding box contains a person.</p>\n   */\n  Confidence?: number;\n\n  /**\n   * <p>The identifier for the detected person. The identifier is only unique for a single call to\n   *          <code>DetectProtectiveEquipment</code>.</p>\n   */\n  Id?: number;\n}\n\nexport namespace ProtectiveEquipmentPerson {\n  export const filterSensitiveLog = (obj: ProtectiveEquipmentPerson): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Summary information for required items of personal protective equipment (PPE) detected on persons by a call\n *          to <a>DetectProtectiveEquipment</a>. You specify the required type of PPE in\n *          the <code>SummarizationAttributes</code>\n *          (<a>ProtectiveEquipmentSummarizationAttributes</a>) input parameter.\n *          The summary includes which persons were detected wearing the required personal protective equipment\n *          (<code>PersonsWithRequiredEquipment</code>),\n *          which persons were detected as not wearing the required PPE (<code>PersonsWithoutRequiredEquipment</code>),\n *          and the persons in which a determination\n *          could not be made (<code>PersonsIndeterminate</code>).</p>\n *          <p>To get a total for each category, use the size of the field array. For example,\n *          to find out how many people were detected as wearing the specified PPE,\n *          use the size of the <code>PersonsWithRequiredEquipment</code> array.\n *          If you want to find out more about a person, such as the\n *          location (<a>BoundingBox</a>) of the person on the image, use the person ID in each array element.\n *          Each person ID matches the ID field of a <a>ProtectiveEquipmentPerson</a> object returned\n *          in the <code>Persons</code> array by <code>DetectProtectiveEquipment</code>.</p>\n */\nexport interface ProtectiveEquipmentSummary {\n  /**\n   * <p>An array of IDs for persons who are wearing detected personal protective equipment.\n   *       </p>\n   */\n  PersonsWithRequiredEquipment?: number[];\n\n  /**\n   * <p>An array of IDs for persons who are not wearing all of the types of PPE specified in the RequiredEquipmentTypes field of\n   *          the detected personal protective equipment.\n   *       </p>\n   */\n  PersonsWithoutRequiredEquipment?: number[];\n\n  /**\n   * <p>An array of IDs for persons where it was not possible to determine if they are wearing personal protective equipment.\n   *       </p>\n   */\n  PersonsIndeterminate?: number[];\n}\n\nexport namespace ProtectiveEquipmentSummary {\n  export const filterSensitiveLog = (obj: ProtectiveEquipmentSummary): any => ({\n    ...obj,\n  });\n}\n\nexport interface DetectProtectiveEquipmentResponse {\n  /**\n   * <p>The version number of the PPE detection model used to detect PPE in the image.</p>\n   */\n  ProtectiveEquipmentModelVersion?: string;\n\n  /**\n   * <p>An array of persons detected in the image (including persons not wearing PPE).</p>\n   */\n  Persons?: ProtectiveEquipmentPerson[];\n\n  /**\n   * <p>Summary information for the types of PPE specified in the <code>SummarizationAttributes</code> input\n   *       parameter.</p>\n   */\n  Summary?: ProtectiveEquipmentSummary;\n}\n\nexport namespace DetectProtectiveEquipmentResponse {\n  export const filterSensitiveLog = (obj: DetectProtectiveEquipmentResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Specifies a location within the frame that Rekognition checks for text. Uses a <code>BoundingBox</code>\n *       object to set a region of the screen.</p>\n *          <p>A word is included in the region if the word is more than half in that region. If there is more than\n *       one region, the word will be compared with all regions of the screen. Any word more than half in a region\n *       is kept in the results.</p>\n */\nexport interface RegionOfInterest {\n  /**\n   * <p>The box representing a region of interest on screen.</p>\n   */\n  BoundingBox?: BoundingBox;\n}\n\nexport namespace RegionOfInterest {\n  export const filterSensitiveLog = (obj: RegionOfInterest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>A set of optional parameters that you can use to set the criteria that the text must meet to be included in your response.\n *       <code>WordFilter</code> looks at a word’s height, width, and minimum confidence. <code>RegionOfInterest</code>\n *       lets you set a specific region of the image to look for text in.\n *       </p>\n */\nexport interface DetectTextFilters {\n  /**\n   * <p>A set of parameters that allow you to filter out certain results from your returned results.</p>\n   */\n  WordFilter?: DetectionFilter;\n\n  /**\n   * <p> A Filter focusing on a certain area of the image. Uses a <code>BoundingBox</code> object to set the region\n   *       of the image.</p>\n   */\n  RegionsOfInterest?: RegionOfInterest[];\n}\n\nexport namespace DetectTextFilters {\n  export const filterSensitiveLog = (obj: DetectTextFilters): any => ({\n    ...obj,\n  });\n}\n\nexport interface DetectTextRequest {\n  /**\n   * <p>The input image as base64-encoded bytes or an Amazon S3 object. If you use the AWS CLI\n   *       to call Amazon Rekognition operations, you can't pass image bytes. </p>\n   *          <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes\n   *       passed using the <code>Bytes</code> field.\n   *       For more information, see Images in the Amazon Rekognition developer guide.</p>\n   */\n  Image: Image | undefined;\n\n  /**\n   * <p>Optional parameters that let you set the criteria that the text must meet to be included in your response.</p>\n   */\n  Filters?: DetectTextFilters;\n}\n\nexport namespace DetectTextRequest {\n  export const filterSensitiveLog = (obj: DetectTextRequest): any => ({\n    ...obj,\n  });\n}\n\nexport enum TextTypes {\n  LINE = \"LINE\",\n  WORD = \"WORD\",\n}\n\n/**\n * <p>Information about a word or line of text detected by <a>DetectText</a>.</p>\n *          <p>The <code>DetectedText</code> field contains the text that Amazon Rekognition detected in the\n *       image. </p>\n *          <p>Every word and line has an identifier (<code>Id</code>). Each word belongs to a line\n *       and has a parent identifier (<code>ParentId</code>) that identifies the line of text in which\n *       the word appears. The word <code>Id</code> is also an index for the word within a line of\n *       words. </p>\n *\n *          <p>For more information, see Detecting Text in the Amazon Rekognition Developer Guide.</p>\n */\nexport interface TextDetection {\n  /**\n   * <p>The word or line of text recognized by Amazon Rekognition. </p>\n   */\n  DetectedText?: string;\n\n  /**\n   * <p>The type of text that was detected.</p>\n   */\n  Type?: TextTypes | string;\n\n  /**\n   * <p>The identifier for the detected text. The identifier is only unique for a single call\n   *       to <code>DetectText</code>. </p>\n   */\n  Id?: number;\n\n  /**\n   * <p>The Parent identifier for the detected text identified by the value of <code>ID</code>.\n   *       If the type of detected text is <code>LINE</code>, the value of <code>ParentId</code> is\n   *         <code>Null</code>. </p>\n   */\n  ParentId?: number;\n\n  /**\n   * <p>The confidence that Amazon Rekognition has in the accuracy of the detected text and the accuracy\n   *       of the geometry points around the detected text.</p>\n   */\n  Confidence?: number;\n\n  /**\n   * <p>The location of the detected text on the image. Includes an axis aligned coarse\n   *       bounding box surrounding the text and a finer grain polygon for more accurate spatial\n   *       information.</p>\n   */\n  Geometry?: Geometry;\n}\n\nexport namespace TextDetection {\n  export const filterSensitiveLog = (obj: TextDetection): any => ({\n    ...obj,\n  });\n}\n\nexport interface DetectTextResponse {\n  /**\n   * <p>An array of text that was detected in the input image.</p>\n   */\n  TextDetections?: TextDetection[];\n\n  /**\n   * <p>The model version used to detect text.</p>\n   */\n  TextModelVersion?: string;\n}\n\nexport namespace DetectTextResponse {\n  export const filterSensitiveLog = (obj: DetectTextResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Describes the face properties such as the bounding box, face ID, image ID of the input\n *       image, and external image ID that you assigned. </p>\n */\nexport interface Face {\n  /**\n   * <p>Unique identifier that Amazon Rekognition assigns to the face.</p>\n   */\n  FaceId?: string;\n\n  /**\n   * <p>Bounding box of the face.</p>\n   */\n  BoundingBox?: BoundingBox;\n\n  /**\n   * <p>Unique identifier that Amazon Rekognition assigns to the input image.</p>\n   */\n  ImageId?: string;\n\n  /**\n   * <p>Identifier that you assign to all the faces in the input image.</p>\n   */\n  ExternalImageId?: string;\n\n  /**\n   * <p>Confidence level that the bounding box contains a face (and not a different object such\n   *       as a tree).</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace Face {\n  export const filterSensitiveLog = (obj: Face): any => ({\n    ...obj,\n  });\n}\n\nexport enum FaceAttributes {\n  ALL = \"ALL\",\n  DEFAULT = \"DEFAULT\",\n}\n\n/**\n * <p>Information about a face detected in a video analysis request and the time the face was detected in the video. </p>\n */\nexport interface FaceDetection {\n  /**\n   * <p>Time, in milliseconds from the start of the video, that the face was detected.</p>\n   */\n  Timestamp?: number;\n\n  /**\n   * <p>The face properties for the detected face.</p>\n   */\n  Face?: FaceDetail;\n}\n\nexport namespace FaceDetection {\n  export const filterSensitiveLog = (obj: FaceDetection): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Provides face metadata. In addition, it also provides the confidence in the match of\n *       this face with the input face.</p>\n */\nexport interface FaceMatch {\n  /**\n   * <p>Confidence in the match of this face with the input face.</p>\n   */\n  Similarity?: number;\n\n  /**\n   * <p>Describes the face properties such as the bounding box, face ID, image ID of the source\n   *       image, and external image ID that you assigned.</p>\n   */\n  Face?: Face;\n}\n\nexport namespace FaceMatch {\n  export const filterSensitiveLog = (obj: FaceMatch): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Object containing both the face metadata (stored in the backend database), and facial\n *       attributes that are detected but aren't stored in the database.</p>\n */\nexport interface FaceRecord {\n  /**\n   * <p>Describes the face properties such as the bounding box, face ID, image ID of the input\n   *       image, and external image ID that you assigned. </p>\n   */\n  Face?: Face;\n\n  /**\n   * <p>Structure containing attributes of the face that the algorithm detected.</p>\n   */\n  FaceDetail?: FaceDetail;\n}\n\nexport namespace FaceRecord {\n  export const filterSensitiveLog = (obj: FaceRecord): any => ({\n    ...obj,\n  });\n}\n\nexport enum FaceSearchSortBy {\n  INDEX = \"INDEX\",\n  TIMESTAMP = \"TIMESTAMP\",\n}\n\nexport interface GetCelebrityInfoRequest {\n  /**\n   * <p>The ID for the celebrity. You get the celebrity ID from a call to the <a>RecognizeCelebrities</a> operation,\n   *    which recognizes celebrities in an image. </p>\n   */\n  Id: string | undefined;\n}\n\nexport namespace GetCelebrityInfoRequest {\n  export const filterSensitiveLog = (obj: GetCelebrityInfoRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetCelebrityInfoResponse {\n  /**\n   * <p>An array of URLs pointing to additional celebrity information. </p>\n   */\n  Urls?: string[];\n\n  /**\n   * <p>The name of the celebrity.</p>\n   */\n  Name?: string;\n}\n\nexport namespace GetCelebrityInfoResponse {\n  export const filterSensitiveLog = (obj: GetCelebrityInfoResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetCelebrityRecognitionRequest {\n  /**\n   * <p>Job identifier for the required celebrity recognition analysis. You can get the job identifer from\n   *       a call to <code>StartCelebrityRecognition</code>.</p>\n   */\n  JobId: string | undefined;\n\n  /**\n   * <p>Maximum number of results to return per paginated call. The largest value you can specify is 1000.\n   *       If you specify a value greater than 1000, a maximum of 1000 results is returned.\n   *       The default value is 1000.</p>\n   */\n  MaxResults?: number;\n\n  /**\n   * <p>If the previous response was incomplete (because there is more recognized celebrities to retrieve), Amazon Rekognition Video returns a pagination\n   *       token in the response. You can use this pagination token to retrieve the next set of celebrities. </p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Sort to use for celebrities returned in <code>Celebrities</code> field. Specify <code>ID</code> to sort by the celebrity identifier,\n   *         specify <code>TIMESTAMP</code> to sort by the time the celebrity was recognized.</p>\n   */\n  SortBy?: CelebrityRecognitionSortBy | string;\n}\n\nexport namespace GetCelebrityRecognitionRequest {\n  export const filterSensitiveLog = (obj: GetCelebrityRecognitionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport enum VideoJobStatus {\n  FAILED = \"FAILED\",\n  IN_PROGRESS = \"IN_PROGRESS\",\n  SUCCEEDED = \"SUCCEEDED\",\n}\n\n/**\n * <p>Information about a video that Amazon Rekognition analyzed. <code>Videometadata</code> is returned in\n *             every page of paginated responses from a Amazon Rekognition video operation.</p>\n */\nexport interface VideoMetadata {\n  /**\n   * <p>Type of compression used in the analyzed video. </p>\n   */\n  Codec?: string;\n\n  /**\n   * <p>Length of the video in milliseconds.</p>\n   */\n  DurationMillis?: number;\n\n  /**\n   * <p>Format of the analyzed video. Possible values are MP4, MOV and AVI. </p>\n   */\n  Format?: string;\n\n  /**\n   * <p>Number of frames per second in the video.</p>\n   */\n  FrameRate?: number;\n\n  /**\n   * <p>Vertical pixel dimension of the video.</p>\n   */\n  FrameHeight?: number;\n\n  /**\n   * <p>Horizontal pixel dimension of the video.</p>\n   */\n  FrameWidth?: number;\n}\n\nexport namespace VideoMetadata {\n  export const filterSensitiveLog = (obj: VideoMetadata): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetCelebrityRecognitionResponse {\n  /**\n   * <p>The current status of the celebrity recognition job.</p>\n   */\n  JobStatus?: VideoJobStatus | string;\n\n  /**\n   * <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>\n   */\n  StatusMessage?: string;\n\n  /**\n   * <p>Information about a video that Amazon Rekognition Video analyzed. <code>Videometadata</code> is returned in\n   *       every page of paginated responses from a Amazon Rekognition Video operation.</p>\n   */\n  VideoMetadata?: VideoMetadata;\n\n  /**\n   * <p>If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent request\n   *       to retrieve the next set of celebrities.</p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Array of celebrities recognized in the video.</p>\n   */\n  Celebrities?: CelebrityRecognition[];\n}\n\nexport namespace GetCelebrityRecognitionResponse {\n  export const filterSensitiveLog = (obj: GetCelebrityRecognitionResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetContentModerationRequest {\n  /**\n   * <p>The identifier for the unsafe content job. Use <code>JobId</code> to identify the job in\n   *        a subsequent call to <code>GetContentModeration</code>.</p>\n   */\n  JobId: string | undefined;\n\n  /**\n   * <p>Maximum number of results to return per paginated call. The largest value you can specify is 1000.\n   *     If you specify a value greater than 1000, a maximum of 1000 results is returned.\n   *     The default value is 1000.</p>\n   */\n  MaxResults?: number;\n\n  /**\n   * <p>If the previous response was incomplete (because there is more data to retrieve), Amazon Rekognition\n   *         returns a pagination token in the response. You can use this pagination token\n   *         to retrieve the next set of unsafe content labels.</p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Sort to use for elements in the <code>ModerationLabelDetections</code> array.\n   *        Use <code>TIMESTAMP</code> to sort array elements by the time labels are detected.\n   *        Use <code>NAME</code> to alphabetically group elements for a label together.\n   *        Within each label group, the array element are sorted by detection confidence.\n   *        The default sort is by <code>TIMESTAMP</code>.</p>\n   */\n  SortBy?: ContentModerationSortBy | string;\n}\n\nexport namespace GetContentModerationRequest {\n  export const filterSensitiveLog = (obj: GetContentModerationRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetContentModerationResponse {\n  /**\n   * <p>The current status of the unsafe content analysis job.</p>\n   */\n  JobStatus?: VideoJobStatus | string;\n\n  /**\n   * <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>\n   */\n  StatusMessage?: string;\n\n  /**\n   * <p>Information about a video that Amazon Rekognition analyzed. <code>Videometadata</code>\n   *      is returned in every page of paginated responses from <code>GetContentModeration</code>. </p>\n   */\n  VideoMetadata?: VideoMetadata;\n\n  /**\n   * <p>The detected unsafe content labels and the time(s) they were detected.</p>\n   */\n  ModerationLabels?: ContentModerationDetection[];\n\n  /**\n   * <p>If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent\n   *      request to retrieve the next set of unsafe content labels. </p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Version number of the moderation detection model that was used to detect unsafe content.</p>\n   */\n  ModerationModelVersion?: string;\n}\n\nexport namespace GetContentModerationResponse {\n  export const filterSensitiveLog = (obj: GetContentModerationResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetFaceDetectionRequest {\n  /**\n   * <p>Unique identifier for the face detection job. The <code>JobId</code> is returned from <code>StartFaceDetection</code>.</p>\n   */\n  JobId: string | undefined;\n\n  /**\n   * <p>Maximum number of results to return per paginated call. The largest value you can specify is 1000.\n   *        If you specify a value greater than 1000, a maximum of 1000 results is returned.\n   *        The default value is 1000.</p>\n   */\n  MaxResults?: number;\n\n  /**\n   * <p>If the previous response was incomplete (because there are more faces to retrieve), Amazon Rekognition Video returns a pagination\n   *        token in the response. You can use this pagination token to retrieve the next set of faces.</p>\n   */\n  NextToken?: string;\n}\n\nexport namespace GetFaceDetectionRequest {\n  export const filterSensitiveLog = (obj: GetFaceDetectionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetFaceDetectionResponse {\n  /**\n   * <p>The current status of the face detection job.</p>\n   */\n  JobStatus?: VideoJobStatus | string;\n\n  /**\n   * <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>\n   */\n  StatusMessage?: string;\n\n  /**\n   * <p>Information about a video that Amazon Rekognition Video analyzed. <code>Videometadata</code> is returned in\n   *        every page of paginated responses from a Amazon Rekognition video operation.</p>\n   */\n  VideoMetadata?: VideoMetadata;\n\n  /**\n   * <p>If the response is truncated, Amazon Rekognition returns this token that you can use in the subsequent request to retrieve the next set of faces. </p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>An array of faces detected in the video. Each element contains a detected face's details and the time,\n   *        in milliseconds from the start of the video, the face was detected. </p>\n   */\n  Faces?: FaceDetection[];\n}\n\nexport namespace GetFaceDetectionResponse {\n  export const filterSensitiveLog = (obj: GetFaceDetectionResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetFaceSearchRequest {\n  /**\n   * <p>The job identifer for the search request. You get the job identifier from an initial call to <code>StartFaceSearch</code>.</p>\n   */\n  JobId: string | undefined;\n\n  /**\n   * <p>Maximum number of results to return per paginated call. The largest value you can specify is 1000.\n   *       If you specify a value greater than 1000, a maximum of 1000 results is returned.\n   *       The default value is 1000.</p>\n   */\n  MaxResults?: number;\n\n  /**\n   * <p>If the previous response was incomplete (because there is more search results to retrieve), Amazon Rekognition Video returns a pagination\n   *       token in the response. You can use this pagination token to retrieve the next set of search results. </p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Sort to use for grouping faces in the response. Use <code>TIMESTAMP</code> to group faces by the time\n   *       that they are recognized. Use <code>INDEX</code> to sort by recognized faces. </p>\n   */\n  SortBy?: FaceSearchSortBy | string;\n}\n\nexport namespace GetFaceSearchRequest {\n  export const filterSensitiveLog = (obj: GetFaceSearchRequest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Details about a person detected in a video analysis request.</p>\n */\nexport interface PersonDetail {\n  /**\n   * <p>Identifier for the person detected person within a video. Use to keep track of the person throughout the video. The identifier is not stored by Amazon Rekognition.</p>\n   */\n  Index?: number;\n\n  /**\n   * <p>Bounding box around the detected person.</p>\n   */\n  BoundingBox?: BoundingBox;\n\n  /**\n   * <p>Face details for the detected person.</p>\n   */\n  Face?: FaceDetail;\n}\n\nexport namespace PersonDetail {\n  export const filterSensitiveLog = (obj: PersonDetail): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Information about a person whose face matches a face(s) in an Amazon Rekognition collection.\n *       Includes information about the faces in the Amazon Rekognition collection (<a>FaceMatch</a>), information about the person (<a>PersonDetail</a>),\n *       and the time stamp for when the person was detected in a video. An array of\n *         <code>PersonMatch</code> objects is returned by <a>GetFaceSearch</a>. </p>\n */\nexport interface PersonMatch {\n  /**\n   * <p>The time, in milliseconds from the beginning of the video, that the person was matched in the video.</p>\n   */\n  Timestamp?: number;\n\n  /**\n   * <p>Information about the matched person.</p>\n   */\n  Person?: PersonDetail;\n\n  /**\n   * <p>Information about the faces in the input collection that match the face of a person in the video.</p>\n   */\n  FaceMatches?: FaceMatch[];\n}\n\nexport namespace PersonMatch {\n  export const filterSensitiveLog = (obj: PersonMatch): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetFaceSearchResponse {\n  /**\n   * <p>The current status of the face search job.</p>\n   */\n  JobStatus?: VideoJobStatus | string;\n\n  /**\n   * <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>\n   */\n  StatusMessage?: string;\n\n  /**\n   * <p>If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent request to retrieve the next set of search results. </p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Information about a video that Amazon Rekognition analyzed. <code>Videometadata</code> is returned in every page of paginated responses\n   *       from a Amazon Rekognition Video operation. </p>\n   */\n  VideoMetadata?: VideoMetadata;\n\n  /**\n   * <p>An array of persons,  <a>PersonMatch</a>,\n   *       in the video whose face(s) match the face(s) in an Amazon Rekognition collection. It also includes time information\n   *        for when persons are matched in the video.\n   *       You specify the input collection in an initial call to <code>StartFaceSearch</code>.\n   *       Each  <code>Persons</code> element includes a time the person was matched,\n   *       face match details (<code>FaceMatches</code>) for matching faces in the collection,\n   *        and person information (<code>Person</code>) for the matched person. </p>\n   */\n  Persons?: PersonMatch[];\n}\n\nexport namespace GetFaceSearchResponse {\n  export const filterSensitiveLog = (obj: GetFaceSearchResponse): any => ({\n    ...obj,\n  });\n}\n\nexport enum LabelDetectionSortBy {\n  NAME = \"NAME\",\n  TIMESTAMP = \"TIMESTAMP\",\n}\n\nexport interface GetLabelDetectionRequest {\n  /**\n   * <p>Job identifier for the label detection operation for which you want results returned. You get the job identifer from\n   *       an initial call to <code>StartlabelDetection</code>.</p>\n   */\n  JobId: string | undefined;\n\n  /**\n   * <p>Maximum number of results to return per paginated call. The largest value you can specify is 1000.\n   *        If you specify a value greater than 1000, a maximum of 1000 results is returned.\n   *        The default value is 1000.</p>\n   */\n  MaxResults?: number;\n\n  /**\n   * <p>If the previous response was incomplete (because there are more labels to retrieve), Amazon Rekognition Video returns a pagination\n   *          token in the response. You can use this pagination token to retrieve the next set of labels. </p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Sort to use for elements in the <code>Labels</code> array.\n   *       Use <code>TIMESTAMP</code> to sort array elements by the time labels are detected.\n   *       Use <code>NAME</code> to alphabetically group elements for a label together.\n   *       Within each label group, the array element are sorted by detection confidence.\n   *       The default sort is by <code>TIMESTAMP</code>.</p>\n   */\n  SortBy?: LabelDetectionSortBy | string;\n}\n\nexport namespace GetLabelDetectionRequest {\n  export const filterSensitiveLog = (obj: GetLabelDetectionRequest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Information about a label detected in a video analysis request and the time the label was detected in the video. </p>\n */\nexport interface LabelDetection {\n  /**\n   * <p>Time, in milliseconds from the start of the video, that the label was detected.</p>\n   */\n  Timestamp?: number;\n\n  /**\n   * <p>Details about the detected label.</p>\n   */\n  Label?: Label;\n}\n\nexport namespace LabelDetection {\n  export const filterSensitiveLog = (obj: LabelDetection): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetLabelDetectionResponse {\n  /**\n   * <p>The current status of the label detection job.</p>\n   */\n  JobStatus?: VideoJobStatus | string;\n\n  /**\n   * <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>\n   */\n  StatusMessage?: string;\n\n  /**\n   * <p>Information about a video that Amazon Rekognition Video analyzed. <code>Videometadata</code> is returned in\n   *        every page of paginated responses from a Amazon Rekognition video operation.</p>\n   */\n  VideoMetadata?: VideoMetadata;\n\n  /**\n   * <p>If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent request\n   *         to retrieve the next set of labels.</p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>An array of labels detected in the video. Each element contains the detected label and the time,\n   *         in milliseconds from the start of the video, that the label was detected. </p>\n   */\n  Labels?: LabelDetection[];\n\n  /**\n   * <p>Version number of the label detection model that was used to detect labels.</p>\n   */\n  LabelModelVersion?: string;\n}\n\nexport namespace GetLabelDetectionResponse {\n  export const filterSensitiveLog = (obj: GetLabelDetectionResponse): any => ({\n    ...obj,\n  });\n}\n\nexport enum PersonTrackingSortBy {\n  INDEX = \"INDEX\",\n  TIMESTAMP = \"TIMESTAMP\",\n}\n\nexport interface GetPersonTrackingRequest {\n  /**\n   * <p>The identifier for a job that tracks persons in a video. You get the <code>JobId</code> from a call to <code>StartPersonTracking</code>.\n   *         </p>\n   */\n  JobId: string | undefined;\n\n  /**\n   * <p>Maximum number of results to return per paginated call. The largest value you can specify is 1000.\n   *       If you specify a value greater than 1000, a maximum of 1000 results is returned.\n   *       The default value is 1000.</p>\n   */\n  MaxResults?: number;\n\n  /**\n   * <p>If the previous response was incomplete (because there are more persons to retrieve), Amazon Rekognition Video returns a pagination\n   *        token in the response. You can use this pagination token to retrieve the next set of persons. </p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Sort to use for elements in the <code>Persons</code> array. Use <code>TIMESTAMP</code> to sort array elements\n   *        by the time persons are detected. Use <code>INDEX</code> to sort by the tracked persons.\n   *        If you sort by <code>INDEX</code>, the array elements for each person are sorted by detection confidence.\n   *        The default sort is by <code>TIMESTAMP</code>.</p>\n   */\n  SortBy?: PersonTrackingSortBy | string;\n}\n\nexport namespace GetPersonTrackingRequest {\n  export const filterSensitiveLog = (obj: GetPersonTrackingRequest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Details and path tracking information for a single time a person's path is tracked in a video.\n *             Amazon Rekognition operations that track people's paths return an array of <code>PersonDetection</code> objects\n *             with elements for each time a person's path is tracked in a video. </p>\n *\n *          <p>For more information, see GetPersonTracking in the Amazon Rekognition Developer Guide. </p>\n */\nexport interface PersonDetection {\n  /**\n   * <p>The time, in milliseconds from the start of the video, that the person's path was tracked.</p>\n   */\n  Timestamp?: number;\n\n  /**\n   * <p>Details about a person whose path was tracked in a video.</p>\n   */\n  Person?: PersonDetail;\n}\n\nexport namespace PersonDetection {\n  export const filterSensitiveLog = (obj: PersonDetection): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetPersonTrackingResponse {\n  /**\n   * <p>The current status of the person tracking job.</p>\n   */\n  JobStatus?: VideoJobStatus | string;\n\n  /**\n   * <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>\n   */\n  StatusMessage?: string;\n\n  /**\n   * <p>Information about a video that Amazon Rekognition Video analyzed. <code>Videometadata</code> is returned in\n   *        every page of paginated responses from a Amazon Rekognition Video operation.</p>\n   */\n  VideoMetadata?: VideoMetadata;\n\n  /**\n   * <p>If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent request to retrieve the next set of persons. </p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>An array of the persons detected in the video and the time(s) their path was tracked throughout the video.\n   *         An array element will exist for each time a person's path is tracked. </p>\n   */\n  Persons?: PersonDetection[];\n}\n\nexport namespace GetPersonTrackingResponse {\n  export const filterSensitiveLog = (obj: GetPersonTrackingResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetSegmentDetectionRequest {\n  /**\n   * <p>Job identifier for the text detection operation for which you want results returned.\n   *       You get the job identifer from an initial call to <code>StartSegmentDetection</code>.</p>\n   */\n  JobId: string | undefined;\n\n  /**\n   * <p>Maximum number of results to return per paginated call. The largest value you can specify is 1000.</p>\n   */\n  MaxResults?: number;\n\n  /**\n   * <p>If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent\n   *       request to retrieve the next set of text.</p>\n   */\n  NextToken?: string;\n}\n\nexport namespace GetSegmentDetectionRequest {\n  export const filterSensitiveLog = (obj: GetSegmentDetectionRequest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Information about a shot detection segment detected in a video. For more information,\n *       see <a>SegmentDetection</a>.</p>\n */\nexport interface ShotSegment {\n  /**\n   * <p>An Identifier for a shot detection segment detected in a video. </p>\n   */\n  Index?: number;\n\n  /**\n   * <p>The confidence that Amazon Rekognition Video has in the accuracy of the detected segment.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace ShotSegment {\n  export const filterSensitiveLog = (obj: ShotSegment): any => ({\n    ...obj,\n  });\n}\n\nexport enum TechnicalCueType {\n  BLACK_FRAMES = \"BlackFrames\",\n  COLOR_BARS = \"ColorBars\",\n  END_CREDITS = \"EndCredits\",\n}\n\n/**\n * <p>Information about a technical cue segment. For more information, see <a>SegmentDetection</a>.</p>\n */\nexport interface TechnicalCueSegment {\n  /**\n   * <p>The type of the technical cue.</p>\n   */\n  Type?: TechnicalCueType | string;\n\n  /**\n   * <p>The confidence that Amazon Rekognition Video has in the accuracy of the detected segment.</p>\n   */\n  Confidence?: number;\n}\n\nexport namespace TechnicalCueSegment {\n  export const filterSensitiveLog = (obj: TechnicalCueSegment): any => ({\n    ...obj,\n  });\n}\n\nexport enum SegmentType {\n  SHOT = \"SHOT\",\n  TECHNICAL_CUE = \"TECHNICAL_CUE\",\n}\n\n/**\n * <p>A technical cue or shot detection segment detected in a video. An array\n *     of <code>SegmentDetection</code> objects containing all segments detected in a stored video\n *       is returned by <a>GetSegmentDetection</a>.\n *     </p>\n */\nexport interface SegmentDetection {\n  /**\n   * <p>The type of the  segment. Valid values are <code>TECHNICAL_CUE</code> and <code>SHOT</code>.</p>\n   */\n  Type?: SegmentType | string;\n\n  /**\n   * <p>The start time of the detected segment in milliseconds from the start of the video. This value\n   *       is rounded down. For example, if the actual timestamp is 100.6667 milliseconds, Amazon Rekognition Video returns a value of\n   *       100 millis.</p>\n   */\n  StartTimestampMillis?: number;\n\n  /**\n   * <p>The end time of the detected segment, in milliseconds, from the start of the video.\n   *     This value is rounded down.</p>\n   */\n  EndTimestampMillis?: number;\n\n  /**\n   * <p>The duration of the detected segment in milliseconds. </p>\n   */\n  DurationMillis?: number;\n\n  /**\n   * <p>The frame-accurate SMPTE timecode, from the start of a video, for the start of a detected segment.\n   *       <code>StartTimecode</code> is in <i>HH:MM:SS:fr</i> format\n   *       (and <i>;fr</i> for drop frame-rates). </p>\n   */\n  StartTimecodeSMPTE?: string;\n\n  /**\n   * <p>The frame-accurate SMPTE timecode, from the start of a video, for the end of a detected segment.\n   *       <code>EndTimecode</code> is in <i>HH:MM:SS:fr</i> format\n   *       (and <i>;fr</i> for drop frame-rates).</p>\n   */\n  EndTimecodeSMPTE?: string;\n\n  /**\n   * <p>The duration of the timecode for the detected segment in SMPTE format.</p>\n   */\n  DurationSMPTE?: string;\n\n  /**\n   * <p>If the segment is a technical cue, contains information about the technical cue.</p>\n   */\n  TechnicalCueSegment?: TechnicalCueSegment;\n\n  /**\n   * <p>If the segment is a shot detection, contains information about the shot detection.</p>\n   */\n  ShotSegment?: ShotSegment;\n}\n\nexport namespace SegmentDetection {\n  export const filterSensitiveLog = (obj: SegmentDetection): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Information about the type of a segment requested in a call to <a>StartSegmentDetection</a>.\n *       An array of <code>SegmentTypeInfo</code> objects is returned  by the response from <a>GetSegmentDetection</a>.</p>\n */\nexport interface SegmentTypeInfo {\n  /**\n   * <p>The type of a segment (technical cue or shot detection).</p>\n   */\n  Type?: SegmentType | string;\n\n  /**\n   * <p>The version of the model used to detect segments.</p>\n   */\n  ModelVersion?: string;\n}\n\nexport namespace SegmentTypeInfo {\n  export const filterSensitiveLog = (obj: SegmentTypeInfo): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetSegmentDetectionResponse {\n  /**\n   * <p>Current status of the segment detection job.</p>\n   */\n  JobStatus?: VideoJobStatus | string;\n\n  /**\n   * <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>\n   */\n  StatusMessage?: string;\n\n  /**\n   * <p>Currently, Amazon Rekognition Video returns a single   object in the\n   *       <code>VideoMetadata</code> array. The object\n   *       contains information about the video stream in the input file that Amazon Rekognition Video chose to analyze.\n   *       The <code>VideoMetadata</code> object includes the video codec, video format and other information.\n   *       Video metadata is returned in each page of information returned by <code>GetSegmentDetection</code>.</p>\n   */\n  VideoMetadata?: VideoMetadata[];\n\n  /**\n   * <p>An array of\n   *        objects. There can be multiple audio streams.\n   *       Each <code>AudioMetadata</code> object contains metadata for a single audio stream.\n   *       Audio information in an <code>AudioMetadata</code> objects includes\n   *       the audio codec, the number of audio channels, the duration of the audio stream,\n   *       and the sample rate. Audio metadata is returned in each page of information returned\n   *       by <code>GetSegmentDetection</code>.</p>\n   */\n  AudioMetadata?: AudioMetadata[];\n\n  /**\n   * <p>If the previous response was incomplete (because there are more labels to retrieve), Amazon Rekognition Video returns\n   *       a pagination token in the response. You can use this pagination token to retrieve the next set of text.</p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>An array of segments detected in a video.  The array is sorted by the segment types (TECHNICAL_CUE or SHOT)\n   *       specified in the <code>SegmentTypes</code> input parameter of <code>StartSegmentDetection</code>. Within\n   *       each segment type the array is sorted by timestamp values.</p>\n   */\n  Segments?: SegmentDetection[];\n\n  /**\n   * <p>An array containing the segment types requested in the call to <code>StartSegmentDetection</code>.\n   *     </p>\n   */\n  SelectedSegmentTypes?: SegmentTypeInfo[];\n}\n\nexport namespace GetSegmentDetectionResponse {\n  export const filterSensitiveLog = (obj: GetSegmentDetectionResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetTextDetectionRequest {\n  /**\n   * <p>Job identifier for the text detection operation for which you want results returned.\n   *         You get the job identifer from an initial call to <code>StartTextDetection</code>.</p>\n   */\n  JobId: string | undefined;\n\n  /**\n   * <p>Maximum number of results to return per paginated call. The largest value you can specify is 1000.</p>\n   */\n  MaxResults?: number;\n\n  /**\n   * <p>If the previous response was incomplete (because there are more labels to retrieve), Amazon Rekognition Video returns\n   *       a pagination token in the response. You can use this pagination token to retrieve the next set of text.</p>\n   */\n  NextToken?: string;\n}\n\nexport namespace GetTextDetectionRequest {\n  export const filterSensitiveLog = (obj: GetTextDetectionRequest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Information about text detected in a video. Incudes the detected text,\n *         the time in milliseconds from the start of the video that the text was detected, and where it was detected on the screen.</p>\n */\nexport interface TextDetectionResult {\n  /**\n   * <p>The time, in milliseconds from the start of the video, that the text was detected.</p>\n   */\n  Timestamp?: number;\n\n  /**\n   * <p>Details about text detected in a video.</p>\n   */\n  TextDetection?: TextDetection;\n}\n\nexport namespace TextDetectionResult {\n  export const filterSensitiveLog = (obj: TextDetectionResult): any => ({\n    ...obj,\n  });\n}\n\nexport interface GetTextDetectionResponse {\n  /**\n   * <p>Current status of the text detection job.</p>\n   */\n  JobStatus?: VideoJobStatus | string;\n\n  /**\n   * <p>If the job fails, <code>StatusMessage</code> provides a descriptive error message.</p>\n   */\n  StatusMessage?: string;\n\n  /**\n   * <p>Information about a video that Amazon Rekognition analyzed. <code>Videometadata</code> is returned in\n   *             every page of paginated responses from a Amazon Rekognition video operation.</p>\n   */\n  VideoMetadata?: VideoMetadata;\n\n  /**\n   * <p>An array of text detected in the video. Each element contains the detected text, the time in milliseconds\n   *       from the start of the video that the text was detected, and where it was detected on the screen.</p>\n   */\n  TextDetections?: TextDetectionResult[];\n\n  /**\n   * <p>If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent\n   *         request to retrieve the next set of text.</p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Version number of the text detection model that was used to detect text.</p>\n   */\n  TextModelVersion?: string;\n}\n\nexport namespace GetTextDetectionResponse {\n  export const filterSensitiveLog = (obj: GetTextDetectionResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>A <code>ClientRequestToken</code> input parameter was reused with an operation, but at least one of the other input\n *         parameters is different from the previous call to the operation.</p>\n */\nexport interface IdempotentParameterMismatchException extends __SmithyException, $MetadataBearer {\n  name: \"IdempotentParameterMismatchException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace IdempotentParameterMismatchException {\n  export const filterSensitiveLog = (obj: IdempotentParameterMismatchException): any => ({\n    ...obj,\n  });\n}\n\nexport interface IndexFacesRequest {\n  /**\n   * <p>The ID of an existing collection to which you want to add the faces that are detected\n   *       in the input images.</p>\n   */\n  CollectionId: string | undefined;\n\n  /**\n   * <p>The input image as base64-encoded bytes or an S3 object. If you use the AWS CLI to call\n   *       Amazon Rekognition operations, passing base64-encoded image bytes isn't supported. </p>\n   *          <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes\n   *       passed using the <code>Bytes</code> field.\n   *       For more information, see Images in the Amazon Rekognition developer guide.</p>\n   */\n  Image: Image | undefined;\n\n  /**\n   * <p>The ID you want to assign to all the faces detected in the image.</p>\n   */\n  ExternalImageId?: string;\n\n  /**\n   * <p>An array of facial attributes that you want to be returned. This can be the default\n   *       list of attributes or all attributes. If you don't specify a value for <code>Attributes</code>\n   *       or if you specify <code>[\"DEFAULT\"]</code>, the API returns the following subset of facial\n   *       attributes: <code>BoundingBox</code>, <code>Confidence</code>, <code>Pose</code>,\n   *         <code>Quality</code>, and <code>Landmarks</code>. If you provide <code>[\"ALL\"]</code>, all\n   *       facial attributes are returned, but the operation takes longer to complete.</p>\n   *          <p>If you provide both, <code>[\"ALL\", \"DEFAULT\"]</code>, the service uses a logical AND\n   *       operator to determine which attributes to return (in this case, all attributes). </p>\n   */\n  DetectionAttributes?: (Attribute | string)[];\n\n  /**\n   * <p>The maximum number of faces to index. The value of <code>MaxFaces</code> must be greater\n   *       than or equal to 1. <code>IndexFaces</code> returns no more than 100 detected faces in an\n   *       image, even if you specify a larger value for <code>MaxFaces</code>.</p>\n   *          <p>If <code>IndexFaces</code> detects more faces than the value of <code>MaxFaces</code>, the\n   *       faces with the lowest quality are filtered out first. If there are still more faces than the\n   *       value of <code>MaxFaces</code>, the faces with the smallest bounding boxes are filtered out\n   *       (up to the number that's needed to satisfy the value of <code>MaxFaces</code>). Information\n   *       about the unindexed faces is available in the <code>UnindexedFaces</code> array. </p>\n   *          <p>The faces that are returned by <code>IndexFaces</code> are sorted by the largest face\n   *       bounding box size to the smallest size, in descending order.</p>\n   *          <p>\n   *             <code>MaxFaces</code> can be used with a collection associated with any version of\n   *       the face model.</p>\n   */\n  MaxFaces?: number;\n\n  /**\n   * <p>A filter that specifies a quality bar for how much filtering is done to identify faces.\n   *     Filtered faces aren't indexed. If you specify <code>AUTO</code>, Amazon Rekognition chooses the quality bar.\n   *       If you specify <code>LOW</code>,\n   *       <code>MEDIUM</code>, or <code>HIGH</code>, filtering removes all faces that\n   *       don’t meet the chosen quality bar.  The default value is <code>AUTO</code>.\n   *\n   *       The quality bar is based on a variety of common use cases. Low-quality\n   *       detections can occur for a number of reasons. Some examples are an object that's misidentified\n   *       as a face, a face that's too blurry, or a face with a\n   *       pose that's too extreme to use. If you specify <code>NONE</code>, no\n   *       filtering is performed.\n   *     </p>\n   *          <p>To use quality filtering, the collection you are using must be associated with version 3 of the face model or higher.</p>\n   */\n  QualityFilter?: QualityFilter | string;\n}\n\nexport namespace IndexFacesRequest {\n  export const filterSensitiveLog = (obj: IndexFacesRequest): any => ({\n    ...obj,\n  });\n}\n\nexport enum Reason {\n  EXCEEDS_MAX_FACES = \"EXCEEDS_MAX_FACES\",\n  EXTREME_POSE = \"EXTREME_POSE\",\n  LOW_BRIGHTNESS = \"LOW_BRIGHTNESS\",\n  LOW_CONFIDENCE = \"LOW_CONFIDENCE\",\n  LOW_FACE_QUALITY = \"LOW_FACE_QUALITY\",\n  LOW_SHARPNESS = \"LOW_SHARPNESS\",\n  SMALL_BOUNDING_BOX = \"SMALL_BOUNDING_BOX\",\n}\n\n/**\n * <p>A face that <a>IndexFaces</a> detected, but didn't index. Use the\n *         <code>Reasons</code> response attribute to determine why a face wasn't indexed.</p>\n */\nexport interface UnindexedFace {\n  /**\n   * <p>An array of reasons that specify why a face wasn't indexed. </p>\n   *          <ul>\n   *             <li>\n   *                <p>EXTREME_POSE - The face is at a pose that can't be detected. For example, the head is turned\n   *           too far away from the camera.</p>\n   *             </li>\n   *             <li>\n   *                <p>EXCEEDS_MAX_FACES - The number of faces detected is already higher than that specified by the\n   *       <code>MaxFaces</code> input parameter for <code>IndexFaces</code>.</p>\n   *             </li>\n   *             <li>\n   *                <p>LOW_BRIGHTNESS - The image is too dark.</p>\n   *             </li>\n   *             <li>\n   *                <p>LOW_SHARPNESS - The image is too blurry.</p>\n   *             </li>\n   *             <li>\n   *                <p>LOW_CONFIDENCE - The face was detected with a low confidence.</p>\n   *             </li>\n   *             <li>\n   *                <p>SMALL_BOUNDING_BOX - The bounding box around the face is too small.</p>\n   *             </li>\n   *          </ul>\n   */\n  Reasons?: (Reason | string)[];\n\n  /**\n   * <p>The\n   *       structure that contains attributes of a face that\n   *       <code>IndexFaces</code>detected, but didn't index. </p>\n   */\n  FaceDetail?: FaceDetail;\n}\n\nexport namespace UnindexedFace {\n  export const filterSensitiveLog = (obj: UnindexedFace): any => ({\n    ...obj,\n  });\n}\n\nexport interface IndexFacesResponse {\n  /**\n   * <p>An array of faces detected and added to the collection.\n   *       For more information, see Searching Faces in a Collection in the Amazon Rekognition Developer Guide.\n   *     </p>\n   */\n  FaceRecords?: FaceRecord[];\n\n  /**\n   * <p>If your collection is associated with a face detection model that's later\n   *       than version 3.0, the value of <code>OrientationCorrection</code>\n   *       is always null and no orientation information is returned.</p>\n   *\n   *          <p>If your collection is associated with a face detection model that's\n   *       version 3.0 or earlier, the following applies:</p>\n   *          <ul>\n   *             <li>\n   *                <p>If the input image is in .jpeg format, it might contain exchangeable image file format (Exif) metadata\n   *         that includes the image's orientation. Amazon Rekognition uses this orientation information to perform\n   *         image correction - the bounding box coordinates are translated to represent object locations\n   *         after the orientation information in the Exif metadata is used to correct the image orientation.\n   *         Images in .png format don't contain Exif metadata. The value of <code>OrientationCorrection</code>\n   *         is null.</p>\n   *             </li>\n   *             <li>\n   *                <p>If the image doesn't contain orientation information in its Exif metadata, Amazon Rekognition returns\n   *       an estimated orientation (ROTATE_0, ROTATE_90, ROTATE_180, ROTATE_270). Amazon Rekognition doesn’t perform\n   *       image correction for images. The bounding box coordinates aren't translated and represent the\n   *       object locations before the image is rotated.</p>\n   *             </li>\n   *          </ul>\n   *\n   *\n   *\n   *          <p>Bounding box information is returned in the <code>FaceRecords</code> array. You can get the\n   *     version of the face detection model by calling <a>DescribeCollection</a>. </p>\n   */\n  OrientationCorrection?: OrientationCorrection | string;\n\n  /**\n   * <p>The version number of the face detection model that's associated with the input\n   *       collection (<code>CollectionId</code>).</p>\n   */\n  FaceModelVersion?: string;\n\n  /**\n   * <p>An array of faces that were detected in the image but weren't indexed. They weren't\n   *       indexed because the quality filter identified them as low quality, or the\n   *         <code>MaxFaces</code> request parameter filtered them out. To use the quality filter, you\n   *       specify the <code>QualityFilter</code> request parameter.</p>\n   */\n  UnindexedFaces?: UnindexedFace[];\n}\n\nexport namespace IndexFacesResponse {\n  export const filterSensitiveLog = (obj: IndexFacesResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p></p>\n *\n *\n *          <p>The size of the collection exceeds the allowed limit. For more information, see\n *       Limits in Amazon Rekognition in the Amazon Rekognition Developer Guide. </p>\n */\nexport interface ServiceQuotaExceededException extends __SmithyException, $MetadataBearer {\n  name: \"ServiceQuotaExceededException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace ServiceQuotaExceededException {\n  export const filterSensitiveLog = (obj: ServiceQuotaExceededException): any => ({\n    ...obj,\n  });\n}\n\nexport interface ListCollectionsRequest {\n  /**\n   * <p>Pagination token from the previous response.</p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Maximum number of collection IDs to return. </p>\n   */\n  MaxResults?: number;\n}\n\nexport namespace ListCollectionsRequest {\n  export const filterSensitiveLog = (obj: ListCollectionsRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface ListCollectionsResponse {\n  /**\n   * <p>An array of collection IDs.</p>\n   */\n  CollectionIds?: string[];\n\n  /**\n   * <p>If the result is truncated, the response provides a <code>NextToken</code> that you can\n   *       use in the subsequent request to fetch the next set of collection IDs.</p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Version numbers of the face detection models associated with the collections in the array <code>CollectionIds</code>.\n   *     For example, the value of <code>FaceModelVersions[2]</code> is the version number for the face detection model used\n   *       by the collection in <code>CollectionId[2]</code>.</p>\n   */\n  FaceModelVersions?: string[];\n}\n\nexport namespace ListCollectionsResponse {\n  export const filterSensitiveLog = (obj: ListCollectionsResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface ListFacesRequest {\n  /**\n   * <p>ID of the collection from which to list the faces.</p>\n   */\n  CollectionId: string | undefined;\n\n  /**\n   * <p>If the previous response was incomplete (because there is more data to retrieve),\n   *       Amazon Rekognition returns a pagination token in the response. You can use this pagination token to\n   *       retrieve the next set of faces.</p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Maximum number of faces to return.</p>\n   */\n  MaxResults?: number;\n}\n\nexport namespace ListFacesRequest {\n  export const filterSensitiveLog = (obj: ListFacesRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface ListFacesResponse {\n  /**\n   * <p>An array of <code>Face</code> objects. </p>\n   */\n  Faces?: Face[];\n\n  /**\n   * <p>If the response is truncated, Amazon Rekognition returns this token that you can use in the\n   *       subsequent request to retrieve the next set of faces.</p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Version number of the face detection model associated with the input collection (<code>CollectionId</code>).</p>\n   */\n  FaceModelVersion?: string;\n}\n\nexport namespace ListFacesResponse {\n  export const filterSensitiveLog = (obj: ListFacesResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface ListStreamProcessorsRequest {\n  /**\n   * <p>If the previous response was incomplete (because there are more stream processors to retrieve), Amazon Rekognition Video\n   *             returns a pagination token in the response. You can use this pagination token to retrieve the next set of stream processors. </p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>Maximum number of stream processors you want Amazon Rekognition Video to return in the response. The default is 1000. </p>\n   */\n  MaxResults?: number;\n}\n\nexport namespace ListStreamProcessorsRequest {\n  export const filterSensitiveLog = (obj: ListStreamProcessorsRequest): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>An object that recognizes faces in a streaming video. An Amazon Rekognition stream processor is created by a call to <a>CreateStreamProcessor</a>.  The request\n *         parameters for <code>CreateStreamProcessor</code> describe the Kinesis video stream source for the streaming video, face recognition parameters, and where to stream the analysis resullts.\n *\n *         </p>\n */\nexport interface StreamProcessor {\n  /**\n   * <p>Name of the Amazon Rekognition stream processor. </p>\n   */\n  Name?: string;\n\n  /**\n   * <p>Current status of the Amazon Rekognition stream processor.</p>\n   */\n  Status?: StreamProcessorStatus | string;\n}\n\nexport namespace StreamProcessor {\n  export const filterSensitiveLog = (obj: StreamProcessor): any => ({\n    ...obj,\n  });\n}\n\nexport interface ListStreamProcessorsResponse {\n  /**\n   * <p>If the response is truncated, Amazon Rekognition Video returns this token that you can use in the subsequent\n   *             request to retrieve the next set of stream processors. </p>\n   */\n  NextToken?: string;\n\n  /**\n   * <p>List of stream processors that you have created.</p>\n   */\n  StreamProcessors?: StreamProcessor[];\n}\n\nexport namespace ListStreamProcessorsResponse {\n  export const filterSensitiveLog = (obj: ListStreamProcessorsResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The Amazon Simple Notification Service topic to which Amazon Rekognition publishes the completion status of a video analysis operation. For more information, see\n *             <a>api-video</a>.</p>\n */\nexport interface NotificationChannel {\n  /**\n   * <p>The Amazon SNS topic to which Amazon Rekognition to posts the completion status.</p>\n   */\n  SNSTopicArn: string | undefined;\n\n  /**\n   * <p>The ARN of an IAM role that gives Amazon Rekognition publishing permissions to the Amazon SNS topic. </p>\n   */\n  RoleArn: string | undefined;\n}\n\nexport namespace NotificationChannel {\n  export const filterSensitiveLog = (obj: NotificationChannel): any => ({\n    ...obj,\n  });\n}\n\nexport interface RecognizeCelebritiesRequest {\n  /**\n   * <p>The input image as base64-encoded bytes or an S3 object. If you use the AWS CLI to call\n   *       Amazon Rekognition operations, passing base64-encoded image bytes is not supported. </p>\n   *          <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes\n   *       passed using the <code>Bytes</code> field.\n   *       For more information, see Images in the Amazon Rekognition developer guide.</p>\n   */\n  Image: Image | undefined;\n}\n\nexport namespace RecognizeCelebritiesRequest {\n  export const filterSensitiveLog = (obj: RecognizeCelebritiesRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface RecognizeCelebritiesResponse {\n  /**\n   * <p>Details about each celebrity found in the image. Amazon Rekognition can detect a maximum of 64\n   *       celebrities in an image.</p>\n   */\n  CelebrityFaces?: Celebrity[];\n\n  /**\n   * <p>Details about each unrecognized face in the image.</p>\n   */\n  UnrecognizedFaces?: ComparedFace[];\n\n  /**\n   * <p>The orientation of the input image (counterclockwise direction). If your application\n   *       displays the image, you can use this value to correct the orientation. The bounding box\n   *       coordinates returned in <code>CelebrityFaces</code> and <code>UnrecognizedFaces</code>\n   *       represent face locations before the image orientation is corrected. </p>\n   *          <note>\n   *             <p>If the input image is in .jpeg format, it might contain exchangeable image (Exif)\n   *         metadata that includes the image's orientation. If so, and the Exif metadata for the input\n   *         image populates the orientation field, the value of <code>OrientationCorrection</code> is\n   *         null. The <code>CelebrityFaces</code> and <code>UnrecognizedFaces</code> bounding box\n   *         coordinates represent face locations after Exif metadata is used to correct the image\n   *         orientation. Images in .png format don't contain Exif metadata. </p>\n   *          </note>\n   */\n  OrientationCorrection?: OrientationCorrection | string;\n}\n\nexport namespace RecognizeCelebritiesResponse {\n  export const filterSensitiveLog = (obj: RecognizeCelebritiesResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface SearchFacesRequest {\n  /**\n   * <p>ID of the collection the face belongs to.</p>\n   */\n  CollectionId: string | undefined;\n\n  /**\n   * <p>ID of a face to find matches for in the collection.</p>\n   */\n  FaceId: string | undefined;\n\n  /**\n   * <p>Maximum number of faces to return. The operation returns the maximum number of faces\n   *       with the highest confidence in the match.</p>\n   */\n  MaxFaces?: number;\n\n  /**\n   * <p>Optional value specifying the minimum confidence in the face match to return. For\n   *       example, don't return any matches where confidence in matches is less than 70%.\n   *       The default value is 80%.\n   *     </p>\n   */\n  FaceMatchThreshold?: number;\n}\n\nexport namespace SearchFacesRequest {\n  export const filterSensitiveLog = (obj: SearchFacesRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface SearchFacesResponse {\n  /**\n   * <p>ID of the face that was searched for matches in a collection.</p>\n   */\n  SearchedFaceId?: string;\n\n  /**\n   * <p>An array of faces that matched the input face, along with the confidence in the\n   *       match.</p>\n   */\n  FaceMatches?: FaceMatch[];\n\n  /**\n   * <p>Version number of the face detection model associated with the input collection (<code>CollectionId</code>).</p>\n   */\n  FaceModelVersion?: string;\n}\n\nexport namespace SearchFacesResponse {\n  export const filterSensitiveLog = (obj: SearchFacesResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface SearchFacesByImageRequest {\n  /**\n   * <p>ID of the collection to search.</p>\n   */\n  CollectionId: string | undefined;\n\n  /**\n   * <p>The input image as base64-encoded bytes or an S3 object.\n   *       If you use the AWS CLI to call Amazon Rekognition operations,\n   *       passing base64-encoded image bytes is not supported. </p>\n   *          <p>If you are using an AWS SDK to call Amazon Rekognition, you might not need to base64-encode image bytes\n   *       passed using the <code>Bytes</code> field.\n   *       For more information, see Images in the Amazon Rekognition developer guide.</p>\n   */\n  Image: Image | undefined;\n\n  /**\n   * <p>Maximum number of faces to return. The operation returns the maximum number of faces\n   *       with the highest confidence in the match.</p>\n   */\n  MaxFaces?: number;\n\n  /**\n   * <p>(Optional) Specifies the minimum confidence in the face match to return. For example,\n   *       don't return any matches where confidence in matches is less than 70%.\n   *     The default value is 80%.</p>\n   */\n  FaceMatchThreshold?: number;\n\n  /**\n   * <p>A filter that specifies a quality bar for how much filtering is done to identify faces.\n   *       Filtered faces aren't searched for in the collection. If you specify <code>AUTO</code>, Amazon Rekognition\n   *       chooses the quality bar.  If you specify <code>LOW</code>,\n   *       <code>MEDIUM</code>, or <code>HIGH</code>, filtering removes all faces that\n   *       don’t meet the chosen quality bar.\n   *\n   *       The quality bar is based on a variety of common use cases. Low-quality\n   *       detections can occur for a number of reasons. Some examples are an object that's misidentified\n   *       as a face, a face that's too blurry, or a face with a\n   *       pose that's too extreme to use. If you specify <code>NONE</code>, no\n   *       filtering is performed.  The default value is <code>NONE</code>.\n   *     </p>\n   *          <p>To use quality filtering, the collection you are using must be associated with version 3 of the face model or higher.</p>\n   */\n  QualityFilter?: QualityFilter | string;\n}\n\nexport namespace SearchFacesByImageRequest {\n  export const filterSensitiveLog = (obj: SearchFacesByImageRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface SearchFacesByImageResponse {\n  /**\n   * <p>The bounding box around the face in the input image that Amazon Rekognition used for the\n   *       search.</p>\n   */\n  SearchedFaceBoundingBox?: BoundingBox;\n\n  /**\n   * <p>The level of confidence that the <code>searchedFaceBoundingBox</code>, contains a\n   *       face.</p>\n   */\n  SearchedFaceConfidence?: number;\n\n  /**\n   * <p>An array of faces that match the input face, along with the confidence in the\n   *       match.</p>\n   */\n  FaceMatches?: FaceMatch[];\n\n  /**\n   * <p>Version number of the face detection model associated with the input collection (<code>CollectionId</code>).</p>\n   */\n  FaceModelVersion?: string;\n}\n\nexport namespace SearchFacesByImageResponse {\n  export const filterSensitiveLog = (obj: SearchFacesByImageResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Video file stored in an Amazon S3 bucket. Amazon Rekognition video start operations such as <a>StartLabelDetection</a> use <code>Video</code> to\n *             specify a video for analysis. The supported file formats are .mp4, .mov and .avi.</p>\n */\nexport interface Video {\n  /**\n   * <p>The Amazon S3 bucket name and file name for the video.</p>\n   */\n  S3Object?: S3Object;\n}\n\nexport namespace Video {\n  export const filterSensitiveLog = (obj: Video): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartCelebrityRecognitionRequest {\n  /**\n   * <p>The video in which you want to recognize celebrities. The video must be stored\n   *       in an Amazon S3 bucket.</p>\n   */\n  Video: Video | undefined;\n\n  /**\n   * <p>Idempotent token used to identify the start request. If you use the same token with multiple\n   *     <code>StartCelebrityRecognition</code> requests, the same <code>JobId</code> is returned. Use\n   *       <code>ClientRequestToken</code> to prevent the same job from being accidently started more than once. </p>\n   */\n  ClientRequestToken?: string;\n\n  /**\n   * <p>The Amazon SNS topic ARN that you want Amazon Rekognition Video to publish the completion status of the\n   *       celebrity recognition analysis to.</p>\n   */\n  NotificationChannel?: NotificationChannel;\n\n  /**\n   * <p>An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic.\n   *       For example, you can use <code>JobTag</code> to group related jobs and identify them in the completion notification.</p>\n   */\n  JobTag?: string;\n}\n\nexport namespace StartCelebrityRecognitionRequest {\n  export const filterSensitiveLog = (obj: StartCelebrityRecognitionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartCelebrityRecognitionResponse {\n  /**\n   * <p>The identifier for the celebrity recognition analysis job. Use <code>JobId</code> to identify the job in\n   *       a subsequent call to <code>GetCelebrityRecognition</code>.</p>\n   */\n  JobId?: string;\n}\n\nexport namespace StartCelebrityRecognitionResponse {\n  export const filterSensitiveLog = (obj: StartCelebrityRecognitionResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>The file size or duration of the supplied media is too large. The maximum file size is 10GB.\n *         The maximum duration is 6 hours. </p>\n */\nexport interface VideoTooLargeException extends __SmithyException, $MetadataBearer {\n  name: \"VideoTooLargeException\";\n  $fault: \"client\";\n  Message?: string;\n  Code?: string;\n  /**\n   * <p>A universally unique identifier (UUID) for the request.</p>\n   */\n  Logref?: string;\n}\n\nexport namespace VideoTooLargeException {\n  export const filterSensitiveLog = (obj: VideoTooLargeException): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartContentModerationRequest {\n  /**\n   * <p>The video in which you want to detect unsafe content. The video must be stored\n   *       in an Amazon S3 bucket.</p>\n   */\n  Video: Video | undefined;\n\n  /**\n   * <p>Specifies the minimum confidence that Amazon Rekognition must have in order to return a moderated content label. Confidence\n   *       represents how certain Amazon Rekognition is that the moderated content is correctly identified. 0 is the lowest confidence.\n   *       100 is the highest confidence.  Amazon Rekognition doesn't return any moderated content labels with a confidence level\n   *       lower than this specified value. If you don't specify <code>MinConfidence</code>, <code>GetContentModeration</code>\n   *        returns labels with confidence values greater than or equal to 50 percent.</p>\n   */\n  MinConfidence?: number;\n\n  /**\n   * <p>Idempotent token used to identify the start request. If you use the same token with multiple\n   *       <code>StartContentModeration</code> requests, the same <code>JobId</code> is returned. Use\n   *       <code>ClientRequestToken</code> to prevent the same job from being accidently started more than once. </p>\n   */\n  ClientRequestToken?: string;\n\n  /**\n   * <p>The Amazon SNS topic ARN that you want Amazon Rekognition Video to publish the completion status of the\n   *       unsafe content analysis to.</p>\n   */\n  NotificationChannel?: NotificationChannel;\n\n  /**\n   * <p>An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic.\n   *       For example, you can use <code>JobTag</code> to group related jobs and identify them in the completion notification.</p>\n   */\n  JobTag?: string;\n}\n\nexport namespace StartContentModerationRequest {\n  export const filterSensitiveLog = (obj: StartContentModerationRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartContentModerationResponse {\n  /**\n   * <p>The identifier for the unsafe content analysis job. Use <code>JobId</code> to identify the job in\n   *       a subsequent call to <code>GetContentModeration</code>.</p>\n   */\n  JobId?: string;\n}\n\nexport namespace StartContentModerationResponse {\n  export const filterSensitiveLog = (obj: StartContentModerationResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartFaceDetectionRequest {\n  /**\n   * <p>The video in which you want to detect faces. The video must be stored\n   *       in an Amazon S3 bucket.</p>\n   */\n  Video: Video | undefined;\n\n  /**\n   * <p>Idempotent token used to identify the start request. If you use the same token with multiple\n   *       <code>StartFaceDetection</code> requests, the same <code>JobId</code> is returned. Use\n   *       <code>ClientRequestToken</code> to prevent the same job from being accidently started more than once. </p>\n   */\n  ClientRequestToken?: string;\n\n  /**\n   * <p>The ARN of the Amazon SNS topic to which you want Amazon Rekognition Video to publish the completion status of the\n   *          face detection operation.</p>\n   */\n  NotificationChannel?: NotificationChannel;\n\n  /**\n   * <p>The face attributes you want returned.</p>\n   *          <p>\n   *             <code>DEFAULT</code> - The following subset of facial attributes are returned: BoundingBox, Confidence, Pose, Quality and Landmarks. </p>\n   *          <p>\n   *             <code>ALL</code> - All facial attributes are returned.</p>\n   */\n  FaceAttributes?: FaceAttributes | string;\n\n  /**\n   * <p>An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic.\n   *       For example, you can use <code>JobTag</code> to group related jobs and identify them in the completion notification.</p>\n   */\n  JobTag?: string;\n}\n\nexport namespace StartFaceDetectionRequest {\n  export const filterSensitiveLog = (obj: StartFaceDetectionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartFaceDetectionResponse {\n  /**\n   * <p>The identifier for the face detection job. Use <code>JobId</code> to identify the job in\n   *     a subsequent call to <code>GetFaceDetection</code>.</p>\n   */\n  JobId?: string;\n}\n\nexport namespace StartFaceDetectionResponse {\n  export const filterSensitiveLog = (obj: StartFaceDetectionResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartFaceSearchRequest {\n  /**\n   * <p>The video you want to search. The video must be stored in an Amazon S3 bucket. </p>\n   */\n  Video: Video | undefined;\n\n  /**\n   * <p>Idempotent token used to identify the start request. If you use the same token with multiple\n   *       <code>StartFaceSearch</code> requests, the same <code>JobId</code> is returned. Use\n   *       <code>ClientRequestToken</code> to prevent the same job from being accidently started more than once. </p>\n   */\n  ClientRequestToken?: string;\n\n  /**\n   * <p>The minimum confidence in the person match to return. For example, don't return any matches where confidence in matches is less than 70%.\n   *       The default value is 80%.</p>\n   */\n  FaceMatchThreshold?: number;\n\n  /**\n   * <p>ID of the collection that contains the faces you want to search for.</p>\n   */\n  CollectionId: string | undefined;\n\n  /**\n   * <p>The ARN of the Amazon SNS topic to which you want Amazon Rekognition Video to publish the completion status of the search. </p>\n   */\n  NotificationChannel?: NotificationChannel;\n\n  /**\n   * <p>An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic.\n   *       For example, you can use <code>JobTag</code> to group related jobs and identify them in the completion notification.</p>\n   */\n  JobTag?: string;\n}\n\nexport namespace StartFaceSearchRequest {\n  export const filterSensitiveLog = (obj: StartFaceSearchRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartFaceSearchResponse {\n  /**\n   * <p>The identifier for the search job. Use <code>JobId</code> to identify the job in a subsequent call to <code>GetFaceSearch</code>. </p>\n   */\n  JobId?: string;\n}\n\nexport namespace StartFaceSearchResponse {\n  export const filterSensitiveLog = (obj: StartFaceSearchResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartLabelDetectionRequest {\n  /**\n   * <p>The video in which you want to detect labels. The video must be stored\n   *       in an Amazon S3 bucket.</p>\n   */\n  Video: Video | undefined;\n\n  /**\n   * <p>Idempotent token used to identify the start request. If you use the same token with multiple\n   *       <code>StartLabelDetection</code> requests, the same <code>JobId</code> is returned. Use\n   *       <code>ClientRequestToken</code> to prevent the same job from being accidently started more than once. </p>\n   */\n  ClientRequestToken?: string;\n\n  /**\n   * <p>Specifies the minimum confidence that Amazon Rekognition Video must have in order to return a detected label. Confidence\n   *        represents how certain Amazon Rekognition is that a label is correctly identified.0 is the lowest confidence.\n   *        100 is the highest confidence.  Amazon Rekognition Video doesn't return any labels with a confidence level\n   *        lower than this specified value.</p>\n   *          <p>If you don't specify <code>MinConfidence</code>, the operation returns labels with confidence\n   *      values greater than or equal to 50 percent.</p>\n   */\n  MinConfidence?: number;\n\n  /**\n   * <p>The Amazon SNS topic ARN you want Amazon Rekognition Video to publish the completion status of the label detection\n   *         operation to. </p>\n   */\n  NotificationChannel?: NotificationChannel;\n\n  /**\n   * <p>An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic.\n   *       For example, you can use <code>JobTag</code> to group related jobs and identify them in the completion notification.</p>\n   */\n  JobTag?: string;\n}\n\nexport namespace StartLabelDetectionRequest {\n  export const filterSensitiveLog = (obj: StartLabelDetectionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartLabelDetectionResponse {\n  /**\n   * <p>The identifier for the label detection job. Use <code>JobId</code> to identify the job in\n   *     a subsequent call to <code>GetLabelDetection</code>. </p>\n   */\n  JobId?: string;\n}\n\nexport namespace StartLabelDetectionResponse {\n  export const filterSensitiveLog = (obj: StartLabelDetectionResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartPersonTrackingRequest {\n  /**\n   * <p>The video in which you want to detect people. The video must be stored\n   *       in an Amazon S3 bucket.</p>\n   */\n  Video: Video | undefined;\n\n  /**\n   * <p>Idempotent token used to identify the start request. If you use the same token with multiple\n   *       <code>StartPersonTracking</code> requests, the same <code>JobId</code> is returned. Use\n   *       <code>ClientRequestToken</code> to prevent the same job from being accidently started more than once. </p>\n   */\n  ClientRequestToken?: string;\n\n  /**\n   * <p>The Amazon SNS topic ARN you want Amazon Rekognition Video to publish the completion status of the people detection\n   *         operation to.</p>\n   */\n  NotificationChannel?: NotificationChannel;\n\n  /**\n   * <p>An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic.\n   *       For example, you can use <code>JobTag</code> to group related jobs and identify them in the completion notification.</p>\n   */\n  JobTag?: string;\n}\n\nexport namespace StartPersonTrackingRequest {\n  export const filterSensitiveLog = (obj: StartPersonTrackingRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartPersonTrackingResponse {\n  /**\n   * <p>The identifier for the person detection job. Use <code>JobId</code> to identify the job in\n   *     a subsequent call to <code>GetPersonTracking</code>.</p>\n   */\n  JobId?: string;\n}\n\nexport namespace StartPersonTrackingResponse {\n  export const filterSensitiveLog = (obj: StartPersonTrackingResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartProjectVersionRequest {\n  /**\n   * <p>The Amazon Resource Name(ARN) of the model version that you want to start.</p>\n   */\n  ProjectVersionArn: string | undefined;\n\n  /**\n   * <p>The minimum number of inference units to use. A single\n   *       inference unit represents 1 hour of processing and can support up to 5 Transaction Pers Second (TPS).\n   *       Use a higher number to increase the TPS throughput of your model. You are charged for the number\n   *       of inference units that you use.\n   *     </p>\n   */\n  MinInferenceUnits: number | undefined;\n}\n\nexport namespace StartProjectVersionRequest {\n  export const filterSensitiveLog = (obj: StartProjectVersionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartProjectVersionResponse {\n  /**\n   * <p>The current running status of the model. </p>\n   */\n  Status?: ProjectVersionStatus | string;\n}\n\nexport namespace StartProjectVersionResponse {\n  export const filterSensitiveLog = (obj: StartProjectVersionResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Filters for the shot detection segments returned by <code>GetSegmentDetection</code>.\n *       For more information, see <a>StartSegmentDetectionFilters</a>.</p>\n */\nexport interface StartShotDetectionFilter {\n  /**\n   * <p>Specifies the minimum confidence that Amazon Rekognition Video must have in order to return a detected segment. Confidence\n   *       represents how certain Amazon Rekognition is that a segment is correctly identified. 0 is the lowest confidence.\n   *       100 is the highest confidence.  Amazon Rekognition Video doesn't return any segments with a confidence level\n   *       lower than this specified value.</p>\n   *          <p>If you don't specify <code>MinSegmentConfidence</code>, the <code>GetSegmentDetection</code> returns\n   *         segments with confidence values greater than or equal to 50 percent.</p>\n   */\n  MinSegmentConfidence?: number;\n}\n\nexport namespace StartShotDetectionFilter {\n  export const filterSensitiveLog = (obj: StartShotDetectionFilter): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Filters for the technical segments returned by <a>GetSegmentDetection</a>. For more information,\n *       see <a>StartSegmentDetectionFilters</a>.</p>\n */\nexport interface StartTechnicalCueDetectionFilter {\n  /**\n   * <p>Specifies the minimum confidence that Amazon Rekognition Video must have in order to return a detected segment. Confidence\n   *       represents how certain Amazon Rekognition is that a segment is correctly identified. 0 is the lowest confidence.\n   *       100 is the highest confidence.  Amazon Rekognition Video doesn't return any segments with a confidence level\n   *       lower than this specified value.</p>\n   *          <p>If you don't specify <code>MinSegmentConfidence</code>, <code>GetSegmentDetection</code> returns\n   *       segments with confidence values greater than or equal to 50 percent.</p>\n   */\n  MinSegmentConfidence?: number;\n}\n\nexport namespace StartTechnicalCueDetectionFilter {\n  export const filterSensitiveLog = (obj: StartTechnicalCueDetectionFilter): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Filters applied to the technical cue or shot detection segments.\n *       For more information, see <a>StartSegmentDetection</a>.\n *     </p>\n */\nexport interface StartSegmentDetectionFilters {\n  /**\n   * <p>Filters that are specific to technical cues.</p>\n   */\n  TechnicalCueFilter?: StartTechnicalCueDetectionFilter;\n\n  /**\n   * <p>Filters that are specific to shot detections.</p>\n   */\n  ShotFilter?: StartShotDetectionFilter;\n}\n\nexport namespace StartSegmentDetectionFilters {\n  export const filterSensitiveLog = (obj: StartSegmentDetectionFilters): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartSegmentDetectionRequest {\n  /**\n   * <p>Video file stored in an Amazon S3 bucket. Amazon Rekognition video start operations such as <a>StartLabelDetection</a> use <code>Video</code> to\n   *             specify a video for analysis. The supported file formats are .mp4, .mov and .avi.</p>\n   */\n  Video: Video | undefined;\n\n  /**\n   * <p>Idempotent token used to identify the start request. If you use the same token with multiple\n   *       <code>StartSegmentDetection</code> requests, the same <code>JobId</code> is returned. Use\n   *       <code>ClientRequestToken</code> to prevent the same job from being accidently started more than once. </p>\n   */\n  ClientRequestToken?: string;\n\n  /**\n   * <p>The ARN of the Amazon SNS topic to which you want Amazon Rekognition Video to publish the completion status of the\n   *       segment detection operation.</p>\n   */\n  NotificationChannel?: NotificationChannel;\n\n  /**\n   * <p>An identifier you specify that's returned in the completion notification that's published to your Amazon Simple Notification Service topic.\n   *       For example, you can use <code>JobTag</code> to group related jobs and identify them in the completion notification.</p>\n   */\n  JobTag?: string;\n\n  /**\n   * <p>Filters for technical cue or shot detection.</p>\n   */\n  Filters?: StartSegmentDetectionFilters;\n\n  /**\n   * <p>An array of segment types to detect in the video. Valid values are TECHNICAL_CUE and SHOT.</p>\n   */\n  SegmentTypes: (SegmentType | string)[] | undefined;\n}\n\nexport namespace StartSegmentDetectionRequest {\n  export const filterSensitiveLog = (obj: StartSegmentDetectionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartSegmentDetectionResponse {\n  /**\n   * <p>Unique identifier for the segment detection job. The <code>JobId</code> is returned from <code>StartSegmentDetection</code>.\n   *     </p>\n   */\n  JobId?: string;\n}\n\nexport namespace StartSegmentDetectionResponse {\n  export const filterSensitiveLog = (obj: StartSegmentDetectionResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartStreamProcessorRequest {\n  /**\n   * <p>The name of the stream processor to start processing.</p>\n   */\n  Name: string | undefined;\n}\n\nexport namespace StartStreamProcessorRequest {\n  export const filterSensitiveLog = (obj: StartStreamProcessorRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartStreamProcessorResponse {}\n\nexport namespace StartStreamProcessorResponse {\n  export const filterSensitiveLog = (obj: StartStreamProcessorResponse): any => ({\n    ...obj,\n  });\n}\n\n/**\n * <p>Set of optional parameters that let you set the criteria text must meet to be included in your response.\n *       <code>WordFilter</code> looks at a word's height, width and minimum confidence. <code>RegionOfInterest</code>\n *       lets you set a specific region of the screen to look for text in.</p>\n */\nexport interface StartTextDetectionFilters {\n  /**\n   * <p>Filters focusing on qualities of the text, such as confidence or size.</p>\n   */\n  WordFilter?: DetectionFilter;\n\n  /**\n   * <p>Filter focusing on a certain area of the frame. Uses a <code>BoundingBox</code> object to set the region\n   *       of the screen.</p>\n   */\n  RegionsOfInterest?: RegionOfInterest[];\n}\n\nexport namespace StartTextDetectionFilters {\n  export const filterSensitiveLog = (obj: StartTextDetectionFilters): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartTextDetectionRequest {\n  /**\n   * <p>Video file stored in an Amazon S3 bucket. Amazon Rekognition video start operations such as <a>StartLabelDetection</a> use <code>Video</code> to\n   *             specify a video for analysis. The supported file formats are .mp4, .mov and .avi.</p>\n   */\n  Video: Video | undefined;\n\n  /**\n   * <p>Idempotent token used to identify the start request. If you use the same token with multiple <code>StartTextDetection</code>\n   *       requests, the same <code>JobId</code> is returned. Use <code>ClientRequestToken</code> to prevent the same job\n   *         from being accidentaly started more than once.</p>\n   */\n  ClientRequestToken?: string;\n\n  /**\n   * <p>The Amazon Simple Notification Service topic to which Amazon Rekognition publishes the completion status of a video analysis operation. For more information, see\n   *             <a>api-video</a>.</p>\n   */\n  NotificationChannel?: NotificationChannel;\n\n  /**\n   * <p>An identifier returned in the completion status published by your Amazon Simple Notification Service topic.  For example, you can use <code>JobTag</code> to group related jobs\n   *       and identify them in the completion notification.</p>\n   */\n  JobTag?: string;\n\n  /**\n   * <p>Optional parameters that let you set criteria the text must meet to be included in your response.</p>\n   */\n  Filters?: StartTextDetectionFilters;\n}\n\nexport namespace StartTextDetectionRequest {\n  export const filterSensitiveLog = (obj: StartTextDetectionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface StartTextDetectionResponse {\n  /**\n   * <p>Identifier for the text detection job.  Use <code>JobId</code> to identify the job in a subsequent call to <code>GetTextDetection</code>.</p>\n   */\n  JobId?: string;\n}\n\nexport namespace StartTextDetectionResponse {\n  export const filterSensitiveLog = (obj: StartTextDetectionResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface StopProjectVersionRequest {\n  /**\n   * <p>The Amazon Resource Name (ARN) of the model version that you want to delete.</p>\n   *          <p>This operation requires permissions to perform the <code>rekognition:StopProjectVersion</code> action.</p>\n   */\n  ProjectVersionArn: string | undefined;\n}\n\nexport namespace StopProjectVersionRequest {\n  export const filterSensitiveLog = (obj: StopProjectVersionRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface StopProjectVersionResponse {\n  /**\n   * <p>The current status of the stop operation. </p>\n   */\n  Status?: ProjectVersionStatus | string;\n}\n\nexport namespace StopProjectVersionResponse {\n  export const filterSensitiveLog = (obj: StopProjectVersionResponse): any => ({\n    ...obj,\n  });\n}\n\nexport interface StopStreamProcessorRequest {\n  /**\n   * <p>The name of a stream processor created by <a>CreateStreamProcessor</a>.</p>\n   */\n  Name: string | undefined;\n}\n\nexport namespace StopStreamProcessorRequest {\n  export const filterSensitiveLog = (obj: StopStreamProcessorRequest): any => ({\n    ...obj,\n  });\n}\n\nexport interface StopStreamProcessorResponse {}\n\nexport namespace StopStreamProcessorResponse {\n  export const filterSensitiveLog = (obj: StopStreamProcessorResponse): any => ({\n    ...obj,\n  });\n}\n"],"mappings":";AAiBA,OAAM,IAAWA,qBAAqB;AAAtC,WAAiBA,qBAAqB;EACvBA,qBAAA,CAAAC,kBAAkB,GAAG,UAACC,GAA0B;IAAU,OAAAC,QAAA,KAClED,GAAG;EAD+D,CAErE;AACJ,CAAC,EAJgBF,qBAAqB,KAArBA,qBAAqB;AAwBtC,OAAM,IAAWI,QAAQ;AAAzB,WAAiBA,QAAQ;EACVA,QAAA,CAAAH,kBAAkB,GAAG,UAACC,GAAa;IAAU,OAAAC,QAAA,KACrDD,GAAG;EADkD,CAExD;AACJ,CAAC,EAJgBE,QAAQ,KAARA,QAAQ;AAgCzB,OAAM,IAAWC,QAAQ;AAAzB,WAAiBA,QAAQ;EACVA,QAAA,CAAAJ,kBAAkB,GAAG,UAACC,GAAa;IAAU,OAAAC,QAAA,KACrDD,GAAG;EADkD,CAExD;AACJ,CAAC,EAJgBG,QAAQ,KAARA,QAAQ;AAuBzB,OAAM,IAAWC,mBAAmB;AAApC,WAAiBA,mBAAmB;EACrBA,mBAAA,CAAAL,kBAAkB,GAAG,UAACC,GAAwB;IAAU,OAAAC,QAAA,KAChED,GAAG;EAD6D,CAEnE;AACJ,CAAC,EAJgBI,mBAAmB,KAAnBA,mBAAmB;AAmBpC,OAAM,IAAWC,KAAK;AAAtB,WAAiBA,KAAK;EACPA,KAAA,CAAAN,kBAAkB,GAAG,UAACC,GAAU;IAAU,OAAAC,QAAA,KAClDD,GAAG;EAD+C,CAErD;AACJ,CAAC,EAJgBK,KAAK,KAALA,KAAK;AAMtB,WAAYC,SAGX;AAHD,WAAYA,SAAS;EACnBA,SAAA,eAAW;EACXA,SAAA,uBAAmB;AACrB,CAAC,EAHWA,SAAS,KAATA,SAAS;AA+BrB,OAAM,IAAWC,aAAa;AAA9B,WAAiBA,aAAa;EACfA,aAAA,CAAAR,kBAAkB,GAAG,UAACC,GAAkB;IAAU,OAAAC,QAAA,KAC1DD,GAAG;EADuD,CAE7D;AACJ,CAAC,EAJgBO,aAAa,KAAbA,aAAa;AAsB9B,OAAM,IAAWC,KAAK;AAAtB,WAAiBA,KAAK;EACPA,KAAA,CAAAT,kBAAkB,GAAG,UAACC,GAAU;IAAU,OAAAC,QAAA,KAClDD,GAAG;EAD+C,CAErD;AACJ,CAAC,EAJgBQ,KAAK,KAALA,KAAK;AAMtB,WAAYC,QAKX;AALD,WAAYA,QAAQ;EAClBA,QAAA,iBAAa;EACbA,QAAA,iBAAa;EACbA,QAAA,2BAAuB;EACvBA,QAAA,6BAAyB;AAC3B,CAAC,EALWA,QAAQ,KAARA,QAAQ;AAiDpB,OAAM,IAAWC,WAAW;AAA5B,WAAiBA,WAAW;EACbA,WAAA,CAAAX,kBAAkB,GAAG,UAACC,GAAgB;IAAU,OAAAC,QAAA,KACxDD,GAAG;EADqD,CAE3D;AACJ,CAAC,EAJgBU,WAAW,KAAXA,WAAW;AAsB5B,OAAM,IAAWC,cAAc;AAA/B,WAAiBA,cAAc;EAChBA,cAAA,CAAAZ,kBAAkB,GAAG,UAACC,GAAmB;IAAU,OAAAC,QAAA,KAC3DD,GAAG;EADwD,CAE9D;AACJ,CAAC,EAJgBW,cAAc,KAAdA,cAAc;AAM/B,WAAYC,uBAIX;AAJD,WAAYA,uBAAuB;EACjCA,uBAAA,6BAAyB;EACzBA,uBAAA,6BAAyB;EACzBA,uBAAA,6BAAyB;AAC3B,CAAC,EAJWA,uBAAuB,KAAvBA,uBAAuB;AAiCnC,OAAM,IAAWC,kBAAkB;AAAnC,WAAiBA,kBAAkB;EACpBA,kBAAA,CAAAd,kBAAkB,GAAG,UAACC,GAAuB;IAAU,OAAAC,QAAA,KAC/DD,GAAG;EAD4D,CAElE;AACJ,CAAC,EAJgBa,kBAAkB,KAAlBA,kBAAkB;AA6BnC,OAAM,IAAWC,2BAA2B;AAA5C,WAAiBA,2BAA2B;EAC7BA,2BAAA,CAAAf,kBAAkB,GAAG,UAACC,GAAgC;IAAU,OAAAC,QAAA,KACxED,GAAG;EADqE,CAE3E;AACJ,CAAC,EAJgBc,2BAA2B,KAA3BA,2BAA2B;AAM5C,WAAYC,YA+BX;AA/BD,WAAYA,YAAY;EACtBA,YAAA,6BAAyB;EACzBA,YAAA,uBAAmB;EACnBA,YAAA,yBAAqB;EACrBA,YAAA,uCAAmC;EACnCA,YAAA,yCAAqC;EACrCA,YAAA,mCAA+B;EAC/BA,YAAA,+BAA2B;EAC3BA,YAAA,+BAA2B;EAC3BA,YAAA,iCAA6B;EAC7BA,YAAA,2BAAuB;EACvBA,YAAA,2BAAuB;EACvBA,YAAA,qCAAiC;EACjCA,YAAA,uCAAmC;EACnCA,YAAA,2BAAuB;EACvBA,YAAA,2BAAuB;EACvBA,YAAA,6BAAyB;EACzBA,YAAA,uBAAmB;EACnBA,YAAA,iBAAa;EACbA,YAAA,yBAAqB;EACrBA,YAAA,2BAAuB;EACvBA,YAAA,yCAAqC;EACrCA,YAAA,2CAAuC;EACvCA,YAAA,qCAAiC;EACjCA,YAAA,iCAA6B;EAC7BA,YAAA,iCAA6B;EAC7BA,YAAA,mCAA+B;EAC/BA,YAAA,6BAAyB;EACzBA,YAAA,6BAAyB;EACzBA,YAAA,yCAAqC;EACrCA,YAAA,2CAAuC;AACzC,CAAC,EA/BWA,YAAY,KAAZA,YAAY;AAyDxB,OAAM,IAAWC,QAAQ;AAAzB,WAAiBA,QAAQ;EACVA,QAAA,CAAAjB,kBAAkB,GAAG,UAACC,GAAa;IAAU,OAAAC,QAAA,KACrDD,GAAG;EADkD,CAExD;AACJ,CAAC,EAJgBgB,QAAQ,KAARA,QAAQ;AA0BzB,OAAM,IAAWC,IAAI;AAArB,WAAiBA,IAAI;EACNA,IAAA,CAAAlB,kBAAkB,GAAG,UAACC,GAAS;IAAU,OAAAC,QAAA,KACjDD,GAAG;EAD8C,CAEpD;AACJ,CAAC,EAJgBiB,IAAI,KAAJA,IAAI;AAuBrB,OAAM,IAAWC,YAAY;AAA7B,WAAiBA,YAAY;EACdA,YAAA,CAAAnB,kBAAkB,GAAG,UAACC,GAAiB;IAAU,OAAAC,QAAA,KACzDD,GAAG;EADsD,CAE5D;AACJ,CAAC,EAJgBkB,YAAY,KAAZA,YAAY;AAqC7B,OAAM,IAAWC,YAAY;AAA7B,WAAiBA,YAAY;EACdA,YAAA,CAAApB,kBAAkB,GAAG,UAACC,GAAiB;IAAU,OAAAC,QAAA,KACzDD,GAAG;EADsD,CAE5D;AACJ,CAAC,EAJgBmB,YAAY,KAAZA,YAAY;AAuC7B,OAAM,IAAWC,SAAS;AAA1B,WAAiBA,SAAS;EACXA,SAAA,CAAArB,kBAAkB,GAAG,UAACC,GAAc;IAAU,OAAAC,QAAA,KACtDD,GAAG;EADmD,CAEzD;AACJ,CAAC,EAJgBoB,SAAS,KAATA,SAAS;AAmC1B,OAAM,IAAWC,OAAO;AAAxB,WAAiBA,OAAO;EACTA,OAAA,CAAAtB,kBAAkB,GAAG,UAACC,GAAY;IAAU,OAAAC,QAAA,KACpDD,GAAG;EADiD,CAEvD;AACJ,CAAC,EAJgBqB,OAAO,KAAPA,OAAO;AAsBxB,OAAM,IAAWC,UAAU;AAA3B,WAAiBA,UAAU;EACZA,UAAA,CAAAvB,kBAAkB,GAAG,UAACC,GAAe;IAAU,OAAAC,QAAA,KACvDD,GAAG;EADoD,CAE1D;AACJ,CAAC,EAJgBsB,UAAU,KAAVA,UAAU;AAsB3B,OAAM,IAAWC,OAAO;AAAxB,WAAiBA,OAAO;EACTA,OAAA,CAAAxB,kBAAkB,GAAG,UAACC,GAAY;IAAU,OAAAC,QAAA,KACpDD,GAAG;EADiD,CAEvD;AACJ,CAAC,EAJgBuB,OAAO,KAAPA,OAAO;AAMxB,WAAYC,UAGX;AAHD,WAAYA,UAAU;EACpBA,UAAA,qBAAiB;EACjBA,UAAA,iBAAa;AACf,CAAC,EAHWA,UAAU,KAAVA,UAAU;AAiCtB,OAAM,IAAWC,MAAM;AAAvB,WAAiBA,MAAM;EACRA,MAAA,CAAA1B,kBAAkB,GAAG,UAACC,GAAW;IAAU,OAAAC,QAAA,KACnDD,GAAG;EADgD,CAEtD;AACJ,CAAC,EAJgByB,MAAM,KAANA,MAAM;AAsBvB,OAAM,IAAWC,SAAS;AAA1B,WAAiBA,SAAS;EACXA,SAAA,CAAA3B,kBAAkB,GAAG,UAACC,GAAc;IAAU,OAAAC,QAAA,KACtDD,GAAG;EADmD,CAEzD;AACJ,CAAC,EAJgB0B,SAAS,KAATA,SAAS;AAsB1B,OAAM,IAAWC,QAAQ;AAAzB,WAAiBA,QAAQ;EACVA,QAAA,CAAA5B,kBAAkB,GAAG,UAACC,GAAa;IAAU,OAAAC,QAAA,KACrDD,GAAG;EADkD,CAExD;AACJ,CAAC,EAJgB2B,QAAQ,KAARA,QAAQ;AAsBzB,OAAM,IAAWC,KAAK;AAAtB,WAAiBA,KAAK;EACPA,KAAA,CAAA7B,kBAAkB,GAAG,UAACC,GAAU;IAAU,OAAAC,QAAA,KAClDD,GAAG;EAD+C,CAErD;AACJ,CAAC,EAJgB4B,KAAK,KAALA,KAAK;AAsBtB,OAAM,IAAWC,UAAU;AAA3B,WAAiBA,UAAU;EACZA,UAAA,CAAA9B,kBAAkB,GAAG,UAACC,GAAe;IAAU,OAAAC,QAAA,KACvDD,GAAG;EADoD,CAE1D;AACJ,CAAC,EAJgB6B,UAAU,KAAVA,UAAU;AAyH3B,OAAM,IAAWC,UAAU;AAA3B,WAAiBA,UAAU;EACZA,UAAA,CAAA/B,kBAAkB,GAAG,UAACC,GAAe;IAAU,OAAAC,QAAA,KACvDD,GAAG;EADoD,CAE1D;AACJ,CAAC,EAJgB8B,UAAU,KAAVA,UAAU;AAyC3B,OAAM,IAAWC,eAAe;AAAhC,WAAiBA,eAAe;EACjBA,eAAA,CAAAhC,kBAAkB,GAAG,UAACC,GAAoB;IAAU,OAAAC,QAAA,KAC5DD,GAAG;EADyD,CAE/D;AACJ,CAAC,EAJgB+B,eAAe,KAAfA,eAAe;AAsBhC,OAAM,IAAWC,oBAAoB;AAArC,WAAiBA,oBAAoB;EACtBA,oBAAA,CAAAjC,kBAAkB,GAAG,UAACC,GAAyB;IAAU,OAAAC,QAAA,KACjED,GAAG;EAD8D,CAEpE;AACJ,CAAC,EAJgBgC,oBAAoB,KAApBA,oBAAoB;AAMrC,WAAYC,0BAGX;AAHD,WAAYA,0BAA0B;EACpCA,0BAAA,aAAS;EACTA,0BAAA,2BAAuB;AACzB,CAAC,EAHWA,0BAA0B,KAA1BA,0BAA0B;AAuBtC,OAAM,IAAWC,uBAAuB;AAAxC,WAAiBA,uBAAuB;EACzBA,uBAAA,CAAAnC,kBAAkB,GAAG,UAACC,GAA4B;IAAU,OAAAC,QAAA,KACpED,GAAG;EADiE,CAEvE;AACJ,CAAC,EAJgBkC,uBAAuB,KAAvBA,uBAAuB;AAMxC,WAAYC,aAMX;AAND,WAAYA,aAAa;EACvBA,aAAA,iBAAa;EACbA,aAAA,iBAAa;EACbA,aAAA,eAAW;EACXA,aAAA,qBAAiB;EACjBA,aAAA,iBAAa;AACf,CAAC,EANWA,aAAa,KAAbA,aAAa;AA6CzB,OAAM,IAAWC,KAAK;AAAtB,WAAiBA,KAAK;EACPA,KAAA,CAAArC,kBAAkB,GAAG,UAACC,GAAU;IAAU,OAAAC,QAAA,KAClDD,GAAG;EAD+C,CAErD;AACJ,CAAC,EAJgBoC,KAAK,KAALA,KAAK;AAmDtB,OAAM,IAAWC,mBAAmB;AAApC,WAAiBA,mBAAmB;EACrBA,mBAAA,CAAAtC,kBAAkB,GAAG,UAACC,GAAwB;IAAU,OAAAC,QAAA,KAChED,GAAG;EAD6D,CAEnE;AACJ,CAAC,EAJgBqC,mBAAmB,KAAnBA,mBAAmB;AAyBpC,OAAM,IAAWC,iBAAiB;AAAlC,WAAiBA,iBAAiB;EACnBA,iBAAA,CAAAvC,kBAAkB,GAAG,UAACC,GAAsB;IAAU,OAAAC,QAAA,KAC9DD,GAAG;EAD2D,CAEjE;AACJ,CAAC,EAJgBsC,iBAAiB,KAAjBA,iBAAiB;AAMlC,WAAYC,qBAKX;AALD,WAAYA,qBAAqB;EAC/BA,qBAAA,yBAAqB;EACrBA,qBAAA,6BAAyB;EACzBA,qBAAA,6BAAyB;EACzBA,qBAAA,2BAAuB;AACzB,CAAC,EALWA,qBAAqB,KAArBA,qBAAqB;AAwDjC,OAAM,IAAWC,oBAAoB;AAArC,WAAiBA,oBAAoB;EACtBA,oBAAA,CAAAzC,kBAAkB,GAAG,UAACC,GAAyB;IAAU,OAAAC,QAAA,KACjED,GAAG;EAD8D,CAEpE;AACJ,CAAC,EAJgBwC,oBAAoB,KAApBA,oBAAoB;AAqBrC,OAAM,IAAWC,sBAAsB;AAAvC,WAAiBA,sBAAsB;EACxBA,sBAAA,CAAA1C,kBAAkB,GAAG,UAACC,GAA2B;IAAU,OAAAC,QAAA,KACnED,GAAG;EADgE,CAEtE;AACJ,CAAC,EAJgByC,sBAAsB,KAAtBA,sBAAsB;AAoBvC,OAAM,IAAWC,mBAAmB;AAApC,WAAiBA,mBAAmB;EACrBA,mBAAA,CAAA3C,kBAAkB,GAAG,UAACC,GAAwB;IAAU,OAAAC,QAAA,KAChED,GAAG;EAD6D,CAEnE;AACJ,CAAC,EAJgB0C,mBAAmB,KAAnBA,mBAAmB;AAoBpC,OAAM,IAAWC,2BAA2B;AAA5C,WAAiBA,2BAA2B;EAC7BA,2BAAA,CAAA5C,kBAAkB,GAAG,UAACC,GAAgC;IAAU,OAAAC,QAAA,KACxED,GAAG;EADqE,CAE3E;AACJ,CAAC,EAJgB2C,2BAA2B,KAA3BA,2BAA2B;AAqB5C,OAAM,IAAWC,yBAAyB;AAA1C,WAAiBA,yBAAyB;EAC3BA,yBAAA,CAAA7C,kBAAkB,GAAG,UAACC,GAA8B;IAAU,OAAAC,QAAA,KACtED,GAAG;EADmE,CAEzE;AACJ,CAAC,EAJgB4C,yBAAyB,KAAzBA,yBAAyB;AAoB1C,OAAM,IAAWC,wBAAwB;AAAzC,WAAiBA,wBAAwB;EAC1BA,wBAAA,CAAA9C,kBAAkB,GAAG,UAACC,GAA6B;IAAU,OAAAC,QAAA,KACrED,GAAG;EADkE,CAExE;AACJ,CAAC,EAJgB6C,wBAAwB,KAAxBA,wBAAwB;AAqBzC,OAAM,IAAWC,sCAAsC;AAAvD,WAAiBA,sCAAsC;EACxCA,sCAAA,CAAA/C,kBAAkB,GAAG,UAACC,GAA2C;IAAU,OAAAC,QAAA,KACnFD,GAAG;EADgF,CAEtF;AACJ,CAAC,EAJgB8C,sCAAsC,KAAtCA,sCAAsC;AAoBvD,OAAM,IAAWC,mBAAmB;AAApC,WAAiBA,mBAAmB;EACrBA,mBAAA,CAAAhD,kBAAkB,GAAG,UAACC,GAAwB;IAAU,OAAAC,QAAA,KAChED,GAAG;EAD6D,CAEnE;AACJ,CAAC,EAJgB+C,mBAAmB,KAAnBA,mBAAmB;AAMpC,WAAYC,iBAGX;AAHD,WAAYA,iBAAiB;EAC3BA,iBAAA,gDAA4C;EAC5CA,iBAAA,2FAAuF;AACzF,CAAC,EAHWA,iBAAiB,KAAjBA,iBAAiB;AAgC7B,OAAM,IAAWC,eAAe;AAAhC,WAAiBA,eAAe;EACjBA,eAAA,CAAAlD,kBAAkB,GAAG,UAACC,GAAoB;IAAU,OAAAC,QAAA,KAC5DD,GAAG;EADyD,CAE/D;AACJ,CAAC,EAJgBiD,eAAe,KAAfA,eAAe;AAqBhC,OAAM,IAAWC,0BAA0B;AAA3C,WAAiBA,0BAA0B;EAC5BA,0BAAA,CAAAnD,kBAAkB,GAAG,UAACC,GAA+B;IAAU,OAAAC,QAAA,KACvED,GAAG;EADoE,CAE1E;AACJ,CAAC,EAJgBkD,0BAA0B,KAA1BA,0BAA0B;AAM3C,WAAYC,uBAGX;AAHD,WAAYA,uBAAuB;EACjCA,uBAAA,iBAAa;EACbA,uBAAA,2BAAuB;AACzB,CAAC,EAHWA,uBAAuB,KAAvBA,uBAAuB;AAYnC,OAAM,IAAWC,uBAAuB;AAAxC,WAAiBA,uBAAuB;EACzBA,uBAAA,CAAArD,kBAAkB,GAAG,UAACC,GAA4B;IAAU,OAAAC,QAAA,KACpED,GAAG;EADiE,CAEvE;AACJ,CAAC,EAJgBoD,uBAAuB,KAAvBA,uBAAuB;AAwBxC,OAAM,IAAWC,wBAAwB;AAAzC,WAAiBA,wBAAwB;EAC1BA,wBAAA,CAAAtD,kBAAkB,GAAG,UAACC,GAA6B;IAAU,OAAAC,QAAA,KACrED,GAAG;EADkE,CAExE;AACJ,CAAC,EAJgBqD,wBAAwB,KAAxBA,wBAAwB;AAoBzC,OAAM,IAAWC,8BAA8B;AAA/C,WAAiBA,8BAA8B;EAChCA,8BAAA,CAAAvD,kBAAkB,GAAG,UAACC,GAAmC;IAAU,OAAAC,QAAA,KAC3ED,GAAG;EADwE,CAE9E;AACJ,CAAC,EAJgBsD,8BAA8B,KAA9BA,8BAA8B;AAa/C,OAAM,IAAWC,oBAAoB;AAArC,WAAiBA,oBAAoB;EACtBA,oBAAA,CAAAxD,kBAAkB,GAAG,UAACC,GAAyB;IAAU,OAAAC,QAAA,KACjED,GAAG;EAD8D,CAEpE;AACJ,CAAC,EAJgBuD,oBAAoB,KAApBA,oBAAoB;AAcrC,OAAM,IAAWC,qBAAqB;AAAtC,WAAiBA,qBAAqB;EACvBA,qBAAA,CAAAzD,kBAAkB,GAAG,UAACC,GAA0B;IAAU,OAAAC,QAAA,KAClED,GAAG;EAD+D,CAErE;AACJ,CAAC,EAJgBwD,qBAAqB,KAArBA,qBAAqB;AAsBtC,OAAM,IAAWC,sBAAsB;AAAvC,WAAiBA,sBAAsB;EACxBA,sBAAA,CAAA1D,kBAAkB,GAAG,UAACC,GAA2B;IAAU,OAAAC,QAAA,KACnED,GAAG;EADgE,CAEtE;AACJ,CAAC,EAJgByD,sBAAsB,KAAtBA,sBAAsB;AAoBvC,OAAM,IAAWC,sBAAsB;AAAvC,WAAiBA,sBAAsB;EACxBA,sBAAA,CAAA3D,kBAAkB,GAAG,UAACC,GAA2B;IAAU,OAAAC,QAAA,KACnED,GAAG;EADgE,CAEtE;AACJ,CAAC,EAJgB0D,sBAAsB,KAAtBA,sBAAsB;AAqBvC,OAAM,IAAWC,YAAY;AAA7B,WAAiBA,YAAY;EACdA,YAAA,CAAA5D,kBAAkB,GAAG,UAACC,GAAiB;IAAU,OAAAC,QAAA,KACzDD,GAAG;EADsD,CAE5D;AACJ,CAAC,EAJgB2D,YAAY,KAAZA,YAAY;AAsB7B,OAAM,IAAWC,WAAW;AAA5B,WAAiBA,WAAW;EACbA,WAAA,CAAA7D,kBAAkB,GAAG,UAACC,GAAgB;IAAU,OAAAC,QAAA,KACxDD,GAAG;EADqD,CAE3D;AACJ,CAAC,EAJgB4D,WAAW,KAAXA,WAAW;AAgB5B,OAAM,IAAWC,YAAY;AAA7B,WAAiBA,YAAY;EACdA,YAAA,CAAA9D,kBAAkB,GAAG,UAACC,GAAiB;IAAU,OAAAC,QAAA,KACzDD,GAAG;EADsD,CAE5D;AACJ,CAAC,EAJgB6D,YAAY,KAAZA,YAAY;AAkC7B,OAAM,IAAWC,2BAA2B;AAA5C,WAAiBA,2BAA2B;EAC7BA,2BAAA,CAAA/D,kBAAkB,GAAG,UAACC,GAAgC;IAAU,OAAAC,QAAA,KACxED,GAAG;EADqE,CAE3E;AACJ,CAAC,EAJgB8D,2BAA2B,KAA3BA,2BAA2B;AAc5C,OAAM,IAAWC,4BAA4B;AAA7C,WAAiBA,4BAA4B;EAC9BA,4BAAA,CAAAhE,kBAAkB,GAAG,UAACC,GAAiC;IAAU,OAAAC,QAAA,KACzED,GAAG;EADsE,CAE5E;AACJ,CAAC,EAJgB+D,4BAA4B,KAA5BA,4BAA4B;AAoB7C,OAAM,IAAWC,yBAAyB;AAA1C,WAAiBA,yBAAyB;EAC3BA,yBAAA,CAAAjE,kBAAkB,GAAG,UAACC,GAA8B;IAAU,OAAAC,QAAA,KACtED,GAAG;EADmE,CAEzE;AACJ,CAAC,EAJgBgE,yBAAyB,KAAzBA,yBAAyB;AAiB1C,OAAM,IAAWC,kBAAkB;AAAnC,WAAiBA,kBAAkB;EACpBA,kBAAA,CAAAlE,kBAAkB,GAAG,UAACC,GAAuB;IAAU,OAAAC,QAAA,KAC/DD,GAAG;EAD4D,CAElE;AACJ,CAAC,EAJgBiE,kBAAkB,KAAlBA,kBAAkB;AAgBnC,OAAM,IAAWC,oBAAoB;AAArC,WAAiBA,oBAAoB;EACtBA,oBAAA,CAAAnE,kBAAkB,GAAG,UAACC,GAAyB;IAAU,OAAAC,QAAA,KACjED,GAAG;EAD8D,CAEpE;AACJ,CAAC,EAJgBkE,oBAAoB,KAApBA,oBAAoB;AAiBrC,OAAM,IAAWC,iBAAiB;AAAlC,WAAiBA,iBAAiB;EACnBA,iBAAA,CAAApE,kBAAkB,GAAG,UAACC,GAAsB;IAAU,OAAAC,QAAA,KAC9DD,GAAG;EAD2D,CAEjE;AACJ,CAAC,EAJgBmE,iBAAiB,KAAjBA,iBAAiB;AAiBlC,OAAM,IAAWC,qBAAqB;AAAtC,WAAiBA,qBAAqB;EACvBA,qBAAA,CAAArE,kBAAkB,GAAG,UAACC,GAA0B;IAAU,OAAAC,QAAA,KAClED,GAAG;EAD+D,CAErE;AACJ,CAAC,EAJgBoE,qBAAqB,KAArBA,qBAAqB;AAuBtC,OAAM,IAAWC,kBAAkB;AAAnC,WAAiBA,kBAAkB;EACpBA,kBAAA,CAAAtE,kBAAkB,GAAG,UAACC,GAAuB;IAAU,OAAAC,QAAA,KAC/DD,GAAG;EAD4D,CAElE;AACJ,CAAC,EAJgBqE,kBAAkB,KAAlBA,kBAAkB;AAgBnC,OAAM,IAAWC,uBAAuB;AAAxC,WAAiBA,uBAAuB;EACzBA,uBAAA,CAAAvE,kBAAkB,GAAG,UAACC,GAA4B;IAAU,OAAAC,QAAA,KACpED,GAAG;EADiE,CAEvE;AACJ,CAAC,EAJgBsE,uBAAuB,KAAvBA,uBAAuB;AAqCxC,OAAM,IAAWC,4BAA4B;AAA7C,WAAiBA,4BAA4B;EAC9BA,4BAAA,CAAAxE,kBAAkB,GAAG,UAACC,GAAiC;IAAU,OAAAC,QAAA,KACzED,GAAG;EADsE,CAE5E;AACJ,CAAC,EAJgBuE,4BAA4B,KAA5BA,4BAA4B;AAa7C,OAAM,IAAWC,6BAA6B;AAA9C,WAAiBA,6BAA6B;EAC/BA,6BAAA,CAAAzE,kBAAkB,GAAG,UAACC,GAAkC;IAAU,OAAAC,QAAA,KAC1ED,GAAG;EADuE,CAE7E;AACJ,CAAC,EAJgBwE,6BAA6B,KAA7BA,6BAA6B;AA4B9C,OAAM,IAAWC,KAAK;AAAtB,WAAiBA,KAAK;EACPA,KAAA,CAAA1E,kBAAkB,GAAG,UAACC,GAAU;IAAU,OAAAC,QAAA,KAClDD,GAAG;EAD+C,CAErD;AACJ,CAAC,EAJgByE,KAAK,KAALA,KAAK;AAuBtB,OAAM,IAAWC,QAAQ;AAAzB,WAAiBA,QAAQ;EACVA,QAAA,CAAA3E,kBAAkB,GAAG,UAACC,GAAa;IAAU,OAAAC,QAAA,KACrDD,GAAG;EADkD,CAExD;AACJ,CAAC,EAJgB0E,QAAQ,KAARA,QAAQ;AA6BzB,OAAM,IAAWC,WAAW;AAA5B,WAAiBA,WAAW;EACbA,WAAA,CAAA5E,kBAAkB,GAAG,UAACC,GAAgB;IAAU,OAAAC,QAAA,KACxDD,GAAG;EADqD,CAE3D;AACJ,CAAC,EAJgB2E,WAAW,KAAXA,WAAW;AAa5B,OAAM,IAAWC,uBAAuB;AAAxC,WAAiBA,uBAAuB;EACzBA,uBAAA,CAAA7E,kBAAkB,GAAG,UAACC,GAA4B;IAAU,OAAAC,QAAA,KACpED,GAAG;EADiE,CAEvE;AACJ,CAAC,EAJgB4E,uBAAuB,KAAvBA,uBAAuB;AAaxC,OAAM,IAAWC,wBAAwB;AAAzC,WAAiBA,wBAAwB;EAC1BA,wBAAA,CAAA9E,kBAAkB,GAAG,UAACC,GAA6B;IAAU,OAAAC,QAAA,KACrED,GAAG;EADkE,CAExE;AACJ,CAAC,EAJgB6E,wBAAwB,KAAxBA,wBAAwB;AAkBzC,OAAM,IAAWC,kBAAkB;AAAnC,WAAiBA,kBAAkB;EACpBA,kBAAA,CAAA/E,kBAAkB,GAAG,UAACC,GAAuB;IAAU,OAAAC,QAAA,KAC/DD,GAAG;EAD4D,CAElE;AACJ,CAAC,EAJgB8E,kBAAkB,KAAlBA,kBAAkB;AAanC,OAAM,IAAWC,mBAAmB;AAApC,WAAiBA,mBAAmB;EACrBA,mBAAA,CAAAhF,kBAAkB,GAAG,UAACC,GAAwB;IAAU,OAAAC,QAAA,KAChED,GAAG;EAD6D,CAEnE;AACJ,CAAC,EAJgB+E,mBAAmB,KAAnBA,mBAAmB;AAapC,OAAM,IAAWC,oBAAoB;AAArC,WAAiBA,oBAAoB;EACtBA,oBAAA,CAAAjF,kBAAkB,GAAG,UAACC,GAAyB;IAAU,OAAAC,QAAA,KACjED,GAAG;EAD8D,CAEpE;AACJ,CAAC,EAJgBgF,oBAAoB,KAApBA,oBAAoB;AAMrC,WAAYC,aAIX;AAJD,WAAYA,aAAa;EACvBA,aAAA,uBAAmB;EACnBA,aAAA,yBAAqB;EACrBA,aAAA,yBAAqB;AACvB,CAAC,EAJWA,aAAa,KAAbA,aAAa;AAazB,OAAM,IAAWC,qBAAqB;AAAtC,WAAiBA,qBAAqB;EACvBA,qBAAA,CAAAnF,kBAAkB,GAAG,UAACC,GAA0B;IAAU,OAAAC,QAAA,KAClED,GAAG;EAD+D,CAErE;AACJ,CAAC,EAJgBkF,qBAAqB,KAArBA,qBAAqB;AAatC,OAAM,IAAWC,2BAA2B;AAA5C,WAAiBA,2BAA2B;EAC7BA,2BAAA,CAAApF,kBAAkB,GAAG,UAACC,GAAgC;IAAU,OAAAC,QAAA,KACxED,GAAG;EADqE,CAE3E;AACJ,CAAC,EAJgBmF,2BAA2B,KAA3BA,2BAA2B;AAM5C,WAAYC,oBAUX;AAVD,WAAYA,oBAAoB;EAC9BA,oBAAA,yBAAqB;EACrBA,oBAAA,qBAAiB;EACjBA,oBAAA,uBAAmB;EACnBA,oBAAA,yBAAqB;EACrBA,oBAAA,uBAAmB;EACnBA,oBAAA,yBAAqB;EACrBA,oBAAA,6CAAyC;EACzCA,oBAAA,uCAAmC;EACnCA,oBAAA,iDAA6C;AAC/C,CAAC,EAVWA,oBAAoB,KAApBA,oBAAoB;AAmBhC,OAAM,IAAWC,4BAA4B;AAA7C,WAAiBA,4BAA4B;EAC9BA,4BAAA,CAAAtF,kBAAkB,GAAG,UAACC,GAAiC;IAAU,OAAAC,QAAA,KACzED,GAAG;EADsE,CAE5E;AACJ,CAAC,EAJgBqF,4BAA4B,KAA5BA,4BAA4B;AAa7C,OAAM,IAAWC,4BAA4B;AAA7C,WAAiBA,4BAA4B;EAC9BA,4BAAA,CAAAvF,kBAAkB,GAAG,UAACC,GAAiC;IAAU,OAAAC,QAAA,KACzED,GAAG;EADsE,CAE5E;AACJ,CAAC,EAJgBsF,4BAA4B,KAA5BA,4BAA4B;AAQ7C,OAAM,IAAWC,6BAA6B;AAA9C,WAAiBA,6BAA6B;EAC/BA,6BAAA,CAAAxF,kBAAkB,GAAG,UAACC,GAAkC;IAAU,OAAAC,QAAA,KAC1ED,GAAG;EADuE,CAE7E;AACJ,CAAC,EAJgBuF,6BAA6B,KAA7BA,6BAA6B;AAa9C,OAAM,IAAWC,yBAAyB;AAA1C,WAAiBA,yBAAyB;EAC3BA,yBAAA,CAAAzF,kBAAkB,GAAG,UAACC,GAA8B;IAAU,OAAAC,QAAA,KACtED,GAAG;EADmE,CAEzE;AACJ,CAAC,EAJgBwF,yBAAyB,KAAzBA,yBAAyB;AAiC1C,OAAM,IAAWC,0BAA0B;AAA3C,WAAiBA,0BAA0B;EAC5BA,0BAAA,CAAA1F,kBAAkB,GAAG,UAACC,GAA+B;IAAU,OAAAC,QAAA,KACvED,GAAG;EADoE,CAE1E;AACJ,CAAC,EAJgByF,0BAA0B,KAA1BA,0BAA0B;AAsB3C,OAAM,IAAWC,uBAAuB;AAAxC,WAAiBA,uBAAuB;EACzBA,uBAAA,CAAA3F,kBAAkB,GAAG,UAACC,GAA4B;IAAU,OAAAC,QAAA,KACpED,GAAG;EADiE,CAEvE;AACJ,CAAC,EAJgB0F,uBAAuB,KAAvBA,uBAAuB;AA0BxC,OAAM,IAAWC,kBAAkB;AAAnC,WAAiBA,kBAAkB;EACpBA,kBAAA,CAAA5F,kBAAkB,GAAG,UAACC,GAAuB;IAAU,OAAAC,QAAA,KAC/DD,GAAG;EAD4D,CAElE;AACJ,CAAC,EAJgB2F,kBAAkB,KAAlBA,kBAAkB;AAoBnC,OAAM,IAAWC,wBAAwB;AAAzC,WAAiBA,wBAAwB;EAC1BA,wBAAA,CAAA7F,kBAAkB,GAAG,UAACC,GAA6B;IAAU,OAAAC,QAAA,KACrED,GAAG;EADkE,CAExE;AACJ,CAAC,EAJgB4F,wBAAwB,KAAxBA,wBAAwB;AAoBzC,OAAM,IAAWC,+BAA+B;AAAhD,WAAiBA,+BAA+B;EACjCA,+BAAA,CAAA9F,kBAAkB,GAAG,UAACC,GAAoC;IAAU,OAAAC,QAAA,KAC5ED,GAAG;EADyE,CAE/E;AACJ,CAAC,EAJgB6F,+BAA+B,KAA/BA,+BAA+B;AAmChD,OAAM,IAAWC,8BAA8B;AAA/C,WAAiBA,8BAA8B;EAChCA,8BAAA,CAAA/F,kBAAkB,GAAG,UAACC,GAAmC;IAAU,OAAAC,QAAA,KAC3ED,GAAG;EADwE,CAE9E;AACJ,CAAC,EAJgB8F,8BAA8B,KAA9BA,8BAA8B;AA0B/C,OAAM,IAAWC,OAAO;AAAxB,WAAiBA,OAAO;EACTA,OAAA,CAAAhG,kBAAkB,GAAG,UAACC,GAAY;IAAU,OAAAC,QAAA,KACpDD,GAAG;EADiD,CAEvD;AACJ,CAAC,EAJgB+F,OAAO,KAAPA,OAAO;AAyBxB,OAAM,IAAWC,gBAAgB;AAAjC,WAAiBA,gBAAgB;EAClBA,gBAAA,CAAAjG,kBAAkB,GAAG,UAACC,GAAqB;IAAU,OAAAC,QAAA,KAC7DD,GAAG;EAD0D,CAEhE;AACJ,CAAC,EAJgBgG,gBAAgB,KAAhBA,gBAAgB;AA2BjC,OAAM,IAAWC,cAAc;AAA/B,WAAiBA,cAAc;EAChBA,cAAA,CAAAlG,kBAAkB,GAAG,UAACC,GAAmB;IAAU,OAAAC,QAAA,KAC3DD,GAAG;EADwD,CAE9D;AACJ,CAAC,EAJgBiG,cAAc,KAAdA,cAAc;AA2B/B,OAAM,IAAWC,iBAAiB;AAAlC,WAAiBA,iBAAiB;EACnBA,iBAAA,CAAAnG,kBAAkB,GAAG,UAACC,GAAsB;IAAU,OAAAC,QAAA,KAC9DD,GAAG;EAD2D,CAEjE;AACJ,CAAC,EAJgBkG,iBAAiB,KAAjBA,iBAAiB;AA0BlC,OAAM,IAAWC,kBAAkB;AAAnC,WAAiBA,kBAAkB;EACpBA,kBAAA,CAAApG,kBAAkB,GAAG,UAACC,GAAuB;IAAU,OAAAC,QAAA,KAC/DD,GAAG;EAD4D,CAElE;AACJ,CAAC,EAJgBmG,kBAAkB,KAAlBA,kBAAkB;AA0EnC,OAAM,IAAWC,yBAAyB;AAA1C,WAAiBA,yBAAyB;EAC3BA,yBAAA,CAAArG,kBAAkB,GAAG,UAACC,GAA8B;IAAU,OAAAC,QAAA,KACtED,GAAG;EADmE,CAEzE;AACJ,CAAC,EAJgBoG,yBAAyB,KAAzBA,yBAAyB;AAqB1C,OAAM,IAAWC,+BAA+B;AAAhD,WAAiBA,+BAA+B;EACjCA,+BAAA,CAAAtG,kBAAkB,GAAG,UAACC,GAAoC;IAAU,OAAAC,QAAA,KAC5ED,GAAG;EADyE,CAE/E;AACJ,CAAC,EAJgBqG,+BAA+B,KAA/BA,+BAA+B;AAahD,OAAM,IAAWC,8BAA8B;AAA/C,WAAiBA,8BAA8B;EAChCA,8BAAA,CAAAvG,kBAAkB,GAAG,UAACC,GAAmC;IAAU,OAAAC,QAAA,KAC3ED,GAAG;EADwE,CAE9E;AACJ,CAAC,EAJgBsG,8BAA8B,KAA9BA,8BAA8B;AAM/C,WAAYC,qBAMX;AAND,WAAYA,qBAAqB;EAC/BA,qBAAA,qBAAiB;EACjBA,qBAAA,uBAAmB;EACnBA,qBAAA,yBAAqB;EACrBA,qBAAA,uBAAmB;EACnBA,qBAAA,yBAAqB;AACvB,CAAC,EANWA,qBAAqB,KAArBA,qBAAqB;AA+DjC,OAAM,IAAWC,+BAA+B;AAAhD,WAAiBA,+BAA+B;EACjCA,+BAAA,CAAAzG,kBAAkB,GAAG,UAACC,GAAoC;IAAU,OAAAC,QAAA,KAC5ED,GAAG;EADyE,CAE/E;AACJ,CAAC,EAJgBwG,+BAA+B,KAA/BA,+BAA+B;AAsDhD,OAAM,IAAWC,yBAAyB;AAA1C,WAAiBA,yBAAyB;EAC3BA,yBAAA,CAAA1G,kBAAkB,GAAG,UAACC,GAA8B;IAAU,OAAAC,QAAA,KACtED,GAAG;EADmE,CAEzE;AACJ,CAAC,EAJgByG,yBAAyB,KAAzBA,yBAAyB;AAa1C,OAAM,IAAWC,0BAA0B;AAA3C,WAAiBA,0BAA0B;EAC5BA,0BAAA,CAAA3G,kBAAkB,GAAG,UAACC,GAA+B;IAAU,OAAAC,QAAA,KACvED,GAAG;EADoE,CAE1E;AACJ,CAAC,EAJgB0G,0BAA0B,KAA1BA,0BAA0B;AAsB3C,OAAM,IAAWC,yBAAyB;AAA1C,WAAiBA,yBAAyB;EAC3BA,yBAAA,CAAA5G,kBAAkB,GAAG,UAACC,GAA8B;IAAU,OAAAC,QAAA,KACtED,GAAG;EADmE,CAEzE;AACJ,CAAC,EAJgB2G,yBAAyB,KAAzBA,yBAAyB;AA6B1C,OAAM,IAAWC,kBAAkB;AAAnC,WAAiBA,kBAAkB;EACpBA,kBAAA,CAAA7G,kBAAkB,GAAG,UAACC,GAAuB;IAAU,OAAAC,QAAA,KAC/DD,GAAG;EAD4D,CAElE;AACJ,CAAC,EAJgB4G,kBAAkB,KAAlBA,kBAAkB;AA2BnC,OAAM,IAAWC,mBAAmB;AAApC,WAAiBA,mBAAmB;EACrBA,mBAAA,CAAA9G,kBAAkB,GAAG,UAACC,GAAwB;IAAU,OAAAC,QAAA,KAChED,GAAG;EAD6D,CAEnE;AACJ,CAAC,EAJgB6G,mBAAmB,KAAnBA,mBAAmB;AA8BpC,OAAM,IAAWC,eAAe;AAAhC,WAAiBA,eAAe;EACjBA,eAAA,CAAA/G,kBAAkB,GAAG,UAACC,GAAoB;IAAU,OAAAC,QAAA,KAC5DD,GAAG;EADyD,CAE/D;AACJ,CAAC,EAJgB8G,eAAe,KAAfA,eAAe;AAgChC,OAAM,IAAWC,mBAAmB;AAApC,WAAiBA,mBAAmB;EACrBA,mBAAA,CAAAhH,kBAAkB,GAAG,UAACC,GAAwB;IAAU,OAAAC,QAAA,KAChED,GAAG;EAD6D,CAEnE;AACJ,CAAC,EAJgB+G,mBAAmB,KAAnBA,mBAAmB;AAsBpC,OAAM,IAAWC,QAAQ;AAAzB,WAAiBA,QAAQ;EACVA,QAAA,CAAAjH,kBAAkB,GAAG,UAACC,GAAa;IAAU,OAAAC,QAAA,KACrDD,GAAG;EADkD,CAExD;AACJ,CAAC,EAJgBgH,QAAQ,KAARA,QAAQ;AAgBzB,OAAM,IAAWC,MAAM;AAAvB,WAAiBA,MAAM;EACRA,MAAA,CAAAlH,kBAAkB,GAAG,UAACC,GAAW;IAAU,OAAAC,QAAA,KACnDD,GAAG;EADgD,CAEtD;AACJ,CAAC,EAJgBiH,MAAM,KAANA,MAAM;AAmCvB,OAAM,IAAWC,KAAK;AAAtB,WAAiBA,KAAK;EACPA,KAAA,CAAAnH,kBAAkB,GAAG,UAACC,GAAU;IAAU,OAAAC,QAAA,KAClDD,GAAG;EAD+C,CAErD;AACJ,CAAC,EAJgBkH,KAAK,KAALA,KAAK;AAgCtB,OAAM,IAAWC,oBAAoB;AAArC,WAAiBA,oBAAoB;EACtBA,oBAAA,CAAApH,kBAAkB,GAAG,UAACC,GAAyB;IAAU,OAAAC,QAAA,KACjED,GAAG;EAD8D,CAEpE;AACJ,CAAC,EAJgBmH,oBAAoB,KAApBA,oBAAoB;AAiBrC,OAAM,IAAWC,uBAAuB;AAAxC,WAAiBA,uBAAuB;EACzBA,uBAAA,CAAArH,kBAAkB,GAAG,UAACC,GAA4B;IAAU,OAAAC,QAAA,KACpED,GAAG;EADiE,CAEvE;AACJ,CAAC,EAJgBoH,uBAAuB,KAAvBA,uBAAuB;AA6BxC,OAAM,IAAWC,eAAe;AAAhC,WAAiBA,eAAe;EACjBA,eAAA,CAAAtH,kBAAkB,GAAG,UAACC,GAAoB;IAAU,OAAAC,QAAA,KAC5DD,GAAG;EADyD,CAE/D;AACJ,CAAC,EAJgBqH,eAAe,KAAfA,eAAe;AAgChC,OAAM,IAAWC,6BAA6B;AAA9C,WAAiBA,6BAA6B;EAC/BA,6BAAA,CAAAvH,kBAAkB,GAAG,UAACC,GAAkC;IAAU,OAAAC,QAAA,KAC1ED,GAAG;EADuE,CAE7E;AACJ,CAAC,EAJgBsH,6BAA6B,KAA7BA,6BAA6B;AA4B9C,OAAM,IAAWC,yBAAyB;AAA1C,WAAiBA,yBAAyB;EAC3BA,yBAAA,CAAAxH,kBAAkB,GAAG,UAACC,GAA8B;IAAU,OAAAC,QAAA,KACtED,GAAG;EADmE,CAEzE;AACJ,CAAC,EAJgBuH,yBAAyB,KAAzBA,yBAAyB;AAwB1C,OAAM,IAAWC,8BAA8B;AAA/C,WAAiBA,8BAA8B;EAChCA,8BAAA,CAAAzH,kBAAkB,GAAG,UAACC,GAAmC;IAAU,OAAAC,QAAA,KAC3ED,GAAG;EADwE,CAE9E;AACJ,CAAC,EAJgBwH,8BAA8B,KAA9BA,8BAA8B;AAmC/C,OAAM,IAAWC,+BAA+B;AAAhD,WAAiBA,+BAA+B;EACjCA,+BAAA,CAAA1H,kBAAkB,GAAG,UAACC,GAAoC;IAAU,OAAAC,QAAA,KAC5ED,GAAG;EADyE,CAE/E;AACJ,CAAC,EAJgByH,+BAA+B,KAA/BA,+BAA+B;AAsChD,OAAM,IAAWC,0CAA0C;AAA3D,WAAiBA,0CAA0C;EAC5CA,0CAAA,CAAA3H,kBAAkB,GAAG,UAACC,GAA+C;IAAU,OAAAC,QAAA,KACvFD,GAAG;EADoF,CAE1F;AACJ,CAAC,EAJgB0H,0CAA0C,KAA1CA,0CAA0C;AAmB3D,OAAM,IAAWC,gCAAgC;AAAjD,WAAiBA,gCAAgC;EAClCA,gCAAA,CAAA5H,kBAAkB,GAAG,UAACC,GAAqC;IAAU,OAAAC,QAAA,KAC7ED,GAAG;EAD0E,CAEhF;AACJ,CAAC,EAJgB2H,gCAAgC,KAAhCA,gCAAgC;AAkCjD,OAAM,IAAWC,yBAAyB;AAA1C,WAAiBA,yBAAyB;EAC3BA,yBAAA,CAAA7H,kBAAkB,GAAG,UAACC,GAA8B;IAAU,OAAAC,QAAA,KACtED,GAAG;EADmE,CAEzE;AACJ,CAAC,EAJgB4H,yBAAyB,KAAzBA,yBAAyB;AA6C1C,OAAM,IAAWC,0BAA0B;AAA3C,WAAiBA,0BAA0B;EAC5BA,0BAAA,CAAA9H,kBAAkB,GAAG,UAACC,GAA+B;IAAU,OAAAC,QAAA,KACvED,GAAG;EADoE,CAE1E;AACJ,CAAC,EAJgB6H,0BAA0B,KAA1BA,0BAA0B;AAwB3C,OAAM,IAAWC,iCAAiC;AAAlD,WAAiBA,iCAAiC;EACnCA,iCAAA,CAAA/H,kBAAkB,GAAG,UAACC,GAAsC;IAAU,OAAAC,QAAA,KAC9ED,GAAG;EAD2E,CAEjF;AACJ,CAAC,EAJgB8H,iCAAiC,KAAjCA,iCAAiC;AAoBlD,OAAM,IAAWC,gBAAgB;AAAjC,WAAiBA,gBAAgB;EAClBA,gBAAA,CAAAhI,kBAAkB,GAAG,UAACC,GAAqB;IAAU,OAAAC,QAAA,KAC7DD,GAAG;EAD0D,CAEhE;AACJ,CAAC,EAJgB+H,gBAAgB,KAAhBA,gBAAgB;AAyBjC,OAAM,IAAWC,iBAAiB;AAAlC,WAAiBA,iBAAiB;EACnBA,iBAAA,CAAAjI,kBAAkB,GAAG,UAACC,GAAsB;IAAU,OAAAC,QAAA,KAC9DD,GAAG;EAD2D,CAEjE;AACJ,CAAC,EAJgBgI,iBAAiB,KAAjBA,iBAAiB;AAsBlC,OAAM,IAAWC,iBAAiB;AAAlC,WAAiBA,iBAAiB;EACnBA,iBAAA,CAAAlI,kBAAkB,GAAG,UAACC,GAAsB;IAAU,OAAAC,QAAA,KAC9DD,GAAG;EAD2D,CAEjE;AACJ,CAAC,EAJgBiI,iBAAiB,KAAjBA,iBAAiB;AAMlC,WAAYC,SAGX;AAHD,WAAYA,SAAS;EACnBA,SAAA,iBAAa;EACbA,SAAA,iBAAa;AACf,CAAC,EAHWA,SAAS,KAATA,SAAS;AAsDrB,OAAM,IAAWC,aAAa;AAA9B,WAAiBA,aAAa;EACfA,aAAA,CAAApI,kBAAkB,GAAG,UAACC,GAAkB;IAAU,OAAAC,QAAA,KAC1DD,GAAG;EADuD,CAE7D;AACJ,CAAC,EAJgBmI,aAAa,KAAbA,aAAa;AAkB9B,OAAM,IAAWC,kBAAkB;AAAnC,WAAiBA,kBAAkB;EACpBA,kBAAA,CAAArI,kBAAkB,GAAG,UAACC,GAAuB;IAAU,OAAAC,QAAA,KAC/DD,GAAG;EAD4D,CAElE;AACJ,CAAC,EAJgBoI,kBAAkB,KAAlBA,kBAAkB;AAsCnC,OAAM,IAAWC,IAAI;AAArB,WAAiBA,IAAI;EACNA,IAAA,CAAAtI,kBAAkB,GAAG,UAACC,GAAS;IAAU,OAAAC,QAAA,KACjDD,GAAG;EAD8C,CAEpD;AACJ,CAAC,EAJgBqI,IAAI,KAAJA,IAAI;AAMrB,WAAYC,cAGX;AAHD,WAAYA,cAAc;EACxBA,cAAA,eAAW;EACXA,cAAA,uBAAmB;AACrB,CAAC,EAHWA,cAAc,KAAdA,cAAc;AAoB1B,OAAM,IAAWC,aAAa;AAA9B,WAAiBA,aAAa;EACfA,aAAA,CAAAxI,kBAAkB,GAAG,UAACC,GAAkB;IAAU,OAAAC,QAAA,KAC1DD,GAAG;EADuD,CAE7D;AACJ,CAAC,EAJgBuI,aAAa,KAAbA,aAAa;AAuB9B,OAAM,IAAWC,SAAS;AAA1B,WAAiBA,SAAS;EACXA,SAAA,CAAAzI,kBAAkB,GAAG,UAACC,GAAc;IAAU,OAAAC,QAAA,KACtDD,GAAG;EADmD,CAEzD;AACJ,CAAC,EAJgBwI,SAAS,KAATA,SAAS;AAuB1B,OAAM,IAAWC,UAAU;AAA3B,WAAiBA,UAAU;EACZA,UAAA,CAAA1I,kBAAkB,GAAG,UAACC,GAAe;IAAU,OAAAC,QAAA,KACvDD,GAAG;EADoD,CAE1D;AACJ,CAAC,EAJgByI,UAAU,KAAVA,UAAU;AAM3B,WAAYC,gBAGX;AAHD,WAAYA,gBAAgB;EAC1BA,gBAAA,mBAAe;EACfA,gBAAA,2BAAuB;AACzB,CAAC,EAHWA,gBAAgB,KAAhBA,gBAAgB;AAa5B,OAAM,IAAWC,uBAAuB;AAAxC,WAAiBA,uBAAuB;EACzBA,uBAAA,CAAA5I,kBAAkB,GAAG,UAACC,GAA4B;IAAU,OAAAC,QAAA,KACpED,GAAG;EADiE,CAEvE;AACJ,CAAC,EAJgB2I,uBAAuB,KAAvBA,uBAAuB;AAkBxC,OAAM,IAAWC,wBAAwB;AAAzC,WAAiBA,wBAAwB;EAC1BA,wBAAA,CAAA7I,kBAAkB,GAAG,UAACC,GAA6B;IAAU,OAAAC,QAAA,KACrED,GAAG;EADkE,CAExE;AACJ,CAAC,EAJgB4I,wBAAwB,KAAxBA,wBAAwB;AAiCzC,OAAM,IAAWC,8BAA8B;AAA/C,WAAiBA,8BAA8B;EAChCA,8BAAA,CAAA9I,kBAAkB,GAAG,UAACC,GAAmC;IAAU,OAAAC,QAAA,KAC3ED,GAAG;EADwE,CAE9E;AACJ,CAAC,EAJgB6I,8BAA8B,KAA9BA,8BAA8B;AAM/C,WAAYC,cAIX;AAJD,WAAYA,cAAc;EACxBA,cAAA,qBAAiB;EACjBA,cAAA,+BAA2B;EAC3BA,cAAA,2BAAuB;AACzB,CAAC,EAJWA,cAAc,KAAdA,cAAc;AA0C1B,OAAM,IAAWC,aAAa;AAA9B,WAAiBA,aAAa;EACfA,aAAA,CAAAhJ,kBAAkB,GAAG,UAACC,GAAkB;IAAU,OAAAC,QAAA,KAC1DD,GAAG;EADuD,CAE7D;AACJ,CAAC,EAJgB+I,aAAa,KAAbA,aAAa;AAmC9B,OAAM,IAAWC,+BAA+B;AAAhD,WAAiBA,+BAA+B;EACjCA,+BAAA,CAAAjJ,kBAAkB,GAAG,UAACC,GAAoC;IAAU,OAAAC,QAAA,KAC5ED,GAAG;EADyE,CAE/E;AACJ,CAAC,EAJgBgJ,+BAA+B,KAA/BA,+BAA+B;AAqChD,OAAM,IAAWC,2BAA2B;AAA5C,WAAiBA,2BAA2B;EAC7BA,2BAAA,CAAAlJ,kBAAkB,GAAG,UAACC,GAAgC;IAAU,OAAAC,QAAA,KACxED,GAAG;EADqE,CAE3E;AACJ,CAAC,EAJgBiJ,2BAA2B,KAA3BA,2BAA2B;AAwC5C,OAAM,IAAWC,4BAA4B;AAA7C,WAAiBA,4BAA4B;EAC9BA,4BAAA,CAAAnJ,kBAAkB,GAAG,UAACC,GAAiC;IAAU,OAAAC,QAAA,KACzED,GAAG;EADsE,CAE5E;AACJ,CAAC,EAJgBkJ,4BAA4B,KAA5BA,4BAA4B;AA0B7C,OAAM,IAAWC,uBAAuB;AAAxC,WAAiBA,uBAAuB;EACzBA,uBAAA,CAAApJ,kBAAkB,GAAG,UAACC,GAA4B;IAAU,OAAAC,QAAA,KACpED,GAAG;EADiE,CAEvE;AACJ,CAAC,EAJgBmJ,uBAAuB,KAAvBA,uBAAuB;AAmCxC,OAAM,IAAWC,wBAAwB;AAAzC,WAAiBA,wBAAwB;EAC1BA,wBAAA,CAAArJ,kBAAkB,GAAG,UAACC,GAA6B;IAAU,OAAAC,QAAA,KACrED,GAAG;EADkE,CAExE;AACJ,CAAC,EAJgBoJ,wBAAwB,KAAxBA,wBAAwB;AAgCzC,OAAM,IAAWC,oBAAoB;AAArC,WAAiBA,oBAAoB;EACtBA,oBAAA,CAAAtJ,kBAAkB,GAAG,UAACC,GAAyB;IAAU,OAAAC,QAAA,KACjED,GAAG;EAD8D,CAEpE;AACJ,CAAC,EAJgBqJ,oBAAoB,KAApBA,oBAAoB;AA0BrC,OAAM,IAAWC,YAAY;AAA7B,WAAiBA,YAAY;EACdA,YAAA,CAAAvJ,kBAAkB,GAAG,UAACC,GAAiB;IAAU,OAAAC,QAAA,KACzDD,GAAG;EADsD,CAE5D;AACJ,CAAC,EAJgBsJ,YAAY,KAAZA,YAAY;AA6B7B,OAAM,IAAWC,WAAW;AAA5B,WAAiBA,WAAW;EACbA,WAAA,CAAAxJ,kBAAkB,GAAG,UAACC,GAAgB;IAAU,OAAAC,QAAA,KACxDD,GAAG;EADqD,CAE3D;AACJ,CAAC,EAJgBuJ,WAAW,KAAXA,WAAW;AAwC5B,OAAM,IAAWC,qBAAqB;AAAtC,WAAiBA,qBAAqB;EACvBA,qBAAA,CAAAzJ,kBAAkB,GAAG,UAACC,GAA0B;IAAU,OAAAC,QAAA,KAClED,GAAG;EAD+D,CAErE;AACJ,CAAC,EAJgBwJ,qBAAqB,KAArBA,qBAAqB;AAMtC,WAAYC,oBAGX;AAHD,WAAYA,oBAAoB;EAC9BA,oBAAA,iBAAa;EACbA,oBAAA,2BAAuB;AACzB,CAAC,EAHWA,oBAAoB,KAApBA,oBAAoB;AAmChC,OAAM,IAAWC,wBAAwB;AAAzC,WAAiBA,wBAAwB;EAC1BA,wBAAA,CAAA3J,kBAAkB,GAAG,UAACC,GAA6B;IAAU,OAAAC,QAAA,KACrED,GAAG;EADkE,CAExE;AACJ,CAAC,EAJgB0J,wBAAwB,KAAxBA,wBAAwB;AAqBzC,OAAM,IAAWC,cAAc;AAA/B,WAAiBA,cAAc;EAChBA,cAAA,CAAA5J,kBAAkB,GAAG,UAACC,GAAmB;IAAU,OAAAC,QAAA,KAC3DD,GAAG;EADwD,CAE9D;AACJ,CAAC,EAJgB2J,cAAc,KAAdA,cAAc;AAyC/B,OAAM,IAAWC,yBAAyB;AAA1C,WAAiBA,yBAAyB;EAC3BA,yBAAA,CAAA7J,kBAAkB,GAAG,UAACC,GAA8B;IAAU,OAAAC,QAAA,KACtED,GAAG;EADmE,CAEzE;AACJ,CAAC,EAJgB4J,yBAAyB,KAAzBA,yBAAyB;AAM1C,WAAYC,oBAGX;AAHD,WAAYA,oBAAoB;EAC9BA,oBAAA,mBAAe;EACfA,oBAAA,2BAAuB;AACzB,CAAC,EAHWA,oBAAoB,KAApBA,oBAAoB;AAkChC,OAAM,IAAWC,wBAAwB;AAAzC,WAAiBA,wBAAwB;EAC1BA,wBAAA,CAAA/J,kBAAkB,GAAG,UAACC,GAA6B;IAAU,OAAAC,QAAA,KACrED,GAAG;EADkE,CAExE;AACJ,CAAC,EAJgB8J,wBAAwB,KAAxBA,wBAAwB;AAyBzC,OAAM,IAAWC,eAAe;AAAhC,WAAiBA,eAAe;EACjBA,eAAA,CAAAhK,kBAAkB,GAAG,UAACC,GAAoB;IAAU,OAAAC,QAAA,KAC5DD,GAAG;EADyD,CAE/D;AACJ,CAAC,EAJgB+J,eAAe,KAAfA,eAAe;AAmChC,OAAM,IAAWC,yBAAyB;AAA1C,WAAiBA,yBAAyB;EAC3BA,yBAAA,CAAAjK,kBAAkB,GAAG,UAACC,GAA8B;IAAU,OAAAC,QAAA,KACtED,GAAG;EADmE,CAEzE;AACJ,CAAC,EAJgBgK,yBAAyB,KAAzBA,yBAAyB;AAyB1C,OAAM,IAAWC,0BAA0B;AAA3C,WAAiBA,0BAA0B;EAC5BA,0BAAA,CAAAlK,kBAAkB,GAAG,UAACC,GAA+B;IAAU,OAAAC,QAAA,KACvED,GAAG;EADoE,CAE1E;AACJ,CAAC,EAJgBiK,0BAA0B,KAA1BA,0BAA0B;AAsB3C,OAAM,IAAWC,WAAW;AAA5B,WAAiBA,WAAW;EACbA,WAAA,CAAAnK,kBAAkB,GAAG,UAACC,GAAgB;IAAU,OAAAC,QAAA,KACxDD,GAAG;EADqD,CAE3D;AACJ,CAAC,EAJgBkK,WAAW,KAAXA,WAAW;AAM5B,WAAYC,gBAIX;AAJD,WAAYA,gBAAgB;EAC1BA,gBAAA,gCAA4B;EAC5BA,gBAAA,4BAAwB;EACxBA,gBAAA,8BAA0B;AAC5B,CAAC,EAJWA,gBAAgB,KAAhBA,gBAAgB;AAqB5B,OAAM,IAAWC,mBAAmB;AAApC,WAAiBA,mBAAmB;EACrBA,mBAAA,CAAArK,kBAAkB,GAAG,UAACC,GAAwB;IAAU,OAAAC,QAAA,KAChED,GAAG;EAD6D,CAEnE;AACJ,CAAC,EAJgBoK,mBAAmB,KAAnBA,mBAAmB;AAMpC,WAAYC,WAGX;AAHD,WAAYA,WAAW;EACrBA,WAAA,iBAAa;EACbA,WAAA,mCAA+B;AACjC,CAAC,EAHWA,WAAW,KAAXA,WAAW;AAiEvB,OAAM,IAAWC,gBAAgB;AAAjC,WAAiBA,gBAAgB;EAClBA,gBAAA,CAAAvK,kBAAkB,GAAG,UAACC,GAAqB;IAAU,OAAAC,QAAA,KAC7DD,GAAG;EAD0D,CAEhE;AACJ,CAAC,EAJgBsK,gBAAgB,KAAhBA,gBAAgB;AAsBjC,OAAM,IAAWC,eAAe;AAAhC,WAAiBA,eAAe;EACjBA,eAAA,CAAAxK,kBAAkB,GAAG,UAACC,GAAoB;IAAU,OAAAC,QAAA,KAC5DD,GAAG;EADyD,CAE/D;AACJ,CAAC,EAJgBuK,eAAe,KAAfA,eAAe;AAyDhC,OAAM,IAAWC,2BAA2B;AAA5C,WAAiBA,2BAA2B;EAC7BA,2BAAA,CAAAzK,kBAAkB,GAAG,UAACC,GAAgC;IAAU,OAAAC,QAAA,KACxED,GAAG;EADqE,CAE3E;AACJ,CAAC,EAJgBwK,2BAA2B,KAA3BA,2BAA2B;AAyB5C,OAAM,IAAWC,uBAAuB;AAAxC,WAAiBA,uBAAuB;EACzBA,uBAAA,CAAA1K,kBAAkB,GAAG,UAACC,GAA4B;IAAU,OAAAC,QAAA,KACpED,GAAG;EADiE,CAEvE;AACJ,CAAC,EAJgByK,uBAAuB,KAAvBA,uBAAuB;AAsBxC,OAAM,IAAWC,mBAAmB;AAApC,WAAiBA,mBAAmB;EACrBA,mBAAA,CAAA3K,kBAAkB,GAAG,UAACC,GAAwB;IAAU,OAAAC,QAAA,KAChED,GAAG;EAD6D,CAEnE;AACJ,CAAC,EAJgB0K,mBAAmB,KAAnBA,mBAAmB;AAyCpC,OAAM,IAAWC,wBAAwB;AAAzC,WAAiBA,wBAAwB;EAC1BA,wBAAA,CAAA5K,kBAAkB,GAAG,UAACC,GAA6B;IAAU,OAAAC,QAAA,KACrED,GAAG;EADkE,CAExE;AACJ,CAAC,EAJgB2K,wBAAwB,KAAxBA,wBAAwB;AAqBzC,OAAM,IAAWC,oCAAoC;AAArD,WAAiBA,oCAAoC;EACtCA,oCAAA,CAAA7K,kBAAkB,GAAG,UAACC,GAAyC;IAAU,OAAAC,QAAA,KACjFD,GAAG;EAD8E,CAEpF;AACJ,CAAC,EAJgB4K,oCAAoC,KAApCA,oCAAoC;AA0ErD,OAAM,IAAWC,iBAAiB;AAAlC,WAAiBA,iBAAiB;EACnBA,iBAAA,CAAA9K,kBAAkB,GAAG,UAACC,GAAsB;IAAU,OAAAC,QAAA,KAC9DD,GAAG;EAD2D,CAEjE;AACJ,CAAC,EAJgB6K,iBAAiB,KAAjBA,iBAAiB;AAMlC,WAAYC,MAQX;AARD,WAAYA,MAAM;EAChBA,MAAA,2CAAuC;EACvCA,MAAA,iCAA6B;EAC7BA,MAAA,qCAAiC;EACjCA,MAAA,qCAAiC;EACjCA,MAAA,yCAAqC;EACrCA,MAAA,mCAA+B;EAC/BA,MAAA,6CAAyC;AAC3C,CAAC,EARWA,MAAM,KAANA,MAAM;AAkDlB,OAAM,IAAWC,aAAa;AAA9B,WAAiBA,aAAa;EACfA,aAAA,CAAAhL,kBAAkB,GAAG,UAACC,GAAkB;IAAU,OAAAC,QAAA,KAC1DD,GAAG;EADuD,CAE7D;AACJ,CAAC,EAJgB+K,aAAa,KAAbA,aAAa;AA4D9B,OAAM,IAAWC,kBAAkB;AAAnC,WAAiBA,kBAAkB;EACpBA,kBAAA,CAAAjL,kBAAkB,GAAG,UAACC,GAAuB;IAAU,OAAAC,QAAA,KAC/DD,GAAG;EAD4D,CAElE;AACJ,CAAC,EAJgBgL,kBAAkB,KAAlBA,kBAAkB;AAwBnC,OAAM,IAAWC,6BAA6B;AAA9C,WAAiBA,6BAA6B;EAC/BA,6BAAA,CAAAlL,kBAAkB,GAAG,UAACC,GAAkC;IAAU,OAAAC,QAAA,KAC1ED,GAAG;EADuE,CAE7E;AACJ,CAAC,EAJgBiL,6BAA6B,KAA7BA,6BAA6B;AAkB9C,OAAM,IAAWC,sBAAsB;AAAvC,WAAiBA,sBAAsB;EACxBA,sBAAA,CAAAnL,kBAAkB,GAAG,UAACC,GAA2B;IAAU,OAAAC,QAAA,KACnED,GAAG;EADgE,CAEtE;AACJ,CAAC,EAJgBkL,sBAAsB,KAAtBA,sBAAsB;AA0BvC,OAAM,IAAWC,uBAAuB;AAAxC,WAAiBA,uBAAuB;EACzBA,uBAAA,CAAApL,kBAAkB,GAAG,UAACC,GAA4B;IAAU,OAAAC,QAAA,KACpED,GAAG;EADiE,CAEvE;AACJ,CAAC,EAJgBmL,uBAAuB,KAAvBA,uBAAuB;AAyBxC,OAAM,IAAWC,gBAAgB;AAAjC,WAAiBA,gBAAgB;EAClBA,gBAAA,CAAArL,kBAAkB,GAAG,UAACC,GAAqB;IAAU,OAAAC,QAAA,KAC7DD,GAAG;EAD0D,CAEhE;AACJ,CAAC,EAJgBoL,gBAAgB,KAAhBA,gBAAgB;AAwBjC,OAAM,IAAWC,iBAAiB;AAAlC,WAAiBA,iBAAiB;EACnBA,iBAAA,CAAAtL,kBAAkB,GAAG,UAACC,GAAsB;IAAU,OAAAC,QAAA,KAC9DD,GAAG;EAD2D,CAEjE;AACJ,CAAC,EAJgBqL,iBAAiB,KAAjBA,iBAAiB;AAmBlC,OAAM,IAAWC,2BAA2B;AAA5C,WAAiBA,2BAA2B;EAC7BA,2BAAA,CAAAvL,kBAAkB,GAAG,UAACC,GAAgC;IAAU,OAAAC,QAAA,KACxED,GAAG;EADqE,CAE3E;AACJ,CAAC,EAJgBsL,2BAA2B,KAA3BA,2BAA2B;AAwB5C,OAAM,IAAWC,eAAe;AAAhC,WAAiBA,eAAe;EACjBA,eAAA,CAAAxL,kBAAkB,GAAG,UAACC,GAAoB;IAAU,OAAAC,QAAA,KAC5DD,GAAG;EADyD,CAE/D;AACJ,CAAC,EAJgBuL,eAAe,KAAfA,eAAe;AAmBhC,OAAM,IAAWC,4BAA4B;AAA7C,WAAiBA,4BAA4B;EAC9BA,4BAAA,CAAAzL,kBAAkB,GAAG,UAACC,GAAiC;IAAU,OAAAC,QAAA,KACzED,GAAG;EADsE,CAE5E;AACJ,CAAC,EAJgBwL,4BAA4B,KAA5BA,4BAA4B;AAsB7C,OAAM,IAAWC,mBAAmB;AAApC,WAAiBA,mBAAmB;EACrBA,mBAAA,CAAA1L,kBAAkB,GAAG,UAACC,GAAwB;IAAU,OAAAC,QAAA,KAChED,GAAG;EAD6D,CAEnE;AACJ,CAAC,EAJgByL,mBAAmB,KAAnBA,mBAAmB;AAiBpC,OAAM,IAAWC,2BAA2B;AAA5C,WAAiBA,2BAA2B;EAC7BA,2BAAA,CAAA3L,kBAAkB,GAAG,UAACC,GAAgC;IAAU,OAAAC,QAAA,KACxED,GAAG;EADqE,CAE3E;AACJ,CAAC,EAJgB0L,2BAA2B,KAA3BA,2BAA2B;AAmC5C,OAAM,IAAWC,4BAA4B;AAA7C,WAAiBA,4BAA4B;EAC9BA,4BAAA,CAAA5L,kBAAkB,GAAG,UAACC,GAAiC;IAAU,OAAAC,QAAA,KACzED,GAAG;EADsE,CAE5E;AACJ,CAAC,EAJgB2L,4BAA4B,KAA5BA,4BAA4B;AAgC7C,OAAM,IAAWC,kBAAkB;AAAnC,WAAiBA,kBAAkB;EACpBA,kBAAA,CAAA7L,kBAAkB,GAAG,UAACC,GAAuB;IAAU,OAAAC,QAAA,KAC/DD,GAAG;EAD4D,CAElE;AACJ,CAAC,EAJgB4L,kBAAkB,KAAlBA,kBAAkB;AAwBnC,OAAM,IAAWC,mBAAmB;AAApC,WAAiBA,mBAAmB;EACrBA,mBAAA,CAAA9L,kBAAkB,GAAG,UAACC,GAAwB;IAAU,OAAAC,QAAA,KAChED,GAAG;EAD6D,CAEnE;AACJ,CAAC,EAJgB6L,mBAAmB,KAAnBA,mBAAmB;AAqDpC,OAAM,IAAWC,yBAAyB;AAA1C,WAAiBA,yBAAyB;EAC3BA,yBAAA,CAAA/L,kBAAkB,GAAG,UAACC,GAA8B;IAAU,OAAAC,QAAA,KACtED,GAAG;EADmE,CAEzE;AACJ,CAAC,EAJgB8L,yBAAyB,KAAzBA,yBAAyB;AA+B1C,OAAM,IAAWC,0BAA0B;AAA3C,WAAiBA,0BAA0B;EAC5BA,0BAAA,CAAAhM,kBAAkB,GAAG,UAACC,GAA+B;IAAU,OAAAC,QAAA,KACvED,GAAG;EADoE,CAE1E;AACJ,CAAC,EAJgB+L,0BAA0B,KAA1BA,0BAA0B;AAiB3C,OAAM,IAAWC,KAAK;AAAtB,WAAiBA,KAAK;EACPA,KAAA,CAAAjM,kBAAkB,GAAG,UAACC,GAAU;IAAU,OAAAC,QAAA,KAClDD,GAAG;EAD+C,CAErD;AACJ,CAAC,EAJgBgM,KAAK,KAALA,KAAK;AAiCtB,OAAM,IAAWC,gCAAgC;AAAjD,WAAiBA,gCAAgC;EAClCA,gCAAA,CAAAlM,kBAAkB,GAAG,UAACC,GAAqC;IAAU,OAAAC,QAAA,KAC7ED,GAAG;EAD0E,CAEhF;AACJ,CAAC,EAJgBiM,gCAAgC,KAAhCA,gCAAgC;AAcjD,OAAM,IAAWC,iCAAiC;AAAlD,WAAiBA,iCAAiC;EACnCA,iCAAA,CAAAnM,kBAAkB,GAAG,UAACC,GAAsC;IAAU,OAAAC,QAAA,KAC9ED,GAAG;EAD2E,CAEjF;AACJ,CAAC,EAJgBkM,iCAAiC,KAAjCA,iCAAiC;AAqBlD,OAAM,IAAWC,sBAAsB;AAAvC,WAAiBA,sBAAsB;EACxBA,sBAAA,CAAApM,kBAAkB,GAAG,UAACC,GAA2B;IAAU,OAAAC,QAAA,KACnED,GAAG;EADgE,CAEtE;AACJ,CAAC,EAJgBmM,sBAAsB,KAAtBA,sBAAsB;AA0CvC,OAAM,IAAWC,6BAA6B;AAA9C,WAAiBA,6BAA6B;EAC/BA,6BAAA,CAAArM,kBAAkB,GAAG,UAACC,GAAkC;IAAU,OAAAC,QAAA,KAC1ED,GAAG;EADuE,CAE7E;AACJ,CAAC,EAJgBoM,6BAA6B,KAA7BA,6BAA6B;AAc9C,OAAM,IAAWC,8BAA8B;AAA/C,WAAiBA,8BAA8B;EAChCA,8BAAA,CAAAtM,kBAAkB,GAAG,UAACC,GAAmC;IAAU,OAAAC,QAAA,KAC3ED,GAAG;EADwE,CAE9E;AACJ,CAAC,EAJgBqM,8BAA8B,KAA9BA,8BAA8B;AA0C/C,OAAM,IAAWC,yBAAyB;AAA1C,WAAiBA,yBAAyB;EAC3BA,yBAAA,CAAAvM,kBAAkB,GAAG,UAACC,GAA8B;IAAU,OAAAC,QAAA,KACtED,GAAG;EADmE,CAEzE;AACJ,CAAC,EAJgBsM,yBAAyB,KAAzBA,yBAAyB;AAc1C,OAAM,IAAWC,0BAA0B;AAA3C,WAAiBA,0BAA0B;EAC5BA,0BAAA,CAAAxM,kBAAkB,GAAG,UAACC,GAA+B;IAAU,OAAAC,QAAA,KACvED,GAAG;EADoE,CAE1E;AACJ,CAAC,EAJgBuM,0BAA0B,KAA1BA,0BAA0B;AA0C3C,OAAM,IAAWC,sBAAsB;AAAvC,WAAiBA,sBAAsB;EACxBA,sBAAA,CAAAzM,kBAAkB,GAAG,UAACC,GAA2B;IAAU,OAAAC,QAAA,KACnED,GAAG;EADgE,CAEtE;AACJ,CAAC,EAJgBwM,sBAAsB,KAAtBA,sBAAsB;AAavC,OAAM,IAAWC,uBAAuB;AAAxC,WAAiBA,uBAAuB;EACzBA,uBAAA,CAAA1M,kBAAkB,GAAG,UAACC,GAA4B;IAAU,OAAAC,QAAA,KACpED,GAAG;EADiE,CAEvE;AACJ,CAAC,EAJgByM,uBAAuB,KAAvBA,uBAAuB;AA2CxC,OAAM,IAAWC,0BAA0B;AAA3C,WAAiBA,0BAA0B;EAC5BA,0BAAA,CAAA3M,kBAAkB,GAAG,UAACC,GAA+B;IAAU,OAAAC,QAAA,KACvED,GAAG;EADoE,CAE1E;AACJ,CAAC,EAJgB0M,0BAA0B,KAA1BA,0BAA0B;AAc3C,OAAM,IAAWC,2BAA2B;AAA5C,WAAiBA,2BAA2B;EAC7BA,2BAAA,CAAA5M,kBAAkB,GAAG,UAACC,GAAgC;IAAU,OAAAC,QAAA,KACxED,GAAG;EADqE,CAE3E;AACJ,CAAC,EAJgB2M,2BAA2B,KAA3BA,2BAA2B;AAiC5C,OAAM,IAAWC,0BAA0B;AAA3C,WAAiBA,0BAA0B;EAC5BA,0BAAA,CAAA7M,kBAAkB,GAAG,UAACC,GAA+B;IAAU,OAAAC,QAAA,KACvED,GAAG;EADoE,CAE1E;AACJ,CAAC,EAJgB4M,0BAA0B,KAA1BA,0BAA0B;AAc3C,OAAM,IAAWC,2BAA2B;AAA5C,WAAiBA,2BAA2B;EAC7BA,2BAAA,CAAA9M,kBAAkB,GAAG,UAACC,GAAgC;IAAU,OAAAC,QAAA,KACxED,GAAG;EADqE,CAE3E;AACJ,CAAC,EAJgB6M,2BAA2B,KAA3BA,2BAA2B;AAsB5C,OAAM,IAAWC,0BAA0B;AAA3C,WAAiBA,0BAA0B;EAC5BA,0BAAA,CAAA/M,kBAAkB,GAAG,UAACC,GAA+B;IAAU,OAAAC,QAAA,KACvED,GAAG;EADoE,CAE1E;AACJ,CAAC,EAJgB8M,0BAA0B,KAA1BA,0BAA0B;AAa3C,OAAM,IAAWC,2BAA2B;AAA5C,WAAiBA,2BAA2B;EAC7BA,2BAAA,CAAAhN,kBAAkB,GAAG,UAACC,GAAgC;IAAU,OAAAC,QAAA,KACxED,GAAG;EADqE,CAE3E;AACJ,CAAC,EAJgB+M,2BAA2B,KAA3BA,2BAA2B;AAsB5C,OAAM,IAAWC,wBAAwB;AAAzC,WAAiBA,wBAAwB;EAC1BA,wBAAA,CAAAjN,kBAAkB,GAAG,UAACC,GAA6B;IAAU,OAAAC,QAAA,KACrED,GAAG;EADkE,CAExE;AACJ,CAAC,EAJgBgN,wBAAwB,KAAxBA,wBAAwB;AAsBzC,OAAM,IAAWC,gCAAgC;AAAjD,WAAiBA,gCAAgC;EAClCA,gCAAA,CAAAlN,kBAAkB,GAAG,UAACC,GAAqC;IAAU,OAAAC,QAAA,KAC7ED,GAAG;EAD0E,CAEhF;AACJ,CAAC,EAJgBiN,gCAAgC,KAAhCA,gCAAgC;AAuBjD,OAAM,IAAWC,4BAA4B;AAA7C,WAAiBA,4BAA4B;EAC9BA,4BAAA,CAAAnN,kBAAkB,GAAG,UAACC,GAAiC;IAAU,OAAAC,QAAA,KACzED,GAAG;EADsE,CAE5E;AACJ,CAAC,EAJgBkN,4BAA4B,KAA5BA,4BAA4B;AA2C7C,OAAM,IAAWC,4BAA4B;AAA7C,WAAiBA,4BAA4B;EAC9BA,4BAAA,CAAApN,kBAAkB,GAAG,UAACC,GAAiC;IAAU,OAAAC,QAAA,KACzED,GAAG;EADsE,CAE5E;AACJ,CAAC,EAJgBmN,4BAA4B,KAA5BA,4BAA4B;AAc7C,OAAM,IAAWC,6BAA6B;AAA9C,WAAiBA,6BAA6B;EAC/BA,6BAAA,CAAArN,kBAAkB,GAAG,UAACC,GAAkC;IAAU,OAAAC,QAAA,KAC1ED,GAAG;EADuE,CAE7E;AACJ,CAAC,EAJgBoN,6BAA6B,KAA7BA,6BAA6B;AAa9C,OAAM,IAAWC,2BAA2B;AAA5C,WAAiBA,2BAA2B;EAC7BA,2BAAA,CAAAtN,kBAAkB,GAAG,UAACC,GAAgC;IAAU,OAAAC,QAAA,KACxED,GAAG;EADqE,CAE3E;AACJ,CAAC,EAJgBqN,2BAA2B,KAA3BA,2BAA2B;AAQ5C,OAAM,IAAWC,4BAA4B;AAA7C,WAAiBA,4BAA4B;EAC9BA,4BAAA,CAAAvN,kBAAkB,GAAG,UAACC,GAAiC;IAAU,OAAAC,QAAA,KACzED,GAAG;EADsE,CAE5E;AACJ,CAAC,EAJgBsN,4BAA4B,KAA5BA,4BAA4B;AAwB7C,OAAM,IAAWC,yBAAyB;AAA1C,WAAiBA,yBAAyB;EAC3BA,yBAAA,CAAAxN,kBAAkB,GAAG,UAACC,GAA8B;IAAU,OAAAC,QAAA,KACtED,GAAG;EADmE,CAEzE;AACJ,CAAC,EAJgBuN,yBAAyB,KAAzBA,yBAAyB;AAsC1C,OAAM,IAAWC,yBAAyB;AAA1C,WAAiBA,yBAAyB;EAC3BA,yBAAA,CAAAzN,kBAAkB,GAAG,UAACC,GAA8B;IAAU,OAAAC,QAAA,KACtED,GAAG;EADmE,CAEzE;AACJ,CAAC,EAJgBwN,yBAAyB,KAAzBA,yBAAyB;AAa1C,OAAM,IAAWC,0BAA0B;AAA3C,WAAiBA,0BAA0B;EAC5BA,0BAAA,CAAA1N,kBAAkB,GAAG,UAACC,GAA+B;IAAU,OAAAC,QAAA,KACvED,GAAG;EADoE,CAE1E;AACJ,CAAC,EAJgByN,0BAA0B,KAA1BA,0BAA0B;AAc3C,OAAM,IAAWC,yBAAyB;AAA1C,WAAiBA,yBAAyB;EAC3BA,yBAAA,CAAA3N,kBAAkB,GAAG,UAACC,GAA8B;IAAU,OAAAC,QAAA,KACtED,GAAG;EADmE,CAEzE;AACJ,CAAC,EAJgB0N,yBAAyB,KAAzBA,yBAAyB;AAa1C,OAAM,IAAWC,0BAA0B;AAA3C,WAAiBA,0BAA0B;EAC5BA,0BAAA,CAAA5N,kBAAkB,GAAG,UAACC,GAA+B;IAAU,OAAAC,QAAA,KACvED,GAAG;EADoE,CAE1E;AACJ,CAAC,EAJgB2N,0BAA0B,KAA1BA,0BAA0B;AAa3C,OAAM,IAAWC,0BAA0B;AAA3C,WAAiBA,0BAA0B;EAC5BA,0BAAA,CAAA7N,kBAAkB,GAAG,UAACC,GAA+B;IAAU,OAAAC,QAAA,KACvED,GAAG;EADoE,CAE1E;AACJ,CAAC,EAJgB4N,0BAA0B,KAA1BA,0BAA0B;AAQ3C,OAAM,IAAWC,2BAA2B;AAA5C,WAAiBA,2BAA2B;EAC7BA,2BAAA,CAAA9N,kBAAkB,GAAG,UAACC,GAAgC;IAAU,OAAAC,QAAA,KACxED,GAAG;EADqE,CAE3E;AACJ,CAAC,EAJgB6N,2BAA2B,KAA3BA,2BAA2B","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}